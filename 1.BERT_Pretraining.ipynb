{"cells":[{"cell_type":"code","metadata":{"id":"qtwkhUxBOdRs","colab_type":"code","outputId":"1524a78a-215d-42e9-bd14-fb11e27dad27","executionInfo":{"status":"ok","timestamp":1589215060411,"user_tz":300,"elapsed":24109,"user":{"displayName":"Eadhunath Venghatesan","photoUrl":"","userId":"10975244633908642389"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["%tensorflow_version 1.x\n","import tensorflow\n","print(tensorflow.__version__)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n","1.15.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"e7WoW8gBOgp0","colab_type":"code","outputId":"fdc858ad-7889-4ca6-9066-2ef3e3a29464","executionInfo":{"status":"ok","timestamp":1589215121610,"user_tz":300,"elapsed":82028,"user":{"displayName":"Eadhunath Venghatesan","photoUrl":"","userId":"10975244633908642389"}},"colab":{"base_uri":"https://localhost:8080/","height":490}},"source":["!pip install sentencepiece\n","!git clone https://github.com/google-research/bert\n","\n","\n","\n","import os\n","import sys\n","import json\n","import nltk\n","import random\n","import logging\n","import tensorflow as tf\n","# import sentencepiece as spm\n","\n","from glob import glob\n","from google.colab import auth, drive\n","from tensorflow.keras.utils import Progbar\n","\n","sys.path.append(\"bert\")\n","\n","from bert import modeling, optimization, tokenization\n","from bert.run_pretraining import input_fn_builder, model_fn_builder\n","\n","auth.authenticate_user()\n","\n","# configure logging\n","log = logging.getLogger('tensorflow')\n","log.setLevel(logging.INFO)\n","\n","# create formatter and add it to the handlers\n","formatter = logging.Formatter('%(asctime)s :  %(message)s')\n","sh = logging.StreamHandler()\n","sh.setLevel(logging.INFO)\n","sh.setFormatter(formatter)\n","log.handlers = [sh]\n","\n","\n","if 'COLAB_TPU_ADDR' in os.environ:\n","  log.info(\"Using TPU runtime\")\n","  USE_TPU = True\n","  TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n","\n","  with tf.Session(TPU_ADDRESS) as session:\n","    log.info('TPU address is ' + TPU_ADDRESS)\n","    # Upload credentials to TPU.\n","    with open('/content/adc.json', 'r') as f:\n","      auth_info = json.load(f)\n","    tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n","    \n","else:\n","  log.warning('Not connected to TPU runtime')\n","  USE_TPU = False\n","\n","drive.mount('/content/gdrive')\n","#!ls \"/content/drive/My drive\""],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/2c/8df20f3ac6c22ac224fff307ebc102818206c53fc454ecd37d8ac2060df5/sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 3.3MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.86\n","Cloning into 'bert'...\n","remote: Enumerating objects: 340, done.\u001b[K\n","remote: Total 340 (delta 0), reused 0 (delta 0), pack-reused 340\u001b[K\n","Receiving objects: 100% (340/340), 300.28 KiB | 3.95 MiB/s, done.\n","Resolving deltas: 100% (185/185), done.\n","WARNING:tensorflow:From /content/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["2020-05-11 16:38:08,838 :  Using TPU runtime\n","2020-05-11 16:38:08,840 :  TPU address is grpc://10.31.86.90:8470\n"],"name":"stderr"},{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nUthi7-xnprR","colab_type":"code","colab":{}},"source":["file_id = {'id':\"1agaKgjPrqduNoLl7teW3u7A4N8jSr0Ws\"} # the id can be found in drive, in the sharing link.\n","file_name = \"reddit_corpus.tar.gz\"\n","extracted_file_name = \"reddit\"\n","\n","WIKI_WORDS_IN_VOCAB = 100\n","\n","SUBSAMPLED_DATA_FPATH = \"subsampled.txt\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NgckgCmzO7ak","colab_type":"text"},"source":["### Importing the Reddit Dataset"]},{"cell_type":"code","metadata":{"id":"sFV0PQw0Oq-i","colab_type":"code","outputId":"d95dcddb-08c0-4562-ab88-cc37266e892d","executionInfo":{"status":"ok","timestamp":1589216019915,"user_tz":300,"elapsed":15018,"user":{"displayName":"Eadhunath Venghatesan","photoUrl":"","userId":"10975244633908642389"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import os\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","\n","# 1. Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","downloaded = drive.CreateFile(file_id)   # replace the id with id of file you want to access\n","downloaded.GetContentFile(file_name)        # replace the file name with your file\n","\n","!tar -xf $file_name # Unzipping\n","!ls $extracted_file_name | wc -l #Should be 118059"],"execution_count":16,"outputs":[{"output_type":"stream","text":["118059\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0BZjiYG5qZn0","colab_type":"text"},"source":["#### Combining the files into a single file\n","Change `CORPUS_SIZE` as per requirement. It is the number of lines to be included in the final file.\n","\n","`DEMO_MODE` is used to have a lower `CORPUS_SIZE` for testing/experimental purposes."]},{"cell_type":"code","metadata":{"id":"22s4hmqzPkLW","colab_type":"code","outputId":"6e61e8e8-781a-4e46-d8c0-f25ec32d23e5","executionInfo":{"status":"ok","timestamp":1589216337137,"user_tz":300,"elapsed":10933,"user":{"displayName":"Eadhunath Venghatesan","photoUrl":"","userId":"10975244633908642389"}},"colab":{"base_uri":"https://localhost:8080/","height":390}},"source":["import glob\n","import nltk\n","\n","path = '/content/'+extracted_file_name+'/*.txt'\n","files = glob.glob(path)\n","\n","final_ip = \"final_input.txt\"\n","for name in files:\n","    with open(name) as f:\n","        with open(final_ip, \"a\") as f1:\n","            for line in f:\n","                f1.write(line)\n","                f1.write(\"\\n\")\n","            f1.write(\"\\n\")\n","\n","DEMO_MODE = False #@param {type:\"boolean\"}\n","\n","if DEMO_MODE:\n","  CORPUS_SIZE = 10000\n","else:\n","  CORPUS_SIZE = 100000000 #@param {type: \"integer\"}\n","\n","print(CORPUS_SIZE)\n","  \n","!(head -n $CORPUS_SIZE final_input.txt) > subdataset.txt\n","!mv subdataset.txt dataset.txt\n","\n","!head -n 20 dataset.txt"],"execution_count":17,"outputs":[{"output_type":"stream","text":["100000000\n"," source material is a  Wiki__Podcast__ioeffse  http www racistsandwich com episodes e whats so political about food photography w  Wiki__Celesta__szals  noche as a sjw liberal ill explain it for you in reply to the points you made as undeserving as this clusterfuck of a community is of my efforts im doing this partly because maybe some of you will listen because im a sucker for punishment and because it might be hilarious how you all reply also im bored why do these people ascribe malice to every little thing theres a problem with phrasing in social justice  Wiki__Discourse__fsief  at the moment coming from the top academic  Wiki__Discourse__fsief  to the bottom laymen who maybe read one article on social justice that one time theres a big disconnect not only in terminology but how to appropriately use that terminology like all things rooted in sociology and the  Wiki__Humanities__seiez  in general the  Wiki__Layman__foaobi  are given a media platform you dont see right leaning economists saying what needs to be done with the economy on fox news as often as you hear people like inheritance riding know nothing donald trump fumbling to explain economic concepts and getting a lot wrong while speaking so generally he seems to say nothing at all in the case of this article a similar thing is happening microaggressions the use of this word is always problematic because laymen dont understand that microaggressions arent necessarily aggressive or violent or anything like that yet the word implies that using this term is to be concise but in doing so many special snowflakes get personally offended at the mere mention of the word and react defensively im not a racist dont call me a racist im not being aggressive to people of color wtf but its because the word is probably a poor word to pick out for use here sure its useful in academic circles where someones sociology education allows them to understand what is meant by the use of the word as opposed laymen like yourselves who see the word aggression and just react to it whether its taking photos of dishes with  Wiki__Chopsticks__babi  sticking straight up into rice or noodles which can be seen as offensive in some asian cultures this section is another good example its seen as offensive within the culture if you are a white  Wiki__Dude__eesleq  hanging out in  Wiki__Tokyo__eoosl  and you do it people will just think you are weird i know i was there and one of my best friends for life lived there for years they wont revile and look at you as a racist or anything as she is seemingly implying but i think what she means is to show respect to different cultural conventions around the world i think most people could agree that if every single portrayal of americans was some guy waving a flag shooting a gun and yelling fuck all the time it would be a bit annoying shallow immature and obviously a very inaccurate generalization now it wouldnt matter that much because of the concept of privilege much like being called honky by a  Wiki__Black_people__flfs  you really dont care that for instance the show abenobashi mahou shotengai basically did exactly what i described if you see the scene it makes you giggle but you see japan didnt come in and demolish our country dictate our whole governmental rebuild in its systems etc everything has a sort of  Wiki__Social_environment__ssalzz  that it resides within the  Wiki__Ethical_intuitionism__aaboeb  Wiki__Intuition__knowledge___isfilo  of you not really giving a  Wiki__Shit__eazzi  if someone calls you honky or whatever the  Wiki__Fuck__sslslzz  that the majority of  Wiki__White_people__ibbzoz  likely have is the same kind of thing in the inverse if japanese people misrepresent  Wiki__United_States__efeflso  culture whatever that is arguably even if its not a wholly authentic representation isnt showcasing food from other cultures generally a net positive i actually somewhat agree as a raving sjw libtard myself but this is the first step in a society of integrating elements of another culture into their own or raising their understanding of other cultures this is preliminary sure we had our  Wiki__Lap_steel_guitar__siezib  tiki obsession in the early to mid s but now that we know  Wiki__Hawaii__iezlo  is more than hilariously eating poke all day long or we ought to know since there is plenty of information everywhere using one element of their culture and a bunch of bastardized versions of their culture to get views on a video sell a product etc comes across as lazy uninformed and unnecessarily disrespectful to the source material many of my fellow liberals are having a blast riding the tiki revival in the  Wiki__Cocktail__lsqq  scene they wear traditional  Wiki__Lap_steel_guitar__siezib  garb knock offs incorrectly over  Wiki__Lap_steel_guitar__siezib  Wiki__Aloha_shirt__fsbazb  which arent really  Wiki__Lap_steel_guitar__siezib  either and suck down drinks that white people made up in the s as their interpretation of what a  Wiki__Lap_steel_guitar__siezib   Wiki__Cocktail__lsqq  would taste like now whats the harm you might ask well lets say for instance every time someone meets a  Wiki__Lap_steel_guitar__siezib  person they start asking about poke mai tais etc when the conversation could be about just regular things not infantilizing them by asking them pointed questions about strange bastardized ideas of their culture if we stop spreading pervasive myths or lies about certain cultural traits even by misrepresentation due to ignorance we can cut down on these minor annoyances things like movies where the two main characters are white people that travel to an exotic location and all the locals act in a very hammed up way that is dictated by the movies country of origins stereotypes that have been perpetuated this also potentially cuts down on lead role casting in relevant situations such as this which is better sharing a photo of inauthentic  Wiki__Ph___bzssbs  and exposing people to the concept so they can explore it for themselves or only sharing photos or  Wiki__United_States__efeflso  food because you havent received an advanced degree in the culinary history of every non  Wiki__United_States__efeflso  culture you might encounter this is called a  Wiki__False_dilemma__ilbal  Wiki__Dichotomy__fqiabs  you are acting as if these are the only two options or at least its implied in your incredulous  Wiki__Rhetorical_question__fiieqq  there are more options than just show other cultures food terribly and show  Wiki__United_States__efeflso  burgers and  Wiki__United_States__efeflso   Wiki__Pizza__zflba  all day long theres also as the article that is linked here mentioned the idea that bringing someone in who is from that culture as an expert or taking the time to really understand what the culture is and learn before opening ones mouth or putting pen to the paper thats my rare comment in here dont think i have done one before au revoir you filthy scumbags \n","\n","I studied history in university and please do not ever read history textbooks to learn history! It is taught this way in high school but now when I teach history and have to use textbooks (for a specific exam or curriculum), I just get so frustrated. Textbooks are almost always written in the format of a happened, then b happened, then c happened. I dont understand why this is so because thats a timeline. history is about context. I have no idea how one can teach or learn history by memorizing dates without understanding contextual significance or those slow, simmering historical currents that shape the  Wiki__Human_condition__ibbzie . So here are my thoughts (as someone whose misconceptions of what history is led to her being bitchslapped by the demands of what history actually is in university and who ardently tries to steer current high school students and anyone else away from being similarly bitchslapped): history is bias. every book you read is a product of bias. as long as something has to be interpreted (in historys case, the primary and secondary resources), bias cannot be avoided. Keep this in mind as you choose books and read. I went to a pretty leftist university and our readings were almost always by left-leaning academics. If you are interested in specific topics in history that are very sensitive to  Wiki__Historiography__iezlb , read as much as you can from different perspectives. I always like to read the works cited/bibliography first and see what the author used during research. Very important considerations are: did the author use the canon (or did the author ONLY use the canon which is problematic) in their field? did the author use sources in the LANGUAGE of the region s/he writes about? I know this might sound really duh but I once read this scathing review of what I thought to be a GREAT history book in my very narrow field of Baltic studies because this author did everything right, had great arguments, but had no sources in Estonian, Latvian, or Lithuanian. If you want to get really nitpicky, what are the years of these sources (our professors would always ask us to use caution using  Wiki__Secondary_source__ileiol  written before 1960s). Also ask yourself, what are the institutional affiliations of these writers? Do they usually operate under a Marxist, feminist, post-colonial perspective? Are they revisionist? Etc. You dont always have to read a  Wiki__Secondary_source__ileiol  (like a non-fiction book). In fact, history is so much more exciting when you skip the middleman (secondary sources) and examine the  Wiki__Primary_source__illaqi  themselves. If you study history because you want to relate to the past, its always better, in my opinion, to actually relate yourself with the past. Not related to some historian writing about the past. So many documents are now available in libraries and e-libraries of various universities and government agencies. Read them for yourself and interpret what YOU think. Read the classics in the study of history. People still debate what history is, how it should be studied, why it should be studied, how it is supposed to be taught, how it should be written (this is a big one). The more I studied about my tiny sub-sub-sub field of history, the more I wanted to read about the study of history to understand the minutiae, sustained in the cosmos. I have a lot of books that I like but the one reading that changed my life (how I saw all  Wiki__Social_sciences__zblai  and  Wiki__Humanities__seiez , life in general, human existence) is a very short but difficult reading by  Wiki__Walter_Benjamin__zzliii  called Thesis on the Concept of History. Everything about history is summarized in the short piece. I cannot see history as anything other than what he described. Regarding your question on favourite history books, here is my short list (of mostly authors): Ottoman: Salonica by  Wiki__Mark_Mazower__seeobao  USSR/Eastern Europe:  Wiki__Svetlana_Alexievich__azolliq ,  Wiki__Aleksandr_Solzhenitsyn__ibzs ,  Wiki__Anne_Applebaum__failib ,  Wiki__David_Remnick__eaezlz ,  Wiki__Orlando_Figes__zolabll , Peter Pomerantsev,  Wiki__Orest_Subtelny__blbissi  (Ukrainian history), Ryszard Kapuściński (Imperium). Some  Wiki__Primary_source__illaqi  about the Russian Revolution from  Wiki__Leon_Trotsky__ilaaa  ( Wiki__History_of_the_Russian_Revolution__iifaalfo ), Sukhanovs personal record on the Revolution, Lenins April Thesis ( Wiki__Vladimir_Lenin__iioiszsz  wrote some shorter essays on ethnic issues in the USSR that are quite interesting), Alexandra Kollontais The Autobiography of a Sexually Emancipated Communist Woman. If you need anything else, PM me!\n","\n"," im going to speak agains the crowd and say dont attend cc unless you have to source ht hechingerreport org how often do community college students who get transfer get bachelors degrees go to a  Wiki__Liberal_arts__iablf  college or national  Wiki__University__eilqf  that has a  Wiki__Liberal_arts__iablf  college approach  Wiki__Johns__film___lsefqzs  hopkins brown dartmouth etc these schools can offer you a myriad of resources and allow you to take up to two years before declaring a major unlike universities with different undergrad colleges northwestern cornell  Wiki__Boston__Massachusetts__feoq  college these types of schools allow you to switch your major across the spectrum of the university without the need to apply to a new college \n","\n","I have numerous problems with this, and I will outline them: 1.  Wiki__Adolf_Hitler__zleisae  is not regarded as a hero in Austria. The majority of Austrians, like most Germans, have accepted Allied propaganda which blamed  Wiki__Adolf_Hitler__zleisae  for the war and most of the genocide. And I would appreciate a source for the statues. 2. He did not want to unite the world. He wanted to build a  Wiki__Nazi_Germany__ziziz  which would last 1000 years, and he would do it by accomplishing realistic goals: rebuilding the economy, regaining lost territory and expanding east, and defeating the Soviet Union before it became too powerful. Yes, he wanted Europe to band together against the  Wiki__Russians_and_Americans__aqslqzf , but he never considered uniting the world. If you have a source where he suggested it, I would like to see it. 3.  Wiki__The_Holocaust__ioeqblqe  isnt a lie. Millions of minorities, along with many others, did die during  Wiki__World_War_II__ezqzl , just not the way most people said they did: there were many cases of  Wiki__Starvation__zisaqi , disease, being worked to death, or executions. Saying the Holocaust just didnt happen is like Americans waving off the millions of Germans who starved to death after the end of the war, or Britain ignoring the many cases of famine in India which killed countless millions.  Wiki__The_Holocaust__ioeqblqe  Wiki__The_Holocaust__ioeqblqe  was only intentional in some cases like  Wiki__Heinrich_Himmler__iefeb  Wiki__Heinrich_Himmler__iefeb ,  Wiki__Reinhard_Heydrich__zsqif  Wiki__Reinhard_Heydrich__zsqif , the SS, and leaders of concentration camps, and the  Wiki__The_Holocaust__ioeqblqe  doesnt make the Germans wrong, but its ridiculous to ignore it. But youre right that  Wiki__Adolf_Hitler__zleisae  never ordered the Holocaust, nor did he have extensive knowledge of it. 4. Please be a little more respectful of women. Their role, even when restricted to the family and housekeeping, is very important to raising respectable citizens. On top of that, they can also serve other useful functions, from agriculture to the arts (where would we be without Leni Riefenstahls * Wiki__Triumph_of_the_Will__eooeq *?). Also, please explain what you mean by women owning children with social funds. 5. You really shouldnt laugh at piles of dead people. No matter how bad they are as a group, thats the reaction of sadistic criminals. Im not asking that you show compassion for them, just that you take death a little more seriously. Im sorry if this looks like a rant; Im just trying to address everything and truly would like a response.\n","\n","anything stem will be likely impacted at a uc school meaning a lot of people want to get into those majors but there arent enough spots as a result they only accept a certain number of applicants applying for the major at that school this leads people to strategize how they apply you can apply as a biology medicine tech major and hope you get in you can apply to an unimpacted major such as art history a language  Wiki__Cultural_studies__sqbzs  religion history etc which gives you a better chance of getting into the school this is difficult down the road though because you have to then switch into the major you want when you get to the school you might not get the major you want or you might not get the classes you need i wouldnt recommend applying undeclared and no problem happy to share my experience i was so confused about this stuff when i was in high school \n","\n","The major thing youll notice is the Western focus on  Wiki__Critical_thinking__zlbalz . I have a BAH from Queens University in History and can tell you that things which would be very important in Chinese education (such as remembering specific dates and peoples names) is important, but will not get you a good grade by themselves. To succeed in the  Wiki__Humanities__seiez  in North America (or at least in Canada) you should be looking for the big picture or the story and adding in specific detail to support your main point. If you just present bullet pointed facts that you learned in lecture but dont know how they relate and dont tell a good story, youll get a C. If you tell a good story but dont know the supporting details like names and dates, youll get a B. If you can tell a good story and support it with the minute details of names and dates, youll get an A. What professors are looking for you to do is not impress them by presenting them with the exact same facts they told you word for word (which wont work) but they want to know that you really *understood* what they were telling you and why. They want you to use your own brain to draw your own conclusions. I have gotten As on papers here where my essay was essentially I think the professor is wrong and here is why. which is fine as long as you use logic, evidence, and research to back that up.\n","\n","Major lack? I dont remember there ever being one, but they were needs of the army for a while, which really just means there was like a 3,000 bonus for signing up as one. If youre really into learning and want to learn stuff heres the path to go, Demand as a requisite of signing to get a school called M6 (Mike 6) thats like an extra year of schooling on the government dime. Then take that and apply to a community college when you get to your unit and transfer all that into college credit. Use army tuition assistance program (pays 75% of tuition for enlisted) to do all your remaining gen ed and  Wiki__Liberal_arts__iablf   Wiki__Bullshit__fofaf  classes, then once you have your 2 year degree, drop a packet for Army physicians assistant program. The Army will straight up pay you E-5 base pay plus a housing allowance of probably about 1500 a month, for a combined pay of more than 40 grand a year, and pay your  Wiki__Tuition__zfqezo  just for you to go to school all day. If you pass the accelerated course within the two years you get to be an officer and a PA, and if you fail, you get to be a medic that knows a  Wiki__LOT_Polish_Airlines__ablfo  and is like a walking aid station. If you just want to skip past all that and get to the blood and guts, join the Navy, be a  Wiki__Hospital_Corpsman__zzeelll .\n","\n","Like /u/Positive_Numbers, I did my BA in  Wiki__Linguistics__ilszb  but took many of the same courses as CogSci students. I am currently working on my MSc in  Wiki__Cognitive_science__sbzb  Wiki__Cognitive_science__sbzb  of Language. I had a different experience from /u/Positive_Numbers in that I enjoyed and felt challenged by the program, but part of that may be because I went looking for challenges: I worked as a research assistant (part of a independent research practicum course), conducted my own linguistic research as USRA student and completed my honours  Wiki__Dissertation__eoozis . As the other students have said, I really enjoyed Child  Wiki__Language_acquisition__iabif ,  Wiki__Sociolinguistics__aqafz , and  Wiki__Psycholinguistics__ibosea . In terms of language courses, I had great experience taking Italian with Dr DAngelo. I took first and second year Italian, first year French (which is in its own department), and introductory  Wiki__Sanskrit__zlbqa  (through Religious Studies). Several languages arent offered past first or second year (Mandarin, Japanese, Polish, Russian, Spanish), but we have fairly strong Italian and German programs. You can also fulfill your language requirements by taking language courses outside of the  Wiki__Linguistics__ilszb  department - you can take French (from the French dept), Latin or Greek (Classics), Hebrew or  Wiki__Sanskrit__zlbqa  (Religious Studies). I believe that Religious Studies is also offering Arabic starting this summer or next fall. As far as proficiency goes, when studying a language, you only get out of it what you put into it. Taking 2-4 semesters of a language isnt going to make you fluent, but you can become conversational if you put in the effort. As a CogSci student, you may be less interested in becoming proficient in another language as you would be in figuring out how the language works, whats similar between language X and other languages, and how our brains process language. Either way, it can be a useful endeavor. /u/SalientJuxe said it better than I could when discussing the stigma of being a  Wiki__Humanities__seiez  student. CogSci and Linguistics are fairly  Wiki__Interdisciplinarity__iszoi  programs, and would be at home in  Wiki__Humanities__seiez ,  Wiki__Social_sciences__zblai  Wiki__Social_sciences__zblai  or Science faculties. As a CogSci student, youll be studying neuroscience, anatomy and psychology as well as the core linguistics courses. (Dont forget Linguistics is a science!) As for the comment that youll have to do a Masters degree to get a decent-paying job - this may be true, but it is definitely not constrained to Linguistics or CogSci. The fact is that more places are looking for advanced degrees, and the skills you learn in a Masters program can make you more employable. In general, people who choose CogSci as their undergraduate degree already have plans to go into SLP in the future, which automatically means a MSc. Dont let this discourage you. Sorry about the novel - I have a lot to say about this program! Feel free to PM me if you have any specific questions!\n","\n"," i highly recommend cornell notes for any  Wiki__Humanities__seiez   Wiki__Social_sciences__zblai  lecture classes i cannot speak to the hard sciences i havent had one of those classes in ages \n","\n","Amazing Eulogy that I saved from... Here I think?? I completely forget who posted it though... Might spark some ideas for you My condolences for your loss :( -------------------------------- So my mother is very Irish Catholic and when my dad passed a few days ago she wanted to have a Catholic mass. Naturally, I wanted to give my Dad, who was very non-religious, the respect he deserved.  Wiki__The_Priest__ieeqoebs  gave his typical Prepare for the lords wrath and ascent into Heaven,  Wiki__Bullshit__fofaf  in the middle of mass which really made me mad. Well, this is the  Wiki__Eulogy__zeiieo  I gave from the  Wiki__Pulpit__zsieol  of a Catholic church, and I thought you all would appreciate it: 32 years, 2 weeks, 6 days - thats how long my dad has been present in my life. Hes been gone now for 5 days. Guess which one feels longer? They say that the only two guarantees in life are death and taxes. But there is also time. Time is what makes the universe work. its why atoms spin and cells divide. How elements are forged in the fires of a billion suns, and spread across the galaxy in gigantic  Wiki__Supernova__zlbao  its time that gives us life, grows with us from when we were babies to  Wiki__Adult__ibleef , and its time that takes it all away. as my favorite geneticist once said, “We are going to die, and that makes us the lucky ones. Most people are never going to die because they are never going to be born. The potential people who could have been here in my place but who will in fact never see the light of day outnumber the sand grains of Arabia. Certainly those unborn ghosts include greater poets than Keats, scientists greater than Newton. We know this because the set of possible people allowed by our DNA so massively exceeds the set of actual people. In the teeth of these stupefying odds it is you and I, in our ordinariness, that are here.We privileged few, who won the lottery of birth against all odds, how dare we whine at our inevitable return to that prior state from which the vast majority have never stirred?” i would give anything for more time with my dad, but all i can do now is continue to be the man he raised me to be. He always taught me to be curious, to love science, and knowledge, and question the universe. he was never a man to ask for help -- he would figure it out on his own. he taught me to love sound, and music -- history, photography, and the arts of all ages But he also taught me about hard work and sacrifice. He bought us our first computer, and, believe it or not, taught me how to use Windows. If it was not for him I would have never opened those computer books. He never discouraged me when I broke the damn thing. He never really understood what the heck I did to them anyway, but he always knew i would fix it --- eventually. He taught me how to use my mind to do great things. He taught me how to use my soul to feel great things. And most importantly, he taught me how to be a man. Time wont ever fix this hole in my life where he used to be, but his memories and his lessons are a part of my very soul, and no amount of time will ever take that away. As my Dads favorite,  Wiki__Zane_Grey__zossqe  Wiki__Zane_Grey__zossqe , once said: Recipe For Greatness: To bear up under loss; To fight the bitterness of defeat and the weakness of  Wiki__Grief__zeflqa ; To be victor over anger; To smile when tears are close; To resist disease and evil men and base instincts; To hate hate and to love love; To go on when it would seem good to die; To look up with unquenchable faith in something evermore about to be. That is what any man can do, and be great. We think we have all the time in the world, but we dont. We all will be gone in the  Wiki__Blink__lzsqqz  of an eye in this massive intergalactic timeline. The best we can do is be great to each other, and love each other, and support each other, because on this rock hurtling through space, all we have is each other, but I think thats enough.\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Jk1f0CxrSwvn","colab_type":"text"},"source":["### Preprocess the dataset"]},{"cell_type":"code","metadata":{"id":"udSK4sK2SyZX","colab_type":"code","colab":{}},"source":["regex_tokenizer = nltk.RegexpTokenizer(\"\\w+\")\n","\n","def normalize_text(text):\n","  # lowercase text\n","  text = str(text).lower()\n","  # remove non-UTF\n","  text = text.encode(\"utf-8\", \"ignore\").decode()\n","  # remove punctuation symbols\n","  text = \" \".join(regex_tokenizer.tokenize(text))\n","  return text\n","\n","def count_lines(filename):\n","  count = 0\n","  with open(filename) as fi:\n","    for line in fi:\n","      count += 1\n","  return count"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tfXOUlXFS5Ah","colab_type":"code","outputId":"3eb55bbf-aa21-4a38-f8d3-820d9ec27f6c","executionInfo":{"status":"ok","timestamp":1589216356378,"user_tz":300,"elapsed":21449,"user":{"displayName":"Eadhunath Venghatesan","photoUrl":"","userId":"10975244633908642389"}},"colab":{"base_uri":"https://localhost:8080/","height":390}},"source":["RAW_DATA_FPATH = \"/content/dataset.txt\" #@param {type: \"string\"}\n","PRC_DATA_FPATH = \"/content/proc_dataset.txt\" #@param {type: \"string\"}\n","\n","# apply normalization to the dataset\n","# this will take a minute or two\n","\n","total_lines = count_lines(RAW_DATA_FPATH)\n","bar = Progbar(total_lines)\n","\n","with open(RAW_DATA_FPATH,encoding=\"utf-8\") as fi:\n","  with open(PRC_DATA_FPATH, \"w\",encoding=\"utf-8\") as fo:\n","    for l in fi:\n","      fo.write(normalize_text(l)+\"\\n\")\n","      bar.add(1)\n","\n","!head -n 20 $PRC_DATA_FPATH"],"execution_count":19,"outputs":[{"output_type":"stream","text":["236118/236118 [==============================] - 18s 76us/step\n","source material is a wiki__podcast__ioeffse http www racistsandwich com episodes e whats so political about food photography w wiki__celesta__szals noche as a sjw liberal ill explain it for you in reply to the points you made as undeserving as this clusterfuck of a community is of my efforts im doing this partly because maybe some of you will listen because im a sucker for punishment and because it might be hilarious how you all reply also im bored why do these people ascribe malice to every little thing theres a problem with phrasing in social justice wiki__discourse__fsief at the moment coming from the top academic wiki__discourse__fsief to the bottom laymen who maybe read one article on social justice that one time theres a big disconnect not only in terminology but how to appropriately use that terminology like all things rooted in sociology and the wiki__humanities__seiez in general the wiki__layman__foaobi are given a media platform you dont see right leaning economists saying what needs to be done with the economy on fox news as often as you hear people like inheritance riding know nothing donald trump fumbling to explain economic concepts and getting a lot wrong while speaking so generally he seems to say nothing at all in the case of this article a similar thing is happening microaggressions the use of this word is always problematic because laymen dont understand that microaggressions arent necessarily aggressive or violent or anything like that yet the word implies that using this term is to be concise but in doing so many special snowflakes get personally offended at the mere mention of the word and react defensively im not a racist dont call me a racist im not being aggressive to people of color wtf but its because the word is probably a poor word to pick out for use here sure its useful in academic circles where someones sociology education allows them to understand what is meant by the use of the word as opposed laymen like yourselves who see the word aggression and just react to it whether its taking photos of dishes with wiki__chopsticks__babi sticking straight up into rice or noodles which can be seen as offensive in some asian cultures this section is another good example its seen as offensive within the culture if you are a white wiki__dude__eesleq hanging out in wiki__tokyo__eoosl and you do it people will just think you are weird i know i was there and one of my best friends for life lived there for years they wont revile and look at you as a racist or anything as she is seemingly implying but i think what she means is to show respect to different cultural conventions around the world i think most people could agree that if every single portrayal of americans was some guy waving a flag shooting a gun and yelling fuck all the time it would be a bit annoying shallow immature and obviously a very inaccurate generalization now it wouldnt matter that much because of the concept of privilege much like being called honky by a wiki__black_people__flfs you really dont care that for instance the show abenobashi mahou shotengai basically did exactly what i described if you see the scene it makes you giggle but you see japan didnt come in and demolish our country dictate our whole governmental rebuild in its systems etc everything has a sort of wiki__social_environment__ssalzz that it resides within the wiki__ethical_intuitionism__aaboeb wiki__intuition__knowledge___isfilo of you not really giving a wiki__shit__eazzi if someone calls you honky or whatever the wiki__fuck__sslslzz that the majority of wiki__white_people__ibbzoz likely have is the same kind of thing in the inverse if japanese people misrepresent wiki__united_states__efeflso culture whatever that is arguably even if its not a wholly authentic representation isnt showcasing food from other cultures generally a net positive i actually somewhat agree as a raving sjw libtard myself but this is the first step in a society of integrating elements of another culture into their own or raising their understanding of other cultures this is preliminary sure we had our wiki__lap_steel_guitar__siezib tiki obsession in the early to mid s but now that we know wiki__hawaii__iezlo is more than hilariously eating poke all day long or we ought to know since there is plenty of information everywhere using one element of their culture and a bunch of bastardized versions of their culture to get views on a video sell a product etc comes across as lazy uninformed and unnecessarily disrespectful to the source material many of my fellow liberals are having a blast riding the tiki revival in the wiki__cocktail__lsqq scene they wear traditional wiki__lap_steel_guitar__siezib garb knock offs incorrectly over wiki__lap_steel_guitar__siezib wiki__aloha_shirt__fsbazb which arent really wiki__lap_steel_guitar__siezib either and suck down drinks that white people made up in the s as their interpretation of what a wiki__lap_steel_guitar__siezib wiki__cocktail__lsqq would taste like now whats the harm you might ask well lets say for instance every time someone meets a wiki__lap_steel_guitar__siezib person they start asking about poke mai tais etc when the conversation could be about just regular things not infantilizing them by asking them pointed questions about strange bastardized ideas of their culture if we stop spreading pervasive myths or lies about certain cultural traits even by misrepresentation due to ignorance we can cut down on these minor annoyances things like movies where the two main characters are white people that travel to an exotic location and all the locals act in a very hammed up way that is dictated by the movies country of origins stereotypes that have been perpetuated this also potentially cuts down on lead role casting in relevant situations such as this which is better sharing a photo of inauthentic wiki__ph___bzssbs and exposing people to the concept so they can explore it for themselves or only sharing photos or wiki__united_states__efeflso food because you havent received an advanced degree in the culinary history of every non wiki__united_states__efeflso culture you might encounter this is called a wiki__false_dilemma__ilbal wiki__dichotomy__fqiabs you are acting as if these are the only two options or at least its implied in your incredulous wiki__rhetorical_question__fiieqq there are more options than just show other cultures food terribly and show wiki__united_states__efeflso burgers and wiki__united_states__efeflso wiki__pizza__zflba all day long theres also as the article that is linked here mentioned the idea that bringing someone in who is from that culture as an expert or taking the time to really understand what the culture is and learn before opening ones mouth or putting pen to the paper thats my rare comment in here dont think i have done one before au revoir you filthy scumbags\n","\n","i studied history in university and please do not ever read history textbooks to learn history it is taught this way in high school but now when i teach history and have to use textbooks for a specific exam or curriculum i just get so frustrated textbooks are almost always written in the format of a happened then b happened then c happened i dont understand why this is so because thats a timeline history is about context i have no idea how one can teach or learn history by memorizing dates without understanding contextual significance or those slow simmering historical currents that shape the wiki__human_condition__ibbzie so here are my thoughts as someone whose misconceptions of what history is led to her being bitchslapped by the demands of what history actually is in university and who ardently tries to steer current high school students and anyone else away from being similarly bitchslapped history is bias every book you read is a product of bias as long as something has to be interpreted in historys case the primary and secondary resources bias cannot be avoided keep this in mind as you choose books and read i went to a pretty leftist university and our readings were almost always by left leaning academics if you are interested in specific topics in history that are very sensitive to wiki__historiography__iezlb read as much as you can from different perspectives i always like to read the works cited bibliography first and see what the author used during research very important considerations are did the author use the canon or did the author only use the canon which is problematic in their field did the author use sources in the language of the region s he writes about i know this might sound really duh but i once read this scathing review of what i thought to be a great history book in my very narrow field of baltic studies because this author did everything right had great arguments but had no sources in estonian latvian or lithuanian if you want to get really nitpicky what are the years of these sources our professors would always ask us to use caution using wiki__secondary_source__ileiol written before 1960s also ask yourself what are the institutional affiliations of these writers do they usually operate under a marxist feminist post colonial perspective are they revisionist etc you dont always have to read a wiki__secondary_source__ileiol like a non fiction book in fact history is so much more exciting when you skip the middleman secondary sources and examine the wiki__primary_source__illaqi themselves if you study history because you want to relate to the past its always better in my opinion to actually relate yourself with the past not related to some historian writing about the past so many documents are now available in libraries and e libraries of various universities and government agencies read them for yourself and interpret what you think read the classics in the study of history people still debate what history is how it should be studied why it should be studied how it is supposed to be taught how it should be written this is a big one the more i studied about my tiny sub sub sub field of history the more i wanted to read about the study of history to understand the minutiae sustained in the cosmos i have a lot of books that i like but the one reading that changed my life how i saw all wiki__social_sciences__zblai and wiki__humanities__seiez life in general human existence is a very short but difficult reading by wiki__walter_benjamin__zzliii called thesis on the concept of history everything about history is summarized in the short piece i cannot see history as anything other than what he described regarding your question on favourite history books here is my short list of mostly authors ottoman salonica by wiki__mark_mazower__seeobao ussr eastern europe wiki__svetlana_alexievich__azolliq wiki__aleksandr_solzhenitsyn__ibzs wiki__anne_applebaum__failib wiki__david_remnick__eaezlz wiki__orlando_figes__zolabll peter pomerantsev wiki__orest_subtelny__blbissi ukrainian history ryszard kapuściński imperium some wiki__primary_source__illaqi about the russian revolution from wiki__leon_trotsky__ilaaa wiki__history_of_the_russian_revolution__iifaalfo sukhanovs personal record on the revolution lenins april thesis wiki__vladimir_lenin__iioiszsz wrote some shorter essays on ethnic issues in the ussr that are quite interesting alexandra kollontais the autobiography of a sexually emancipated communist woman if you need anything else pm me\n","\n","im going to speak agains the crowd and say dont attend cc unless you have to source ht hechingerreport org how often do community college students who get transfer get bachelors degrees go to a wiki__liberal_arts__iablf college or national wiki__university__eilqf that has a wiki__liberal_arts__iablf college approach wiki__johns__film___lsefqzs hopkins brown dartmouth etc these schools can offer you a myriad of resources and allow you to take up to two years before declaring a major unlike universities with different undergrad colleges northwestern cornell wiki__boston__massachusetts__feoq college these types of schools allow you to switch your major across the spectrum of the university without the need to apply to a new college\n","\n","i have numerous problems with this and i will outline them 1 wiki__adolf_hitler__zleisae is not regarded as a hero in austria the majority of austrians like most germans have accepted allied propaganda which blamed wiki__adolf_hitler__zleisae for the war and most of the genocide and i would appreciate a source for the statues 2 he did not want to unite the world he wanted to build a wiki__nazi_germany__ziziz which would last 1000 years and he would do it by accomplishing realistic goals rebuilding the economy regaining lost territory and expanding east and defeating the soviet union before it became too powerful yes he wanted europe to band together against the wiki__russians_and_americans__aqslqzf but he never considered uniting the world if you have a source where he suggested it i would like to see it 3 wiki__the_holocaust__ioeqblqe isnt a lie millions of minorities along with many others did die during wiki__world_war_ii__ezqzl just not the way most people said they did there were many cases of wiki__starvation__zisaqi disease being worked to death or executions saying the holocaust just didnt happen is like americans waving off the millions of germans who starved to death after the end of the war or britain ignoring the many cases of famine in india which killed countless millions wiki__the_holocaust__ioeqblqe wiki__the_holocaust__ioeqblqe was only intentional in some cases like wiki__heinrich_himmler__iefeb wiki__heinrich_himmler__iefeb wiki__reinhard_heydrich__zsqif wiki__reinhard_heydrich__zsqif the ss and leaders of concentration camps and the wiki__the_holocaust__ioeqblqe doesnt make the germans wrong but its ridiculous to ignore it but youre right that wiki__adolf_hitler__zleisae never ordered the holocaust nor did he have extensive knowledge of it 4 please be a little more respectful of women their role even when restricted to the family and housekeeping is very important to raising respectable citizens on top of that they can also serve other useful functions from agriculture to the arts where would we be without leni riefenstahls wiki__triumph_of_the_will__eooeq also please explain what you mean by women owning children with social funds 5 you really shouldnt laugh at piles of dead people no matter how bad they are as a group thats the reaction of sadistic criminals im not asking that you show compassion for them just that you take death a little more seriously im sorry if this looks like a rant im just trying to address everything and truly would like a response\n","\n","anything stem will be likely impacted at a uc school meaning a lot of people want to get into those majors but there arent enough spots as a result they only accept a certain number of applicants applying for the major at that school this leads people to strategize how they apply you can apply as a biology medicine tech major and hope you get in you can apply to an unimpacted major such as art history a language wiki__cultural_studies__sqbzs religion history etc which gives you a better chance of getting into the school this is difficult down the road though because you have to then switch into the major you want when you get to the school you might not get the major you want or you might not get the classes you need i wouldnt recommend applying undeclared and no problem happy to share my experience i was so confused about this stuff when i was in high school\n","\n","the major thing youll notice is the western focus on wiki__critical_thinking__zlbalz i have a bah from queens university in history and can tell you that things which would be very important in chinese education such as remembering specific dates and peoples names is important but will not get you a good grade by themselves to succeed in the wiki__humanities__seiez in north america or at least in canada you should be looking for the big picture or the story and adding in specific detail to support your main point if you just present bullet pointed facts that you learned in lecture but dont know how they relate and dont tell a good story youll get a c if you tell a good story but dont know the supporting details like names and dates youll get a b if you can tell a good story and support it with the minute details of names and dates youll get an a what professors are looking for you to do is not impress them by presenting them with the exact same facts they told you word for word which wont work but they want to know that you really understood what they were telling you and why they want you to use your own brain to draw your own conclusions i have gotten as on papers here where my essay was essentially i think the professor is wrong and here is why which is fine as long as you use logic evidence and research to back that up\n","\n","major lack i dont remember there ever being one but they were needs of the army for a while which really just means there was like a 3 000 bonus for signing up as one if youre really into learning and want to learn stuff heres the path to go demand as a requisite of signing to get a school called m6 mike 6 thats like an extra year of schooling on the government dime then take that and apply to a community college when you get to your unit and transfer all that into college credit use army tuition assistance program pays 75 of tuition for enlisted to do all your remaining gen ed and wiki__liberal_arts__iablf wiki__bullshit__fofaf classes then once you have your 2 year degree drop a packet for army physicians assistant program the army will straight up pay you e 5 base pay plus a housing allowance of probably about 1500 a month for a combined pay of more than 40 grand a year and pay your wiki__tuition__zfqezo just for you to go to school all day if you pass the accelerated course within the two years you get to be an officer and a pa and if you fail you get to be a medic that knows a wiki__lot_polish_airlines__ablfo and is like a walking aid station if you just want to skip past all that and get to the blood and guts join the navy be a wiki__hospital_corpsman__zzeelll\n","\n","like u positive_numbers i did my ba in wiki__linguistics__ilszb but took many of the same courses as cogsci students i am currently working on my msc in wiki__cognitive_science__sbzb wiki__cognitive_science__sbzb of language i had a different experience from u positive_numbers in that i enjoyed and felt challenged by the program but part of that may be because i went looking for challenges i worked as a research assistant part of a independent research practicum course conducted my own linguistic research as usra student and completed my honours wiki__dissertation__eoozis as the other students have said i really enjoyed child wiki__language_acquisition__iabif wiki__sociolinguistics__aqafz and wiki__psycholinguistics__ibosea in terms of language courses i had great experience taking italian with dr dangelo i took first and second year italian first year french which is in its own department and introductory wiki__sanskrit__zlbqa through religious studies several languages arent offered past first or second year mandarin japanese polish russian spanish but we have fairly strong italian and german programs you can also fulfill your language requirements by taking language courses outside of the wiki__linguistics__ilszb department you can take french from the french dept latin or greek classics hebrew or wiki__sanskrit__zlbqa religious studies i believe that religious studies is also offering arabic starting this summer or next fall as far as proficiency goes when studying a language you only get out of it what you put into it taking 2 4 semesters of a language isnt going to make you fluent but you can become conversational if you put in the effort as a cogsci student you may be less interested in becoming proficient in another language as you would be in figuring out how the language works whats similar between language x and other languages and how our brains process language either way it can be a useful endeavor u salientjuxe said it better than i could when discussing the stigma of being a wiki__humanities__seiez student cogsci and linguistics are fairly wiki__interdisciplinarity__iszoi programs and would be at home in wiki__humanities__seiez wiki__social_sciences__zblai wiki__social_sciences__zblai or science faculties as a cogsci student youll be studying neuroscience anatomy and psychology as well as the core linguistics courses dont forget linguistics is a science as for the comment that youll have to do a masters degree to get a decent paying job this may be true but it is definitely not constrained to linguistics or cogsci the fact is that more places are looking for advanced degrees and the skills you learn in a masters program can make you more employable in general people who choose cogsci as their undergraduate degree already have plans to go into slp in the future which automatically means a msc dont let this discourage you sorry about the novel i have a lot to say about this program feel free to pm me if you have any specific questions\n","\n","i highly recommend cornell notes for any wiki__humanities__seiez wiki__social_sciences__zblai lecture classes i cannot speak to the hard sciences i havent had one of those classes in ages\n","\n","amazing eulogy that i saved from here i think i completely forget who posted it though might spark some ideas for you my condolences for your loss so my mother is very irish catholic and when my dad passed a few days ago she wanted to have a catholic mass naturally i wanted to give my dad who was very non religious the respect he deserved wiki__the_priest__ieeqoebs gave his typical prepare for the lords wrath and ascent into heaven wiki__bullshit__fofaf in the middle of mass which really made me mad well this is the wiki__eulogy__zeiieo i gave from the wiki__pulpit__zsieol of a catholic church and i thought you all would appreciate it 32 years 2 weeks 6 days thats how long my dad has been present in my life hes been gone now for 5 days guess which one feels longer they say that the only two guarantees in life are death and taxes but there is also time time is what makes the universe work its why atoms spin and cells divide how elements are forged in the fires of a billion suns and spread across the galaxy in gigantic wiki__supernova__zlbao its time that gives us life grows with us from when we were babies to wiki__adult__ibleef and its time that takes it all away as my favorite geneticist once said we are going to die and that makes us the lucky ones most people are never going to die because they are never going to be born the potential people who could have been here in my place but who will in fact never see the light of day outnumber the sand grains of arabia certainly those unborn ghosts include greater poets than keats scientists greater than newton we know this because the set of possible people allowed by our dna so massively exceeds the set of actual people in the teeth of these stupefying odds it is you and i in our ordinariness that are here we privileged few who won the lottery of birth against all odds how dare we whine at our inevitable return to that prior state from which the vast majority have never stirred i would give anything for more time with my dad but all i can do now is continue to be the man he raised me to be he always taught me to be curious to love science and knowledge and question the universe he was never a man to ask for help he would figure it out on his own he taught me to love sound and music history photography and the arts of all ages but he also taught me about hard work and sacrifice he bought us our first computer and believe it or not taught me how to use windows if it was not for him i would have never opened those computer books he never discouraged me when i broke the damn thing he never really understood what the heck i did to them anyway but he always knew i would fix it eventually he taught me how to use my mind to do great things he taught me how to use my soul to feel great things and most importantly he taught me how to be a man time wont ever fix this hole in my life where he used to be but his memories and his lessons are a part of my very soul and no amount of time will ever take that away as my dads favorite wiki__zane_grey__zossqe wiki__zane_grey__zossqe once said recipe for greatness to bear up under loss to fight the bitterness of defeat and the weakness of wiki__grief__zeflqa to be victor over anger to smile when tears are close to resist disease and evil men and base instincts to hate hate and to love love to go on when it would seem good to die to look up with unquenchable faith in something evermore about to be that is what any man can do and be great we think we have all the time in the world but we dont we all will be gone in the wiki__blink__lzsqqz of an eye in this massive intergalactic timeline the best we can do is be great to each other and love each other and support each other because on this rock hurtling through space all we have is each other but i think thats enough\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jhTuGW4Pr_ZA","colab_type":"text"},"source":["Here we build a dictionary of all the wiki words and count the occurences of each of them.\n","\n","The dictionary is `wikis`.\n","We use RegularExpression parsing to get these statistics."]},{"cell_type":"code","metadata":{"id":"5r7UWgCsS9hO","colab_type":"code","colab":{}},"source":["import re\n","wikis = dict()\n","with open(PRC_DATA_FPATH, \"rt\") as f:\n","    for i, line in enumerate(f):\n","        some_ = re.findall('wiki__[a-zA-Z0-9_]+', line, re.MULTILINE)   # Find all words that start with wiki__\n","        for wiki in some_:\n","            if(wiki in wikis.keys()):\n","                wikis[wiki] += 1\n","            else:\n","                wikis[wiki] = 1\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2TvkI4pxUtEn","colab_type":"code","outputId":"8a3c271e-caa8-48d4-d977-5e0dbb7e2b5a","executionInfo":{"status":"ok","timestamp":1589219727708,"user_tz":300,"elapsed":323,"user":{"displayName":"Eadhunath Venghatesan","photoUrl":"","userId":"10975244633908642389"}},"colab":{"base_uri":"https://localhost:8080/","height":195}},"source":["len(wikis.keys()) # Around 78597 -> Number of unique wiki words\n","\n","# Puth these in a dataframe for easy access and use.\n","import pandas as pd\n","df = pd.DataFrame(list(wikis.items()))\n","df = df.sort_values(1, ascending=False)\n","df.rename(columns = {0:'Wikified Words', 1:\"Frequency\"}, inplace = True)\n","df.head()"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Wikified Words</th>\n","      <th>Frequency</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>41</th>\n","      <td>wiki__liberal_arts__iablf</td>\n","      <td>43117</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>wiki__humanities__seiez</td>\n","      <td>42139</td>\n","    </tr>\n","    <tr>\n","      <th>176</th>\n","      <td>wiki__academic_term__sfssll</td>\n","      <td>19185</td>\n","    </tr>\n","    <tr>\n","      <th>100</th>\n","      <td>wiki__whitelist__iazebq</td>\n","      <td>15910</td>\n","    </tr>\n","    <tr>\n","      <th>112</th>\n","      <td>wiki__professor__iblaoo</td>\n","      <td>11946</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                  Wikified Words  Frequency\n","41     wiki__liberal_arts__iablf      43117\n","3        wiki__humanities__seiez      42139\n","176  wiki__academic_term__sfssll      19185\n","100      wiki__whitelist__iazebq      15910\n","112      wiki__professor__iblaoo      11946"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"OzR4iMQPVV7X","colab_type":"code","outputId":"5a797fa2-9a18-408d-ca37-1337e97dc698","executionInfo":{"status":"ok","timestamp":1589219729649,"user_tz":300,"elapsed":303,"user":{"displayName":"Eadhunath Venghatesan","photoUrl":"","userId":"10975244633908642389"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["def most_frequent_wiki_words(n : int):\n","    return set(df.iloc[:n, 0])\n","\n","most_frequent_n_wiki_words = most_frequent_wiki_words(WIKI_WORDS_IN_VOCAB)\n","print(most_frequent_n_wiki_words)"],"execution_count":25,"outputs":[{"output_type":"stream","text":["{'wiki__wikipedia__sofelef', 'wiki__fuck__sslslzz', 'wiki__doctor_of_philosophy__alls', 'wiki__direct_image_functor__eozzlff', 'wiki__student_financial_aid__iiloqaa', 'wiki__theres__ioisibsz', 'wiki__accountability__ibiqle', 'wiki__social_sciences__zblai', 'wiki__dude__eesleq', 'wiki__barack_obama__sefebb', 'wiki__essay__ioelf', 'wiki__humanities__seiez', 'wiki__shit__eazzi', 'wiki__gender_studies__saafq', 'wiki__wiki__ezasi', 'wiki__university__eilqf', 'wiki__whitelist__iazebq', 'wiki__sake__zasoo', 'wiki__twitter__qqaaial', 'wiki__scholarship__iaoeel', 'wiki__california__sfol', 'wiki__natural_science__eaaqo', 'wiki__tuition__zfqezo', 'wiki__ideology__lsfbq', 'wiki__donald_trump__fafazlz', 'wiki__academic_senate__ezieiiq', 'wiki__fine_art__qoeil', 'wiki__capitalism__sfib', 'wiki__standardized_test__zzsqli', 'wiki__literacy__iafsb', 'wiki__homework__ezbsfb', 'wiki__students__union__siqqoi', 'wiki__tutor__zqqqbs', 'wiki__stem_fields__efelbbe', 'wiki__interdisciplinarity__iszoi', 'wiki__lol__zaaelq', 'wiki__student_loan__fbfqqo', 'wiki__portable_network_graphics__zfeob', 'wiki__mathematics__iaaei', 'wiki__rhetoric__zsffl', 'wiki__unemployment__eilfi', 'wiki__curriculum__flqqae', 'wiki__blog__eebfs', 'wiki__accountancy__zsqe', 'wiki__seminar__bllooo', 'wiki__idiot__isoof', 'wiki__undergraduate_education__zilzaq', 'wiki__journalism__isqza', 'wiki__hypertext_transfer_protocol_over_secure_socket_layer__iesab', 'wiki__professor__iblaoo', 'wiki__labour_economics__iaila', 'wiki__facebook__lszqela', 'wiki__state_school__sleqee', 'wiki__critical_thinking__zlbalz', 'wiki__bullshit__fofaf', 'wiki__wow__recording___ibfzoss', 'wiki__social_media__saqllfz', 'wiki__yale_university__efzle', 'wiki__textbook__zsfioa', 'wiki__non_profit_organization__lzfal', 'wiki__liberal_arts__iablf', 'wiki__ivy_league__ifqls', 'wiki__political_science__zfeaa', 'wiki__laptop__iqasaf', 'wiki__liberal_arts_college__iabiz', 'wiki__carnegie_mellon_university__faoqe', 'wiki__podcast__ioeffse', 'wiki__electronic_learning__lfbll', 'wiki__mechanical_engineering__iqsza', 'wiki__study_abroad__iabeiff', 'wiki__php__zfiei', 'wiki__fascism__iiosf', 'wiki________sieaq', 'wiki__dissertation__eoozis', 'wiki__permalink__alaqzb', 'wiki__harvard_university__iafzbsoi', 'wiki__biology__qizlbez', 'wiki__feminism__iiias', 'wiki__youtube__eszflbb', 'wiki__computer_science__seze', 'wiki__graduate_school__efbbza', 'wiki__intern__zqazzi', 'wiki__syllabus__iqllzas', 'wiki__europe__qzeq', 'wiki__marxism__iqofose', 'wiki__russia__zseqi', 'wiki__electrical_engineering__qsei', 'wiki__work_experience__iiifloe', 'wiki__graphics_interchange_format__izloz', 'wiki__chemistry__siao', 'wiki__empathy__eozeiq', 'wiki__arts_integration__qqebfel', 'wiki__jpeg__ibooq', 'wiki__working_time__ssziba', 'wiki__united_states__efeflso', 'wiki__academic_term__sfssll', 'wiki__e_mail__qlea', 'wiki__sexism__zlibs', 'wiki__reddit__eazqoos', 'wiki__tenure__eifqsi'}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"j4ONZhcDs_Wa","colab_type":"text"},"source":["! IMPORTANT\n","\n","This cell takes care of the wikification and de-wikification of the words.\n","\n","If a wiki word occurs in `most_frequent_n_wiki_words`, then the wikification is changed as :\n","\n","wiki__chicago__identifier -> wikichicagoidentifier\n","\n","If a wiki word does NOT occur in `most_frequent_n_wiki_words`, then the wikification is changed as :\n","\n","wiki__chicago__identifier -> chicago\n","\n","wiki__illiois_tech__jargons ->illinois tech\n"]},{"cell_type":"code","metadata":{"id":"wNc8PFWTWDW7","colab_type":"code","colab":{}},"source":["'''\n","with open(PRC_DATA_FPATH, \"rt\") as f:\n","    data1 = f.read()\n","    data1 = data1.lower()\n","    words1 = data1.split()\n","\n","new_set_of_words = list()\n","for i, word in enumerate(words1):\n","\n","    if word in most_frequent_n_wiki_words:\n","        new_set_of_words.append(word.replace(\"_\", \"\"))\n","\n","    else:\n","        # if not most frequent : remove _ and wiki__ and __jargon\n","        if word[:6] == 'wiki__':\n","            new_set_of_words.append(word[6:word.rfind(\"__\")].replace(\"_\", \" \"))\n","        else:\n","            new_set_of_words.append(word)\n","\n","    if i % 1000 == 0:\n","        new_set_of_words.append(\"\\n\")\n","\n","# print(len(words1))\n","# print(len(new_set_of_words))\n","\n","new_data = ' '.join([str(elem) for elem in new_set_of_words])\n","with open('new_dataset.txt', \"w\") as f:\n","    f.write(new_data)\n","'''"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1DBfpQx_uwfp","colab_type":"text"},"source":["This cell is for the smart selection of the data from the corpus.\n","\n","We only choose to include those lines that contain the most frequently occuring wiki-words in them. The other lines are ignored.\n","\n","This is done so that the we can be sure that BERT is learning something meaninful in every line and not just being trainined on plain english."]},{"cell_type":"code","metadata":{"id":"YGi_6IkZWLni","colab_type":"code","colab":{}},"source":["with open(PRC_DATA_FPATH, \"rt\") as f:\n","    with open(SUBSAMPLED_DATA_FPATH, \"a\") as s:\n","        for i, line in enumerate(f):\n","            if(line == \"\\n\"):\n","                s.write(\"\\n\")\n","                continue\n","            wiki_in_line = set(re.findall('wiki__[a-zA-Z0-9_]+', line, re.MULTILINE))\n","            if most_frequent_n_wiki_words & wiki_in_line:\n","                # if in most frequent : remove _'s from the word and\n","                # Put this in a new file\n","                    for j, word in enumerate(line.split(\" \")):\n","                        if word in most_frequent_n_wiki_words:\n","                            s.write(word.replace(\"_\", \"\")+ \" \")\n","\n","                        else:\n","                            # if not most frequent : remove _ and wiki__ and __jargon\n","                            if word[:6] == 'wiki__':\n","                                s.write(word[6:word.rfind(\"__\")].replace(\"_\", \" \")+ \" \")\n","                            else:\n","                                s.write(word + \" \")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pOuitGgVXdRq","colab_type":"code","outputId":"d8c8ccf1-b46e-492a-e720-94864da1d0c0","executionInfo":{"status":"ok","timestamp":1588218218584,"user_tz":300,"elapsed":72932,"user":{"displayName":"Eadhunath Venghatesan","photoUrl":"","userId":"10975244633908642389"}},"colab":{"base_uri":"https://localhost:8080/","height":390}},"source":["!head -n 21 /content/subsampled.txt"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","if trump really fucks things up then sure i could see more than just the left getting involved your people have been making every little thing seems like an apocalyptic event anything trump does is spun into something racism wikisexismzlibs homophobic anti semite anti islam etc the fact that you keep calling people nazism is hilariously pathetic the everyday wikiunitedstatesefeflso sees how your peaceful protests turn out how you silence anyone with an opposing calling them racist instead of having a thoughtful discussion the left is alienating itself into the fringe all that will be remaining are blue haired fat gender fluid highway blocking welfare dependent wikiliberalartsiablf degree educated flat out failures who do nothing for the greater good or even try to consider another perspective seriously look at the type of people who take part in all the protests and scream the loudest i would be so ashamed to even be remotely associated with that it is so crazy how the crap the left claims about everyone else is self projecting how are we all nazism when trump took a clear stance supporting israel supports gays supports gun rights the left is way more nazi like marching around physically attacking opponents calling to purge this country embracing and promoting islam btw muslim believe in killing jew in case you havent heard and silencing opposing views by all means i am very open to changing my views but the insanity tactics have got to stop if someone could calmly and respectfully lay out the reason why their opinion is better and have a constructive debate then i believe way more people would listen\n"," \n","\n","\n","conservatives support classical economic liberalism ideals that better belong in the late 19th century and social ideals that belong in the 1930s that better its very very slightly better but youre still editorializing to put your own opinion on the topic which youre allowed to hold but when describing what other people think its a little bit more genuine to describe their views as they would proffer them i believe i mean which statement would you prefer progressives support policies that aim toward equality using law and social programs to help even out differences caused by social class and historical oppression and conflict and social ideals that enable people who dont fit historical norms to be accepted or progressive support economic ideals that steal money from hard workers to pay for lazy shiftless people and enable gays to ghillean prance about in public while black people steal our women i mean clearly youre going to prefer the former and it would stand to reason that you would only bother having a discussion with a conservative willing to give your viewpoints a fair treatment and then disagree with them in a methodical fashion rather than just relying on invective if you prefer that you have to do the same thing in return as for your specific points please keep in mind i do not identify as a conservative but im familiar with many of their views so let me play devil s advocate and suggest what an open minded conservative might respond with they dont believe in utilizing the state to make the economy better the conservative view on this is that the state doesnt do a very good job of meddling with the economy that stimulus packages and bail out  finance  encourage poorly run businesses to continue being shitty that make work projects that pay people are just adhesive bandage that will never result in long term stable wealth creating employment they generally want government out if everything this comes from the view that government involvement in everything is the path toward communism which has been shown to be impossible to implement and that it tries to replace the natural self correcting tendencies of a full price discovering free market with rational business processes that are inevitably slow and unintelligent and cannot respond to supply and demand changes as quickly as private industry can i havent met many conservative in canada that truly want there to be no government regulation theyre in favour of building code environmental standards fisheries and hunting laws and so forth they just want that to remain as a framework within which private industries compete to provide better service rather than putting it all onto the government which most people can agree is notoriously slow and inefficient i myself like crown corporations for a variety of things which is where i find myself in disagreement with a lot of conservatives they seem to me to be a good blend of private industry mentality with government oversight and have historically done a decent job in many sectors unless its stopping people from using drugs i think its safe to say this can be blamed on a feedback cycle of propaganda that stems from many sources at least part of this mentality is the fear of what drugs can do to a community surely you dont think heroin methamphetamine and crack have done good things for the country do you some of it also stems from a lack of knowledge about what different drugs can do look at how lysergic acid diethylamide is portrayed in the media for an example of just how disingenuous we are in all of our public conversations about drugs ive successfully argued for cannabis legalization and regulation with some very conservative people when you frame the conversation as a three pronged effort to fight organized crime reduce harm via exposure to children and the gateway effect that comes from knowing illegal drug trade who can get other substance and create jobs via an actual wealth producing industry that will pay its fair share of taxes well you can have some good results when you frame the conversation as fuck the man ill do what i want all drugs should be legal then youre going to be treated as a petulant child incapable of seeing a bigger piture this ideology has been tried and it fails miserably every time it is implemented has it been im not sure that theres ever actually been an attempt at conservative government in canada in the last century do keep in mind that the progressive conservatives aka the tories of old do not qualify for this they were socially progressive to some extent again lagging 20 years behind the curve to pander to their voting block and economically keynesian with a slightly frugal bent im not sure this qualifies as what im talking about the reform party mergers influence didnt help any all it did is further that pandering to the religious right in order to keep the votes coming in i submit that we havent tried anything really conservative or right wing in canada weve never really had any nationalistic tendencies and despite the ostensible authority of the pm weve almost never had a leader that people rallied behind in any sense of actual social union behind a power structure canada is a very left wing place relative to the rest of the world our overton window is entirely left of the usa for example putting our conservatives left of their democrats and weve actually only shifted leftward for the entire time that the country has existed economic policy that is obsessed with benefiting those who are successful and damn those who arent i think it might be more fair to characterize it as economic policy that aims to provide people with the chance to become successful but requires them to put in their own hard work and sacrifice to get there and has little pity for anyone not willing to put in the effort this can be cruel some people are simply incapable of the effort required whether because of a handicap of some kind or sheer lack of opportunity i personally believe we do need to temper this to some extent with social programs designed to help prevent people from completely hitting bottom however theres some sense in recognizing that coddling people too much is dangerous for society overall and that it will lead to problems in the future an entrepreneurial spirit is essential to a growing nation and thats the antithesis of making it possible for people to live in their parents basements until theyre 30 while taking wikiliberalartsiablf courses that will never result in gainful employment\n"," \n","\n","\n","and then they bitch on wikiredditeazqoos about how they cant afford their student loans after studying at some ludicrously expensive wikiliberalartsiablf college that nobody cares about i dont know anymore most people will say wikiliberalartsiablf but when you actually look and apply for jobs most job applications say bachelors degree not degree in business degree in wikicomputerscienceseze degree in engineering there are of course exceptions but for the most part employers just want to know you got a degree any degree stem itself isnt frivolous its stemlordism dismissal of the wikihumanitiesseiez thats trite and economics as a field is pretty much just propaganda justifying the pro percent agenda it isnt a science and it isnt neutral im far enough left of bernie that he isnt really my guy the wikihumanitiesseiez arts philosophy and critical theory stemlords act like a paycheck is the sole role of an education didnt lead to a firing if i remember correctly but an arts teacher at our school was a heavy smoker art room was on third floor of the building too far for a coffee break so in the short breaks or while we were futzing around trying to paint something shed sneak out into the corridor and have a smoke she had an ashtray which she stored in the art supplies cupboard outside of the arts room yup the supplies cupboard with all the paint thinner brush cleaning stuff etc so one day one happy little accident later were all standing outside the school while the local fire brigade is trying to put out the blaze caused by her half extinguished cigarette in her little ashtray whenever these threads come up i wonder if its some wizard looking for genuine answers so they can pick people to give a do over to for me id own myself more year old me was a deeply closeted and kinda weird guy id probably still be closeted cuz lets be real still wasnt a great time to be out and proud id get my license sooner it wasnt a priority for me but i realize now how much better things couldve been if i had had my own vehicle before my s id not let myself be talked out of pursuing my original career path my parents thought i wasnt smart enough at the sciences to go into the medical field in any form encouraged me to go after english and the wikihumanitiesseiez i ended up starting and english degree and hating it and then getting a degree in health science anyways theres not a whole lot else id change with my current knowledge id probably be more jaded about things and how the world works but i wouldnt really change much getting a wikiliberalartsiablf degree colossal waste of money not at the weight i want to be at im studying law at uni people my age i grew up with are living lives i envy i lack discipline girls dont like me im shy i wanna be a millionare but statistically thats hard im a creative person in the arts but im confined to office life i mean math is a liberal arts major the ba is a bachelor of arts though the point of wikiliberalartsiablf is that you have a variety to learn from a former coworker of mine was kicked out of the navy for drug possession so i believe you from what ive heard it sounds like military parties can definitly be much crazier than the parties at my small wikiliberalartsiablf school oh man my mum is amazing libraries and the arts would get so much more funding there would be a minimum living wage for everyone one tax code for the country rather than multiple and if people wanted to be tax exemption they would have to explain it in person to her with a timer so they dont go overboard she also gets the final say on every law and they explained in a way that makes sense to her over a bottle of wine of course it is addressed on a visit but as is said by spoonsiest that is not always part of the process and even if it is you need to get to that point in the wikihumanitiesseiez and even in some upper echelon stem fields you are accepted to work with a specific group or individual professor for a ph d it is very important that your interest fit theirs and they are not looking at hundreds of applications in fact i would say start before the formal application process and write directly to the person you with whom you hope to work it is very different that applying to a generic masters program in biology nseriously please dont give general advice based on your field assuming other fields operate in the same way me too friend me too earth needs that unobtonium fuck the aliens humanities survival is at stake there are no worthless degrees n its not all about a high paying career path in engineering sorry n college is not a vocational school a person with an art degree share percent common elements as any other wikiliberalartsiablf degree skills and knowledge all degrees share about percent in common in what they learn n college does not need to cost a lot of money and require loans i could i did years of humanities but im a software engineering some good tech writing courses would improve my writing a lot im far too verbose if you find a school with a more theory oriented computer science department youll have professors who are closer to mathematicians than engineers those places tend to have a lot more respect for the humanities and might fit your personality better stem is really engineering and some other things to make us engineers look modest mathematicians and scientists dont much care for the term and attendant elitism i find especially mathematicians theyre basically philosophers at heart also dual majors are an option i have a friend who did cs humanities its generally very difficult but we both do different things im in the wikihumanitiesseiez and hes in the wikisocialscienceszblai he was hired first and i kind of had to be a plus one for a little while but now were both doing well in our fields so its working out for everyone it also helps that were at a university that likes to support families couples and partners happier employees better retention rates cost effective in the long run usually science and maths related subjects humanities are typically by word this makes sense because the two are pretty much not related whatsoever you shouldnt expect any more from a computer engineering major than you would a wikiliberalartsiablf major when it comes to changing a computer theme not fan of buzzfeed but a big fan of humanities i was about during spring break one day i played d amp d all day online with my buddies they had to go i was exhausted but i decided to make a chat room in the arts and entertainment section in aol chat on a whim it was entitled guy in here this was weird for me because i never went into chats i met a bunch of teenage girls one seemed really cool we continued to chat it turns out she was a couple of years older she was in high school and had a car and lived minutes away we continued to chat and eventually met we watched pearl pearl harbor when it came out in theaters we fell in love a couple of years later my dad died she was there to pull me out of a crippling depression and urged me to take a damn shower and get a job i did we moved in together i worked really really hard in retail she got a nonretail job and suggested to get boss she hire me she did that started a career that has lead to me owning my own successful business we got married we bought a house we had kids because i decided on a whim to make a chat room i met my future wife i was not destroyed by my fathers death i run my own successful business and people exist who otherwise would not exist it blows my mind to think about it the adult ed classes are taught at our local wikiliberalartsiablf wikiliberalartscollegeiabiz tcu and are fairly affordable i think the sewing class is under dollar for seven of eight two clock face on sessions theyre not for credit hours more just for people to learn new things ive never taken any of them but have a friend who has and shes really enjoyed them it definitely had a huge blow to my self confidence i always wonder if im secretly annoying people or if im overweight sometimes i worry people dont really want to hang with me and so it takes a while to build trust that they seriously want to be my friend and wont drop me im definitely kinder to people because of it and have always tried to reach out to people who are marginalized in groups and help them to feel more like they belong most of the time theyre just nice weirdos with social anxiety anxiety sometimes theyre genuinely awful but thats rare i downplay my talents a lot more i used to just do my thing at the arts and not really think about what others thought i was just trying my hardest at something i loved but some people got jealous when i got leads in plays or awards for art so now i hide my light under a bushel until my skill is needed for something like im a professional artist but some people wont even know it because i dont mention it keanu reeves went to my high school for a short amount of time and came back to do a motivational speech about following a career in the arts it was an arts high school he shook my hand and told me he liked my hair which boosted my self esteem more than anything else in the world i met romanian wine diesel at my elementary school while they were filming the pacifier apparently i had no clue who he was but insisted that he give me a piggyback ride so that i could be u cadult tall u d i went to several of the same bar bar and bat mitzvah u s as justin bieber and he was a dick back then too percent i met michael cera and jay baruchel at a party get together and got to hang out with them for a little bit they were really cool guys and were really nice to me despite the age difference i saw them a couple of times since then and they always remember me which is the best feeling in the world north carolina has decent state schools but still not everyone can get a scholarship i guess dollar k is really not much to some people especially if youre looking at it as an investment it just seems like such a setback for someone who might be making dollar k or so right out of college with a wikiliberalartsiablf degree college is becoming more of an expectation than it used to be and it doesnt seem sustainable for so many low middle class people to start out with such a large debt adolf hitler was originally the artsy type so he could probably do a mean interpretive dance ya if i dont write stuff down ill forget im really bad with remembering names as well in college the programming courses amp math courses were ez mode i could knock out the work really quick and get back to cs dust or go to the bar or play snes read only memory the arts ya those smacked me around something fierce this is probably something i should get tested for but im in my mid s and am not sure the juice is worth the squeeze no idea what her name was i only attended my wikiliberalartsiablf classes i didnt make friends in them i went to a wikiliberalartsiablf college and majored in dance a lot of people thought it was pointless and sometimes i even feel weird as an adult saying to people that i have a degree in dance it can be hard to make money with an arts career but if its something you are truly passionate about and believe in there are plenty of opportunities you just have to be open creative and dedicated i learned so much and grew tremendously as a person from my years in school more than i believe i would have if i went to a university and majored in something more conventional i am now working as a dance instructor at a major university in douglas fir if i had to apply right now id most likely apply as a cs major at the engineering school this makes me a bit nervous because i know the arts and sciences school is pretty easy to get into but with my grades and test scores the engineering school wont quite be a reach but definitely not a safety im hoping worst case i get offered to do the pre engineering program then switch into the engineering school sophomore year thanks for the reply ive yet to hear bad things about this school from students alums my mom has a degree in wikihumanitiesseiez and has been teaching th grade for years i u m in a wikiliberalartsiablf honors program at my state school and run into these people all the time in my classes they u re not bad people or anything a lot of them are just so fixated on their own perceived u cgreatness u d that they u re not afraid to say super pseudo intellectual things that really aren u t very profound and that always bugs me any good wikisocialscienceszblai or wikihumanitiesseiez or arts grad program will fund their graduates and the research they do is rigorous and there are jobs for people with those qualifications if you have to take out significant loans to pay for wikigraduateschoolefbbza youre probably not a very good candidate and you should probably reevaluate whether you have what it takes to really contribute to the body of knowledge in your field but to call everyone doing a non stem grad program prolonging their childhood shows an extremely dismissive attitude and a great misunderstanding of the role that the arts wikihumanitiesseiez wikisocialscienceszblai play in our society we dont talk as frequently as we used to maybe once every two months over the phone he started buying into the whole business is a way of life gimmick his dad kept trying to sell to him it was a weird transformation to see because he was the artsy musical type who wanted to stick it to the man now whenever i see him all he ever talks about are profit margins and new business ventures this will vary heavily university to university and department to department my observations anecdotal is that stem professors were worse at teaching and wikihumanitiesseiez were best wikisocialscienceszblai were somewhere in between that being said i went to a small wikiliberalartsiablf school and my professors were great overall even my worst wikiprofessoriblaoo and he was horrible was still better than half of my high school teachers and i went to a good district p s regardless never use a bad wikiprofessoriblaoo as an excuse you cant make the professor better at teaching but you can work harder and adapt to them make the best out of the class you have rather than wishing for a hypothetical better class i got extra credit for it because im in a wikiliberalartsiablf program at my school and we just finished the race unit not a lot of policy are you shitting me on january th dt said that he would cut funding for the national endowment for the arts on january th dt said that he would cut funding for the national endowment for the wikihumanitiesseiez on january th dt said that he would cut funding for the corporation for public broadcasting on january th dt said that he would cut funding for the minority business development agency on january th dt said that he would cut funding for the economic development administration on january th dt said that he would cut funding for the international trade administration on january th dt said that he would cut funding for the manufacturing extension partnership on january th dt said that he would cut funding for the office of community oriented policing services on january th dt said that he would cut funding for the legal services corporation on january th dt said that he would cut funding for the united states department of justice civil rights division of the doj on january th dt said that he would cut funding for the environmental and natural resource division of the doj on january th dt said that he would cut funding for the overseas private investment company on january th dt said that he would cut funding for the un intergovernmental panel on climate change on january th dt said that he would cut funding for the office of electricity deliverability and energy reliability on january th dt said that he would cut funding for the office of energy efficiency and renewable energy on january th dt said that he would cut funding for the office of fossil fuel on january th dt ordered all regulatory powers of all federal agencies frozen on january th dt ordered the national parks service to stop using wikisocialmediasaqllfz after rting factual side by side photos of the crowds for the and inaugurations on january th roughly protestors were arrested in dc and face unprecedented felony riot charges among them were legal observers journalists and medics on january th a member of the international workers of the world was shot in the stomach at an anti fascism protest in seattle he remains in medical conditions on january st dt brought a group of cheerleaders to a meeting with the cia to cheer for him during a speech that consisted almost entirely of framing himself as the victim of dishonest press on january st white house press secretary sean spicer held a news conference largely to attack the press for accurately reporting the size of attendance at the inaugural festivities saying that the inauguration had the largest audience of any in history u cperiod u d on january nd white house advisor kellyann conway defended spicer u s lies as u calternative facts u d on national television news on january nd dt appeared to blow a kiss to director james comey during a meeting with the fbi and then opened his arms in a gesture of strange paternal affection before hugging him with a pat on the back on january rd dt reinstated the global gag order which defunds international organization that even mention abortion as a medical option on january rd spicer said that the us will not tolerate china u s expansion onto islands in the south china sea essentially threatening war with china on january rd dt repeated the lie that million people voted u cillegally u d thus costing him the popular vote on january rd it was announced that the man who shot the anti fascism protester in seattle was released without charges despite turning himself in on january th spicer reiterated the lie that million people voted u cillegally u d thus costing dt the popular vote on january th dt tweeted a picture from his personal wikitwitterqqaaial account of a photo he says depicts the crowd at his inauguration and will hang in the white house press room the photo is curiously dated january st the day after the inauguration and the day of the women u s march the largest inauguration related protest in history on january th the epa was ordered to stop communicating with the public through wikisocialmediasaqllfz or the press and to freeze all grants and contracts on january th the usda was ordered to stop communicating with the public through wikisocialmediasaqllfz or the press and to stop publishing any papers or research all communication with the press would also have to be authorized and vetted by the white house on january th hr a bill that would prohibit federal funding not only to abortion service providers but to any insurance coverage including medicaid that provides abortion coverage went to the floor of the house for a vote on january th director of the department of health and human service nominee tom price characterized federal guidelines on transgender equality as u cabsurd u d on january th dt ordered the resumption of construction on the dakota access pipeline while the north dakota congress of nuevo le n considers a bill that would legalize hitting and killing protestors with cars if they are on roadways on january th it was discovered that police officers had used confiscated mobile phone to search the emails and messages of the demonstrators now facing felony riot charges for protesting on january th including lawyers and journalists whose email accounts contain privileged information of clients and sources on january th dt announced that he would be signing an executive order banning travel from six predominantly muslim majority countries on january th dt announced plans to build a border wall with mexico this was going to be paid for by mexico on january th dt was asked in an interview whether mexico would pay for the wall he told the interviewer that mexico would reimburse americans for the wall on january th it was revealed that half of dt u s white house staff used private email addresses on an rnc server on january th four senior members of the state department left the state department the administration claimed that they were asked to leave others reported that they resigned on january th it was revealed that dt u s wikitwitterqqaaial account was linked to an unsecured gmail account on january th the alternative wikitwitterqqaaial account that the national parks service had set up to continue tweeting scientific facts was turned over to non nps employees because of u clegal ramifications u d and the threat of u ccriminal prosecution u d on january th it was revealed that ivanka trump was voter registration in two different states as was her husband jared kushner this follows one day after the revelation that steve bannon dt u s chief strategist is also voter registration in two states on january th dt ordered the weekly publication of a list of all crimes committed by immigrants on january th dt ordered that all the united states environmental protection agency u s scientific studies must go through political review before their results are released to the press on january th dt cut all advertising for obamacare u advertising that the wikibarackobamasefebb administration had already paid for u five days before the end of the open enrollment period on january th propublica reported that by a spread trump voters believe that dt should have a private email server my university a wikiliberalartsiablf wikiliberalartscollegeiabiz is one of two big universities in the state the other one focuses more on agribusiness and stem fields my university has recently been cutting a ton of wikiliberalartsiablf programs to become more like the other school apparently not seeing that if people wanted a school like the other one theyd just go to the other one attendance has been tanking and they cut winter break short to try and curb dropout rates not understanding that thats not the problem or something good idea in theory because improving the university is great but isnt working because theyre doing the exact opposite of what they should be doing to achieve that elon musk seems to have the world and humanities benefit on his mind so probably him and if they dont have the money to play for a private organization a lot of sports in the us are still pay to play outside of schools i was fortunate to be brought up in a well off household but to play competitive level sports the dues can reach a few thousand dollars per season and that is very unattainable for a great number of kids who play school sports sheffer stroke this is not directed at you just a theme ive seen there are a lot of people in this thread who say reduce sports and increase the arts what makes the arts more important than sports and how would the money sports can bring in be replaced meet the cooper  profession  i finally understood the intense hate for white people that permeates today those fucks were awful nothing worse than a family full of wikiliberalartsiablf majors from the northeast see im the opposite i dont get art very little of it has actually impressed me with the skill of the artist and dont even get me started about abstract art i feel like a lot of the arts music etc are only popular because someone influential decided they should be not because theyre particularly better yeah thats what im saying but the graphics and the arts were out of this world for that time and the story was not so bad either nothing mind blowingly innovative but it kept me entertained until the credits i attended a school of people half of those kids didnt give a shit percent werent particularly bright percent were wikihumanitiesseiez types percent had some niche talent like art percent were math types i was average in the math types so i was in the top percent of math in my school this means im doing average in my field but i was told i was gifted for years i see what youre getting at but do you mean a generic wikiliberalartsiablf degree or a specialized one im working on a final paper and i realize that i suck at english im horrible at grammar and im not good in translating ideas on complex plays and books into original words and ideas if i didnt need a wikiliberalartsiablf elective i wouldnt take one liberal arts can i take your order please my vacation and all the artsy pictures i hope to take while on it only thing is that among the humanities its one of the hardest and most complicated and a lot of is not all that different than mathematics at the same time you get none of the job potential and useful skill sets that youd get from a similarly difficult major basically its a joke major which is actually in reality very difficult and has a huge workload not the greatest combination\n"," \n","the ignorance and distain that americans have toward native wikiunitedstatesefeflso history is appalling i recall being in a high level literary criticism class at a prestigious university where the wikiprofessoriblaoo did a very clever thing one day the lecture and discussion was on jew literature in post ww2 germany and the next day was on native american authors and poets during the trail of tears watching the very same students who were literally crying about anne frank and making impassioned speeches about villainous germany and the horrors of genocide one day then laughing at native culture and speaking in me hum injun running deer saying stuff like they should be glad that we brought civilization to them when were talking about the attempted extermination and subjugation of an entire continent by us it was shocking to watch which was of course the entire point of the lecture once the professor pointed out the crazy double standard and that the majority of the students were classic good germans people couldnt process being effectively punkd and flipped out becoming super defensive and yelling crazy shit like you cant compare indians to jews and this again was in a graduate level literature class a top tier wikiliberalartsiablf college in one of the most liberal states in america i learned a lot about people that week it was depressing as fuck coolstorybro edit a lot of people seem to want to play the prestigious college oneupmanship game which is silly i stress that point for i think pretty obvious reasons\n"," \n","\n","hey u xed randon thank you for your submission unfortunately it has been because your submission must be a direct link to an image or an album of images please feel free to re submit using a wikidirectimagefunctoreozzlff link must end in wikijpegibooq wikigraphicsinterchangeformatizloz wikiportablenetworkgraphicszfeob etc wikiblogeebfs wikisocialmediasaqllfz and similar types of websites will not be approved as per rule also if your link ends in please remove that and anything else extraneous before submitting also we just started working on a wikiwhitelistiazebq for legitimate reputable news sites related to the arts world if your post links to such a website please send us a modmail so that we can add it to the wikiwhitelistiazebq again no blogs thanks for understanding lt i am a bot and this action was performed automatically please contact the moderators of this subreddit message compose to r art if you have any questions or concerns\n"," \n","\n","nah the winner is going to be some wikiliberalartsiablf major who accidentally yoloed correctly six or seven times\n"," \n","lincoln land community college s student newspaper the lamp was honored with 13 awards including the top staff award and reporter of the year at the illinois community college wikijournalismisqza association awards banquet april    winning first place in the mike foster general excellence award was gratifying said tim mckenzie adviser of the lamp and wikiprofessoriblaoo of wikijournalismisqza and wikihumanitiesseiez it shows the whole staff did great work covering the campus and consistently putting out a top notch publication lamp editor ryan wilson ofauburn pictured left was named reporter of the year based on one reporter s ability to use a range of styles to cover a variety of topics ryan is a great writer and leader as editor said mckenzie he goes above and beyond at every turn this year has been a chance to rebuild and grow the newspaper we would not have made the strides we did without ryan the judge noted ryan wilson s writing and reporting abilities are at an extraordinary skill level and he clearly stands out amongst his community college journalism peers wilson knows what it takes to produce worthwhile journalism and he has a bright future ahead of him wilson also won first place in the news story category and both first and second place in the sports feature category and shared a third place honor for news story of the year with rebecca lange ofspringfield and jordan minder  tv series  ofrochester minder also placed third and received an honorable mention in features photo other winners included ryan mazrim ofchatham second place news column and third place news photo dominique lamp ofspringfield third place features story alisha kirkley ofspringfield honorable mention graphics and brennan stidham  oklahoma ofspringfield honorable mention news story more information on the lamp is available from mckenzie at 786 4656 or timothy mckenzie llcc edu\n"," \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jMDU0QnS4Y-k","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"2ece1585-44c7-4138-a93a-33509538b4f2","executionInfo":{"status":"ok","timestamp":1589219967397,"user_tz":300,"elapsed":1896,"user":{"displayName":"Eadhunath Venghatesan","photoUrl":"","userId":"10975244633908642389"}}},"source":["# Words to be added to the `vocab.txt`\n","for word in most_frequent_n_wiki_words:\n","    print(word.replace(\"_\", \"\"))"],"execution_count":28,"outputs":[{"output_type":"stream","text":["wikiwikipediasofelef\n","wikifucksslslzz\n","wikidoctorofphilosophyalls\n","wikidirectimagefunctoreozzlff\n","wikistudentfinancialaidiiloqaa\n","wikitheresioisibsz\n","wikiaccountabilityibiqle\n","wikisocialscienceszblai\n","wikidudeeesleq\n","wikibarackobamasefebb\n","wikiessayioelf\n","wikihumanitiesseiez\n","wikishiteazzi\n","wikigenderstudiessaafq\n","wikiwikiezasi\n","wikiuniversityeilqf\n","wikiwhitelistiazebq\n","wikisakezasoo\n","wikitwitterqqaaial\n","wikischolarshipiaoeel\n","wikicaliforniasfol\n","wikinaturalscienceeaaqo\n","wikituitionzfqezo\n","wikiideologylsfbq\n","wikidonaldtrumpfafazlz\n","wikiacademicsenateezieiiq\n","wikifineartqoeil\n","wikicapitalismsfib\n","wikistandardizedtestzzsqli\n","wikiliteracyiafsb\n","wikihomeworkezbsfb\n","wikistudentsunionsiqqoi\n","wikitutorzqqqbs\n","wikistemfieldsefelbbe\n","wikiinterdisciplinarityiszoi\n","wikilolzaaelq\n","wikistudentloanfbfqqo\n","wikiportablenetworkgraphicszfeob\n","wikimathematicsiaaei\n","wikirhetoriczsffl\n","wikiunemploymenteilfi\n","wikicurriculumflqqae\n","wikiblogeebfs\n","wikiaccountancyzsqe\n","wikiseminarbllooo\n","wikiidiotisoof\n","wikiundergraduateeducationzilzaq\n","wikijournalismisqza\n","wikihypertexttransferprotocoloversecuresocketlayeriesab\n","wikiprofessoriblaoo\n","wikilaboureconomicsiaila\n","wikifacebooklszqela\n","wikistateschoolsleqee\n","wikicriticalthinkingzlbalz\n","wikibullshitfofaf\n","wikiwowrecordingibfzoss\n","wikisocialmediasaqllfz\n","wikiyaleuniversityefzle\n","wikitextbookzsfioa\n","wikinonprofitorganizationlzfal\n","wikiliberalartsiablf\n","wikiivyleagueifqls\n","wikipoliticalsciencezfeaa\n","wikilaptopiqasaf\n","wikiliberalartscollegeiabiz\n","wikicarnegiemellonuniversityfaoqe\n","wikipodcastioeffse\n","wikielectroniclearninglfbll\n","wikimechanicalengineeringiqsza\n","wikistudyabroadiabeiff\n","wikiphpzfiei\n","wikifascismiiosf\n","wikisieaq\n","wikidissertationeoozis\n","wikipermalinkalaqzb\n","wikiharvarduniversityiafzbsoi\n","wikibiologyqizlbez\n","wikifeminismiiias\n","wikiyoutubeeszflbb\n","wikicomputerscienceseze\n","wikigraduateschoolefbbza\n","wikiinternzqazzi\n","wikisyllabusiqllzas\n","wikieuropeqzeq\n","wikimarxismiqofose\n","wikirussiazseqi\n","wikielectricalengineeringqsei\n","wikiworkexperienceiiifloe\n","wikigraphicsinterchangeformatizloz\n","wikichemistrysiao\n","wikiempathyeozeiq\n","wikiartsintegrationqqebfel\n","wikijpegibooq\n","wikiworkingtimessziba\n","wikiunitedstatesefeflso\n","wikiacademictermsfssll\n","wikiemailqlea\n","wikisexismzlibs\n","wikiredditeazqoos\n","wikitenureeifqsi\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"h5wlTB0xbxO6","colab_type":"text"},"source":["### Create Pre-training Data"]},{"cell_type":"markdown","metadata":{"id":"qZiLHMlowbh2","colab_type":"text"},"source":["Before doing this, we need to make sure to download the vocab.txt from the [Google BERT Repository](https://github.com/google-research/bert#pre-trained-models). In this vocab.txt, we need to replace the required number of `[UNUSED_N]` words with these frequently occuring wiki-words.\n","\n","It is essential that this is done before we create pre-training data.\n","\n","Also make sure that the name of this vocab file is `vocab.txt`"]},{"cell_type":"code","metadata":{"id":"Ibf_5qQZtXTF","colab_type":"code","outputId":"ebe7aca4-1f65-44a6-9c26-af97eb9a0faa","executionInfo":{"status":"ok","timestamp":1588218457851,"user_tz":300,"elapsed":3539,"user":{"displayName":"Eadhunath Venghatesan","photoUrl":"","userId":"10975244633908642389"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["!mkdir ./shards_new\n","\n","# Split the data file into shards so that the data chunks fit in the RAM.\n","# Feel free to modify the arguments as per requirement. \n","# More information about the command here : http://man7.org/linux/man-pages/man1/split.1.html\n","!split -a 4 -l 250000 -d $SUBSAMPLED_DATA_FPATH ./shards_new/shard_\n","!ls ./shards_new/"],"execution_count":0,"outputs":[{"output_type":"stream","text":["shard_0000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wSTewLoGcMYW","colab_type":"code","colab":{}},"source":["MAX_SEQ_LENGTH = 128 #@param {type:\"integer\"}\n","MASKED_LM_PROB = 0.15 #@param\n","MAX_PREDICTIONS = 20 #@param {type:\"integer\"}\n","DO_LOWER_CASE = True #@param {type:\"boolean\"}\n","PROCESSES = 2 #@param {type:\"integer\"}\n","PRETRAINING_DIR = \"pretraining_data\" #@param {type:\"string\"}\n","VOC_FNAME = \"/content/vocab.txt\" # This is the same vocab file talked about earlier"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kXlGw5lScGnt","colab_type":"code","colab":{}},"source":["XARGS_CMD = (\"ls ./shards_new/ | \"\n","             \"xargs -n 1 -P {} -I{} \"\n","             \"python3 bert/create_pretraining_data.py \"\n","             \"--input_file=./shards_new/{} \"\n","             \"--output_file={}/{}.tfrecord \"\n","             \"--vocab_file={} \"\n","             \"--do_lower_case={} \"\n","             \"--max_predictions_per_seq={} \"\n","             \"--max_seq_length={} \"\n","             \"--masked_lm_prob={} \"\n","             \"--random_seed=34 \"\n","             \"--dupe_factor=5\")\n","\n","XARGS_CMD = XARGS_CMD.format(PROCESSES, '{}', '{}', PRETRAINING_DIR, '{}', \n","                             VOC_FNAME, DO_LOWER_CASE, \n","                             MAX_PREDICTIONS, MAX_SEQ_LENGTH, MASKED_LM_PROB)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"H6VgW8vFeRaq","colab_type":"code","outputId":"ac063ad9-1fbc-4008-d355-f27b2d4a1f98","executionInfo":{"status":"ok","timestamp":1588219854593,"user_tz":300,"elapsed":1395789,"user":{"displayName":"Eadhunath Venghatesan","photoUrl":"","userId":"10975244633908642389"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["tf.gfile.MkDir(PRETRAINING_DIR)\n","!$XARGS_CMD"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From bert/create_pretraining_data.py:469: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n","\n","WARNING:tensorflow:From bert/create_pretraining_data.py:437: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n","\n","W0429 22:47:42.883340 140629406877568 module_wrapper.py:139] From bert/create_pretraining_data.py:437: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n","\n","WARNING:tensorflow:From bert/create_pretraining_data.py:437: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n","\n","W0429 22:47:42.883568 140629406877568 module_wrapper.py:139] From bert/create_pretraining_data.py:437: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n","\n","WARNING:tensorflow:From /content/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n","\n","W0429 22:47:42.883748 140629406877568 module_wrapper.py:139] From /content/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n","\n","WARNING:tensorflow:From bert/create_pretraining_data.py:444: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n","\n","W0429 22:47:42.966245 140629406877568 module_wrapper.py:139] From bert/create_pretraining_data.py:444: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n","\n","WARNING:tensorflow:From bert/create_pretraining_data.py:446: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n","\n","W0429 22:47:42.969016 140629406877568 module_wrapper.py:139] From bert/create_pretraining_data.py:446: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n","\n","INFO:tensorflow:*** Reading from input files ***\n","I0429 22:47:42.969339 140629406877568 create_pretraining_data.py:446] *** Reading from input files ***\n","INFO:tensorflow:  ./shards_new/shard_0000\n","I0429 22:47:42.969530 140629406877568 create_pretraining_data.py:448]   ./shards_new/shard_0000\n","INFO:tensorflow:*** Writing to output files ***\n","I0429 23:09:23.355903 140629406877568 create_pretraining_data.py:457] *** Writing to output files ***\n","INFO:tensorflow:  pretraining_data/shard_0000.tfrecord\n","I0429 23:09:23.356559 140629406877568 create_pretraining_data.py:459]   pretraining_data/shard_0000.tfrecord\n","WARNING:tensorflow:From bert/create_pretraining_data.py:101: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n","\n","W0429 23:09:23.357280 140629406877568 module_wrapper.py:139] From bert/create_pretraining_data.py:101: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n","\n","INFO:tensorflow:*** Example ***\n","I0429 23:09:23.358709 140629406877568 create_pretraining_data.py:149] *** Example ***\n","INFO:tensorflow:tokens: [CLS] [MASK] very fuck i am in pain [UNK] [MASK] ##rist what a nightmare di ma ##r [MASK] ed ##a got to put [MASK] [MASK] [MASK] [MASK] want to relax like me ##rca ##do what do south am ##eric ##ans play when for [MASK] wikieuropeqzeq has euro ##pe [MASK] championship qualifiers assuming they [MASK] all qualified already for the cop ##a am ##eric ##a [SEP] class while taking notes and what not this [MASK] something men ##ial [MASK] his brain [MASK] do allowing [MASK] to pay attention now i believe he [MASK] more on the h ##yper ##active [MASK] but if [MASK] ##e brain is over active having some other stimulation could help in [MASK] regard i get that you got a your first wikiacademictermsfssll Dodgers that [SEP]\n","I0429 23:09:23.358959 140629406877568 create_pretraining_data.py:151] tokens: [CLS] [MASK] very fuck i am in pain [UNK] [MASK] ##rist what a nightmare di ma ##r [MASK] ed ##a got to put [MASK] [MASK] [MASK] [MASK] want to relax like me ##rca ##do what do south am ##eric ##ans play when for [MASK] wikieuropeqzeq has euro ##pe [MASK] championship qualifiers assuming they [MASK] all qualified already for the cop ##a am ##eric ##a [SEP] class while taking notes and what not this [MASK] something men ##ial [MASK] his brain [MASK] do allowing [MASK] to pay attention now i believe he [MASK] more on the h ##yper ##active [MASK] but if [MASK] ##e brain is over active having some other stimulation could help in [MASK] regard i get that you got a your first wikiacademictermsfssll Dodgers that [SEP]\n","INFO:tensorflow:input_ids: 101 103 1304 9367 178 1821 1107 2489 100 103 12937 1184 170 12178 4267 12477 1197 103 5048 1161 1400 1106 1508 103 103 103 103 1328 1106 10482 1176 1143 23433 2572 1184 1202 1588 1821 26237 5443 1505 1165 1111 103 53 1144 27772 3186 103 2899 23504 11577 1152 103 1155 4452 1640 1111 1103 9947 1161 1821 26237 1161 102 1705 1229 1781 3697 1105 1184 1136 1142 103 1380 1441 2916 103 1117 3575 103 1202 3525 103 1106 2653 2209 1208 178 2059 1119 103 1167 1113 1103 177 24312 19667 103 1133 1191 103 1162 3575 1110 1166 2327 1515 1199 1168 23842 1180 1494 1107 103 7328 178 1243 1115 1128 1400 170 1240 1148 3 13839 1115 102\n","I0429 23:09:23.359183 140629406877568 create_pretraining_data.py:161] input_ids: 101 103 1304 9367 178 1821 1107 2489 100 103 12937 1184 170 12178 4267 12477 1197 103 5048 1161 1400 1106 1508 103 103 103 103 1328 1106 10482 1176 1143 23433 2572 1184 1202 1588 1821 26237 5443 1505 1165 1111 103 53 1144 27772 3186 103 2899 23504 11577 1152 103 1155 4452 1640 1111 1103 9947 1161 1821 26237 1161 102 1705 1229 1781 3697 1105 1184 1136 1142 103 1380 1441 2916 103 1117 3575 103 1202 3525 103 1106 2653 2209 1208 178 2059 1119 103 1167 1113 1103 177 24312 19667 103 1133 1191 103 1162 3575 1110 1166 2327 1515 1199 1168 23842 1180 1494 1107 103 7328 178 1243 1115 1128 1400 170 1240 1148 3 13839 1115 102\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","I0429 23:09:23.359354 140629406877568 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","I0429 23:09:23.359500 140629406877568 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","INFO:tensorflow:masked_lm_positions: 1 9 17 23 24 25 26 43 48 53 73 77 80 83 91 98 101 114 125 0\n","I0429 23:09:23.359601 140629406877568 create_pretraining_data.py:161] masked_lm_positions: 1 9 17 23 24 25 26 43 48 53 73 77 80 83 91 98 101 114 125 0\n","INFO:tensorflow:masked_lm_ids: 3205 22572 190 1115 1283 1299 178 1859 1389 1874 1522 1111 1106 1140 1108 1334 1240 1184 1133 0\n","I0429 23:09:23.359712 140629406877568 create_pretraining_data.py:161] masked_lm_ids: 3205 22572 190 1115 1283 1299 178 1859 1389 1874 1522 1111 1106 1140 1108 1334 1240 1184 1133 0\n","INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n","I0429 23:09:23.359936 140629406877568 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n","INFO:tensorflow:next_sentence_labels: 1\n","I0429 23:09:23.360035 140629406877568 create_pretraining_data.py:161] next_sentence_labels: 1\n","INFO:tensorflow:*** Example ***\n","I0429 23:09:23.360615 140629406877568 create_pretraining_data.py:149] *** Example ***\n","INFO:tensorflow:tokens: [CLS] year [MASK] new office had opened regarding harassment i think but irrational ##s about it one of my classics [MASK] ##fs actually [MASK] to deal with a complaint about transgender ##ism or [MASK] such thing [MASK] we were discussing [MASK] ##ree ##k and r ##oman attitudes and [MASK] they ##re shown in myth [MASK] handled [MASK] [MASK] well saying he was [MASK] if [SEP] ##y h ##t [MASK] washing ##ton ##time ##s com e ##ws a ##p ##r [MASK] praised nuclear option when demo [MASK] did it wikibarackobamasefebb living rent free in the tiny little heads of the pathetic right [MASK] ok the other side has a lot of sa ##ne people that de ##press ##uri [MASK] [MASK] insanity that comes from older family or because [SEP]\n","I0429 23:09:23.360857 140629406877568 create_pretraining_data.py:151] tokens: [CLS] year [MASK] new office had opened regarding harassment i think but irrational ##s about it one of my classics [MASK] ##fs actually [MASK] to deal with a complaint about transgender ##ism or [MASK] such thing [MASK] we were discussing [MASK] ##ree ##k and r ##oman attitudes and [MASK] they ##re shown in myth [MASK] handled [MASK] [MASK] well saying he was [MASK] if [SEP] ##y h ##t [MASK] washing ##ton ##time ##s com e ##ws a ##p ##r [MASK] praised nuclear option when demo [MASK] did it wikibarackobamasefebb living rent free in the tiny little heads of the pathetic right [MASK] ok the other side has a lot of sa ##ne people that de ##press ##uri [MASK] [MASK] insanity that comes from older family or because [SEP]\n","INFO:tensorflow:input_ids: 101 1214 103 1207 1701 1125 1533 4423 17514 178 1341 1133 27447 1116 1164 1122 1141 1104 1139 18046 103 22816 2140 103 1106 2239 1114 170 12522 1164 20141 1863 1137 103 1216 1645 103 1195 1127 10751 103 8871 1377 1105 187 27085 15149 1105 103 1152 1874 2602 1107 12849 103 8630 103 103 1218 2157 1119 1108 103 1191 102 1183 177 1204 103 13445 1633 4974 1116 3254 174 10732 170 1643 1197 103 5185 4272 5146 1165 11238 103 1225 1122 60 1690 9795 1714 1107 1103 4296 1376 4075 1104 1103 18970 1268 103 21534 1103 1168 1334 1144 170 1974 1104 21718 1673 1234 1115 1260 11135 8212 103 103 23186 1115 2502 1121 2214 1266 1137 1272 102\n","I0429 23:09:23.361020 140629406877568 create_pretraining_data.py:161] input_ids: 101 1214 103 1207 1701 1125 1533 4423 17514 178 1341 1133 27447 1116 1164 1122 1141 1104 1139 18046 103 22816 2140 103 1106 2239 1114 170 12522 1164 20141 1863 1137 103 1216 1645 103 1195 1127 10751 103 8871 1377 1105 187 27085 15149 1105 103 1152 1874 2602 1107 12849 103 8630 103 103 1218 2157 1119 1108 103 1191 102 1183 177 1204 103 13445 1633 4974 1116 3254 174 10732 170 1643 1197 103 5185 4272 5146 1165 11238 103 1225 1122 60 1690 9795 1714 1107 1103 4296 1376 4075 1104 1103 18970 1268 103 21534 1103 1168 1334 1144 170 1974 1104 21718 1673 1234 1115 1260 11135 8212 103 103 23186 1115 2502 1121 2214 1266 1137 1272 102\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","I0429 23:09:23.361176 140629406877568 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","I0429 23:09:23.361325 140629406877568 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","INFO:tensorflow:masked_lm_positions: 2 12 20 23 33 36 40 48 51 54 56 57 62 68 79 85 101 117 118 0\n","I0429 23:09:23.361423 140629406877568 create_pretraining_data.py:161] masked_lm_positions: 2 12 20 23 33 36 40 48 51 54 56 57 62 68 79 85 101 117 118 0\n","INFO:tensorflow:masked_lm_ids: 170 1115 5250 1125 1199 1165 176 1293 2602 1119 1122 1304 2959 7001 2394 22292 1157 3171 1103 0\n","I0429 23:09:23.361528 140629406877568 create_pretraining_data.py:161] masked_lm_ids: 170 1115 5250 1125 1199 1165 176 1293 2602 1119 1122 1304 2959 7001 2394 22292 1157 3171 1103 0\n","INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n","I0429 23:09:23.361636 140629406877568 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n","INFO:tensorflow:next_sentence_labels: 1\n","I0429 23:09:23.361724 140629406877568 create_pretraining_data.py:161] next_sentence_labels: 1\n","INFO:tensorflow:*** Example ***\n","I0429 23:09:23.362133 140629406877568 create_pretraining_data.py:149] *** Example ***\n","INFO:tensorflow:tokens: [CLS] websites will not be approved as per rule also if your link [MASK] in please remove that and anything else extra ##ne ##ous [MASK] [MASK] ##ting also we just started working on a wikiwhitelistiazebq for legitimate re ##put ##able news sites related to the arts world if your [MASK] links to [MASK] a website please send us a m ##od ##mail so that [SEP] us [MASK] committee of the j ##ew ##ish community of he ##bro ##n and hopes they can make us [MASK] [MASK] away from [MASK] home and our own communities Witness both of us freshman immediately start to s ##tu [MASK] out that were not j ##ew ##ish populations fact ironic ##ally i [MASK] the only other [MASK] at ##heist [MASK] we [MASK] [SEP]\n","I0429 23:09:23.362295 140629406877568 create_pretraining_data.py:151] tokens: [CLS] websites will not be approved as per rule also if your link [MASK] in please remove that and anything else extra ##ne ##ous [MASK] [MASK] ##ting also we just started working on a wikiwhitelistiazebq for legitimate re ##put ##able news sites related to the arts world if your [MASK] links to [MASK] a website please send us a m ##od ##mail so that [SEP] us [MASK] committee of the j ##ew ##ish community of he ##bro ##n and hopes they can make us [MASK] [MASK] away from [MASK] home and our own communities Witness both of us freshman immediately start to s ##tu [MASK] out that were not j ##ew ##ish populations fact ironic ##ally i [MASK] the only other [MASK] at ##heist [MASK] we [MASK] [SEP]\n","INFO:tensorflow:input_ids: 101 12045 1209 1136 1129 4092 1112 1679 3013 1145 1191 1240 5088 103 1107 4268 5782 1115 1105 1625 1950 3908 1673 2285 103 103 1916 1145 1195 1198 1408 1684 1113 170 4 1111 11582 1231 16156 1895 2371 3911 2272 1106 1103 3959 1362 1191 1240 103 6743 1106 103 170 3265 4268 3952 1366 170 182 5412 14746 1177 1115 102 1366 103 3914 1104 1103 179 5773 2944 1661 1104 1119 12725 1179 1105 7816 1152 1169 1294 1366 103 103 1283 1121 103 1313 1105 1412 1319 3611 26874 1241 1104 1366 12141 2411 1838 1106 188 7926 103 1149 1115 1127 1136 179 5773 2944 6623 1864 22564 2716 178 103 1103 1178 1168 103 1120 25811 103 1195 103 102\n","I0429 23:09:23.362477 140629406877568 create_pretraining_data.py:161] input_ids: 101 12045 1209 1136 1129 4092 1112 1679 3013 1145 1191 1240 5088 103 1107 4268 5782 1115 1105 1625 1950 3908 1673 2285 103 103 1916 1145 1195 1198 1408 1684 1113 170 4 1111 11582 1231 16156 1895 2371 3911 2272 1106 1103 3959 1362 1191 1240 103 6743 1106 103 170 3265 4268 3952 1366 170 182 5412 14746 1177 1115 102 1366 103 3914 1104 1103 179 5773 2944 1661 1104 1119 12725 1179 1105 7816 1152 1169 1294 1366 103 103 1283 1121 103 1313 1105 1412 1319 3611 26874 1241 1104 1366 12141 2411 1838 1106 188 7926 103 1149 1115 1127 1136 179 5773 2944 6623 1864 22564 2716 178 103 1103 1178 1168 103 1120 25811 103 1195 103 102\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","I0429 23:09:23.362619 140629406877568 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","I0429 23:09:23.362755 140629406877568 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","INFO:tensorflow:masked_lm_positions: 13 24 25 49 52 66 84 85 88 94 98 104 107 112 117 119 121 124 126 0\n","I0429 23:09:23.362871 140629406877568 create_pretraining_data.py:161] masked_lm_positions: 13 24 25 49 52 66 84 85 88 94 98 104 107 112 117 119 121 124 126 0\n","INFO:tensorflow:masked_lm_ids: 3769 1196 12295 2112 1216 1106 1631 7236 1412 1184 12141 20809 1127 1107 1899 1178 12141 1105 1238 0\n","I0429 23:09:23.362973 140629406877568 create_pretraining_data.py:161] masked_lm_ids: 3769 1196 12295 2112 1216 1106 1631 7236 1412 1184 12141 20809 1127 1107 1899 1178 12141 1105 1238 0\n","INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n","I0429 23:09:23.363074 140629406877568 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n","INFO:tensorflow:next_sentence_labels: 1\n","I0429 23:09:23.440640 140629406877568 create_pretraining_data.py:161] next_sentence_labels: 1\n","INFO:tensorflow:*** Example ***\n","I0429 23:09:23.442040 140629406877568 create_pretraining_data.py:149] *** Example ***\n","INFO:tensorflow:tokens: [CLS] [MASK] in wikijpegibooq wikigraphicsinterchangeformatizloz wikiportablenetworkgraphicszfeob etc wikiblogeebfs wikisocialmediasaqllfz and similar types [MASK] websites will not be approved as per rule also [MASK] your link ends in please remove that and [MASK] [MASK] extra ##ne ##ous before ##grave ##ting also we just started working on a [MASK] for legitimate re ##put ##able news sites related to the arts world if [MASK] [MASK] links to [SEP] my [MASK] just dispute sending me stories on messenger about how whole ##some the k ##k ##k [MASK] when they started and myths about the flags of [MASK] con ##fe [MASK] ##ate states of am ##eric ##a she coached me in the arts of passive aggressive behavior communication [MASK] i finally just said stop sending [MASK] this wikishiteazzi and she un ##friend [SEP]\n","I0429 23:09:23.442297 140629406877568 create_pretraining_data.py:151] tokens: [CLS] [MASK] in wikijpegibooq wikigraphicsinterchangeformatizloz wikiportablenetworkgraphicszfeob etc wikiblogeebfs wikisocialmediasaqllfz and similar types [MASK] websites will not be approved as per rule also [MASK] your link ends in please remove that and [MASK] [MASK] extra ##ne ##ous before ##grave ##ting also we just started working on a [MASK] for legitimate re ##put ##able news sites related to the arts world if [MASK] [MASK] links to [SEP] my [MASK] just dispute sending me stories on messenger about how whole ##some the k ##k ##k [MASK] when they started and myths about the flags of [MASK] con ##fe [MASK] ##ate states of am ##eric ##a she coached me in the arts of passive aggressive behavior communication [MASK] i finally just said stop sending [MASK] this wikishiteazzi and she un ##friend [SEP]\n","INFO:tensorflow:input_ids: 101 103 1107 15 13 14 3576 9 8 1105 1861 3322 103 12045 1209 1136 1129 4092 1112 1679 3013 1145 103 1240 5088 3769 1107 4268 5782 1115 1105 103 103 3908 1673 2285 1196 11886 1916 1145 1195 1198 1408 1684 1113 170 103 1111 11582 1231 16156 1895 2371 3911 2272 1106 1103 3959 1362 1191 103 103 6743 1106 102 1139 103 1198 7287 5416 1143 2801 1113 17957 1164 1293 2006 11743 1103 180 1377 1377 103 1165 1152 1408 1105 21557 1164 1103 14870 1104 103 14255 8124 103 2193 2231 1104 1821 26237 1161 1131 8692 1143 1107 1103 3959 1104 14403 9233 4658 4909 103 178 1921 1198 1163 1831 5416 103 1142 22 1105 1131 8362 23630 102\n","I0429 23:09:23.442530 140629406877568 create_pretraining_data.py:161] input_ids: 101 103 1107 15 13 14 3576 9 8 1105 1861 3322 103 12045 1209 1136 1129 4092 1112 1679 3013 1145 103 1240 5088 3769 1107 4268 5782 1115 1105 103 103 3908 1673 2285 1196 11886 1916 1145 1195 1198 1408 1684 1113 170 103 1111 11582 1231 16156 1895 2371 3911 2272 1106 1103 3959 1362 1191 103 103 6743 1106 102 1139 103 1198 7287 5416 1143 2801 1113 17957 1164 1293 2006 11743 1103 180 1377 1377 103 1165 1152 1408 1105 21557 1164 1103 14870 1104 103 14255 8124 103 2193 2231 1104 1821 26237 1161 1131 8692 1143 1107 1103 3959 1104 14403 9233 4658 4909 103 178 1921 1198 1163 1831 5416 103 1142 22 1105 1131 8362 23630 102\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","I0429 23:09:23.442727 140629406877568 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","I0429 23:09:23.442930 140629406877568 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","INFO:tensorflow:masked_lm_positions: 1 12 19 22 31 32 37 46 60 61 65 66 68 74 82 92 95 113 120 0\n","I0429 23:09:23.443070 140629406877568 create_pretraining_data.py:161] masked_lm_positions: 1 12 19 22 31 32 37 46 60 61 65 66 68 74 82 92 95 113 120 0\n","INFO:tensorflow:masked_lm_ids: 1322 1104 1679 1191 1625 1950 12295 4 1240 2112 1139 4113 1408 1164 1108 1103 2692 1105 1143 0\n","I0429 23:09:23.443204 140629406877568 create_pretraining_data.py:161] masked_lm_ids: 1322 1104 1679 1191 1625 1950 12295 4 1240 2112 1139 4113 1408 1164 1108 1103 2692 1105 1143 0\n","INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n","I0429 23:09:23.443340 140629406877568 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n","INFO:tensorflow:next_sentence_labels: 1\n","I0429 23:09:23.443455 140629406877568 create_pretraining_data.py:161] next_sentence_labels: 1\n","INFO:tensorflow:*** Example ***\n","I0429 23:09:23.444089 140629406877568 create_pretraining_data.py:149] *** Example ***\n","INFO:tensorflow:tokens: [CLS] second ##ed although i have no background in stem related [MASK] i think it would still be a [MASK] ##ly en ##light [MASK] [MASK] id love to know what he [MASK] like as just [MASK] regular wikidudeeesleq i think his [MASK] ##hil ##oso ##phi ##es and [MASK] ##ets would be very inspiring even if i couldn ##t [MASK] directly to his field [SEP] the wikiliberalartsiablf college aren u t [MASK] the ranking there [MASK] also [MASK] wikiliberalartsiablf category which is stupid [SEP]\n","I0429 23:09:23.444259 140629406877568 create_pretraining_data.py:151] tokens: [CLS] second ##ed although i have no background in stem related [MASK] i think it would still be a [MASK] ##ly en ##light [MASK] [MASK] id love to know what he [MASK] like as just [MASK] regular wikidudeeesleq i think his [MASK] ##hil ##oso ##phi ##es and [MASK] ##ets would be very inspiring even if i couldn ##t [MASK] directly to his field [SEP] the wikiliberalartsiablf college aren u t [MASK] the ranking there [MASK] also [MASK] wikiliberalartsiablf category which is stupid [SEP]\n","INFO:tensorflow:input_ids: 101 1248 1174 1780 178 1138 1185 3582 1107 8175 2272 103 178 1341 1122 1156 1253 1129 170 103 1193 4035 4568 103 103 25021 1567 1106 1221 1184 1119 103 1176 1112 1198 103 2366 38 178 1341 1117 103 20473 22354 27008 1279 1105 103 6248 1156 1129 1304 21964 1256 1191 178 1577 1204 103 2626 1106 1117 1768 102 1103 1 2134 4597 190 189 103 1103 5662 1175 103 1145 103 1 4370 1134 1110 4736 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","I0429 23:09:23.444419 140629406877568 create_pretraining_data.py:161] input_ids: 101 1248 1174 1780 178 1138 1185 3582 1107 8175 2272 103 178 1341 1122 1156 1253 1129 170 103 1193 4035 4568 103 103 25021 1567 1106 1221 1184 1119 103 1176 1112 1198 103 2366 38 178 1341 1117 103 20473 22354 27008 1279 1105 103 6248 1156 1129 1304 21964 1256 1191 178 1577 1204 103 2626 1106 1117 1768 102 1103 1 2134 4597 190 189 103 1103 5662 1175 103 1145 103 1 4370 1134 1110 4736 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","I0429 23:09:23.444572 140629406877568 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","I0429 23:09:23.444709 140629406877568 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","INFO:tensorflow:masked_lm_positions: 11 19 23 24 31 35 41 47 58 70 74 76 0 0 0 0 0 0 0 0\n","I0429 23:09:23.444802 140629406877568 create_pretraining_data.py:161] masked_lm_positions: 11 19 23 24 31 35 41 47 58 70 74 76 0 0 0 0 0 0 0 0\n","INFO:tensorflow:masked_lm_ids: 3872 3321 4777 3771 1116 170 185 10089 8681 1107 1110 170 0 0 0 0 0 0 0 0\n","I0429 23:09:23.444922 140629406877568 create_pretraining_data.py:161] masked_lm_ids: 3872 3321 4777 3771 1116 170 185 10089 8681 1107 1110 170 0 0 0 0 0 0 0 0\n","INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n","I0429 23:09:23.445025 140629406877568 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n","INFO:tensorflow:next_sentence_labels: 1\n","I0429 23:09:23.445112 140629406877568 create_pretraining_data.py:161] next_sentence_labels: 1\n","INFO:tensorflow:*** Example ***\n","I0429 23:09:23.445493 140629406877568 create_pretraining_data.py:149] *** Example ***\n","INFO:tensorflow:tokens: [CLS] wikiblogeebfs wikisocialmediasaqllfz and similar types of bomb will not be approved [MASK] per rule also if your link ends in please remove that and anything else extra ##ne ##ous before submit ##ting also we just [MASK] working on a wikiwhitelistiazebq Club [MASK] re ##put ##able news sites related to [MASK] arts world [MASK] your post links [MASK] such a website please send us [SEP] to g ##rad students too also the college you are in [MASK] when your registration window opens a c ##s [MASK] like me is in the wikiliberalartsiablf [MASK] so our window opens later than the window for people in [MASK] engineering Gold e ##ec ##s c [MASK] or e ##e majors [MASK] ornaments same [MASK] division courses as we Bari that [MASK] [SEP]\n","I0429 23:09:23.445646 140629406877568 create_pretraining_data.py:151] tokens: [CLS] wikiblogeebfs wikisocialmediasaqllfz and similar types of bomb will not be approved [MASK] per rule also if your link ends in please remove that and anything else extra ##ne ##ous before submit ##ting also we just [MASK] working on a wikiwhitelistiazebq Club [MASK] re ##put ##able news sites related to [MASK] arts world [MASK] your post links [MASK] such a website please send us [SEP] to g ##rad students too also the college you are in [MASK] when your registration window opens a c ##s [MASK] like me is in the wikiliberalartsiablf [MASK] so our window opens later than the window for people in [MASK] engineering Gold e ##ec ##s c [MASK] or e ##e majors [MASK] ornaments same [MASK] division courses as we Bari that [MASK] [SEP]\n","INFO:tensorflow:input_ids: 101 9 8 1105 1861 3322 1104 5985 1209 1136 1129 4092 103 1679 3013 1145 1191 1240 5088 3769 1107 4268 5782 1115 1105 1625 1950 3908 1673 2285 1196 12295 1916 1145 1195 1198 103 1684 1113 170 4 1998 103 1231 16156 1895 2371 3911 2272 1106 103 3959 1362 103 1240 2112 6743 103 1216 170 3265 4268 3952 1366 102 1106 176 9871 1651 1315 1145 1103 2134 1128 1132 1107 103 1165 1240 9630 2487 7086 170 172 1116 103 1176 1143 1110 1107 1103 1 103 1177 1412 2487 7086 1224 1190 1103 2487 1111 1234 1107 103 3752 3487 174 10294 1116 172 103 1137 174 1162 19796 103 28034 1269 103 2417 4770 1112 1195 25788 1115 103 102\n","I0429 23:09:23.445806 140629406877568 create_pretraining_data.py:161] input_ids: 101 9 8 1105 1861 3322 1104 5985 1209 1136 1129 4092 103 1679 3013 1145 1191 1240 5088 3769 1107 4268 5782 1115 1105 1625 1950 3908 1673 2285 1196 12295 1916 1145 1195 1198 103 1684 1113 170 4 1998 103 1231 16156 1895 2371 3911 2272 1106 103 3959 1362 103 1240 2112 6743 103 1216 170 3265 4268 3952 1366 102 1106 176 9871 1651 1315 1145 1103 2134 1128 1132 1107 103 1165 1240 9630 2487 7086 170 172 1116 103 1176 1143 1110 1107 1103 1 103 1177 1412 2487 7086 1224 1190 1103 2487 1111 1234 1107 103 3752 3487 174 10294 1116 172 103 1137 174 1162 19796 103 28034 1269 103 2417 4770 1112 1195 25788 1115 103 102\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","I0429 23:09:23.445965 140629406877568 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","I0429 23:09:23.446101 140629406877568 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","INFO:tensorflow:masked_lm_positions: 7 12 36 41 42 50 53 57 76 85 92 104 106 111 116 117 119 124 126 0\n","I0429 23:09:23.446193 140629406877568 create_pretraining_data.py:161] masked_lm_positions: 7 12 36 41 42 50 53 57 76 85 92 104 106 111 116 117 119 124 126 0\n","INFO:tensorflow:masked_lm_ids: 12045 1112 1408 1111 11582 1103 1191 1106 17579 1558 2134 1103 2134 1162 1321 1103 3105 1202 1116 0\n","I0429 23:09:23.446291 140629406877568 create_pretraining_data.py:161] masked_lm_ids: 12045 1112 1408 1111 11582 1103 1191 1106 17579 1558 2134 1103 2134 1162 1321 1103 3105 1202 1116 0\n","INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n","I0429 23:09:23.446393 140629406877568 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n","INFO:tensorflow:next_sentence_labels: 1\n","I0429 23:09:23.446480 140629406877568 create_pretraining_data.py:161] next_sentence_labels: 1\n","INFO:tensorflow:*** Example ***\n","I0429 23:09:23.446855 140629406877568 create_pretraining_data.py:149] *** Example ***\n","INFO:tensorflow:tokens: [CLS] term involvement [MASK] helpful in [MASK] of grand ##er goals i disagree with the idea that liberal [MASK] [MASK] ho ##mogeneous i really don ##t think it is that ho ##mo ##gen ##ous the limiting factor in making it more inclusive is the cost ultimately which locks out poor [MASK] who are Bat more likely [MASK] be [MASK] races because long and nasty [SEP] girls showed enhanced spatial [MASK] and girls showed [MASK] short term memory after consuming o ##at ##me ##al results [MASK] Prairie year olds also showed effects Talmud breakfast HK younger children had better spatial memory and better audit ##ory attention and girls [MASK] better short term memory after consuming o ##at ##me [MASK] due to composition ##al [MASK] in protein and fiber [SEP]\n","I0429 23:09:23.447005 140629406877568 create_pretraining_data.py:151] tokens: [CLS] term involvement [MASK] helpful in [MASK] of grand ##er goals i disagree with the idea that liberal [MASK] [MASK] ho ##mogeneous i really don ##t think it is that ho ##mo ##gen ##ous the limiting factor in making it more inclusive is the cost ultimately which locks out poor [MASK] who are Bat more likely [MASK] be [MASK] races because long and nasty [SEP] girls showed enhanced spatial [MASK] and girls showed [MASK] short term memory after consuming o ##at ##me ##al results [MASK] Prairie year olds also showed effects Talmud breakfast HK younger children had better spatial memory and better audit ##ory attention and girls [MASK] better short term memory after consuming o ##at ##me [MASK] due to composition ##al [MASK] in protein and fiber [SEP]\n","INFO:tensorflow:input_ids: 101 1858 6083 103 14739 1107 103 1104 5372 1200 2513 178 23423 1114 1103 1911 1115 7691 103 103 16358 28008 178 1541 1274 1204 1341 1122 1110 1115 16358 3702 4915 2285 1103 15816 5318 1107 1543 1122 1167 21783 1110 1103 2616 4444 1134 11992 1149 2869 103 1150 1132 21928 1167 2620 103 1129 103 3117 1272 1263 1105 13392 102 2636 2799 9927 15442 103 1105 2636 2799 103 1603 1858 2962 1170 16114 184 2980 3263 1348 2686 103 13891 1214 25343 1145 2799 3154 27512 6462 25660 3247 1482 1125 1618 15442 2962 1105 1618 23097 4649 2209 1105 2636 103 1618 1603 1858 2962 1170 16114 184 2980 3263 103 1496 1106 5239 1348 103 1107 4592 1105 12753 102\n","I0429 23:09:23.447153 140629406877568 create_pretraining_data.py:161] input_ids: 101 1858 6083 103 14739 1107 103 1104 5372 1200 2513 178 23423 1114 1103 1911 1115 7691 103 103 16358 28008 178 1541 1274 1204 1341 1122 1110 1115 16358 3702 4915 2285 1103 15816 5318 1107 1543 1122 1167 21783 1110 1103 2616 4444 1134 11992 1149 2869 103 1150 1132 21928 1167 2620 103 1129 103 3117 1272 1263 1105 13392 102 2636 2799 9927 15442 103 1105 2636 2799 103 1603 1858 2962 1170 16114 184 2980 3263 1348 2686 103 13891 1214 25343 1145 2799 3154 27512 6462 25660 3247 1482 1125 1618 15442 2962 1105 1618 23097 4649 2209 1105 2636 103 1618 1603 1858 2962 1170 16114 184 2980 3263 103 1496 1106 5239 1348 103 1107 4592 1105 12753 102\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","I0429 23:09:23.447288 140629406877568 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","I0429 23:09:23.447421 140629406877568 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","INFO:tensorflow:masked_lm_positions: 3 6 18 19 50 53 56 58 69 73 84 85 87 91 93 99 107 117 122 0\n","I0429 23:09:23.447520 140629406877568 create_pretraining_data.py:161] masked_lm_positions: 3 6 18 19 50 53 56 58 69 73 84 85 87 91 93 99 107 117 122 0\n","INFO:tensorflow:masked_lm_ids: 1110 6026 3959 1110 1234 1145 1106 7309 2962 4725 1114 1106 25343 1104 2076 2962 7799 1348 5408 0\n","I0429 23:09:23.447616 140629406877568 create_pretraining_data.py:161] masked_lm_ids: 1110 6026 3959 1110 1234 1145 1106 7309 2962 4725 1114 1106 25343 1104 2076 2962 7799 1348 5408 0\n","INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n","I0429 23:09:23.447718 140629406877568 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n","INFO:tensorflow:next_sentence_labels: 1\n","I0429 23:09:23.447804 140629406877568 create_pretraining_data.py:161] next_sentence_labels: 1\n","INFO:tensorflow:*** Example ***\n","I0429 23:09:23.448193 140629406877568 create_pretraining_data.py:149] *** Example ***\n","INFO:tensorflow:tokens: [CLS] [MASK] study [MASK] and politics etc i really don ##t think its this as much Maccabi [MASK] simply don ##t put any stock in those areas of study a large portion of wikiredditeazqoos rides the whole le stem master race dick [MASK] hard its been ch ##af ##ed [MASK] i mean ##na ##ve been told on multiple occasions [MASK] my field firstly isn [SEP] from one of those prestigious schools but i stuck with it all [MASK] those [MASK] ##tech ##s need people Morales a science degree that might [MASK] be in the research [MASK] you can get an entry level job [MASK] Airborne or quality control some of the bigger [MASK] have micro ##biology v ##iro ##logy and analytical development departments that often have entry [SEP]\n","I0429 23:09:23.448344 140629406877568 create_pretraining_data.py:151] tokens: [CLS] [MASK] study [MASK] and politics etc i really don ##t think its this as much Maccabi [MASK] simply don ##t put any stock in those areas of study a large portion of wikiredditeazqoos rides the whole le stem master race dick [MASK] hard its been ch ##af ##ed [MASK] i mean ##na ##ve been told on multiple occasions [MASK] my field firstly isn [SEP] from one of those prestigious schools but i stuck with it all [MASK] those [MASK] ##tech ##s need people Morales a science degree that might [MASK] be in the research [MASK] you can get an entry level job [MASK] Airborne or quality control some of the bigger [MASK] have micro ##biology v ##iro ##logy and analytical development departments that often have entry [SEP]\n","INFO:tensorflow:input_ids: 101 103 2025 103 1105 4039 3576 178 1541 1274 1204 1341 1157 1142 1112 1277 27016 103 2566 1274 1204 1508 1251 4482 1107 1343 1877 1104 2025 170 1415 3849 1104 7 13863 1103 2006 5837 8175 3283 1886 14458 103 1662 1157 1151 22572 9823 1174 103 178 1928 1605 2707 1151 1500 1113 2967 6070 103 1139 1768 22240 2762 102 1121 1141 1104 1343 8593 2126 1133 178 5342 1114 1122 1155 103 1343 103 22218 1116 1444 1234 18528 170 2598 2178 1115 1547 103 1129 1107 1103 1844 103 1128 1169 1243 1126 3990 1634 2261 103 16607 1137 3068 1654 1199 1104 1103 6706 103 1138 17599 25181 191 9992 6360 1105 22828 1718 7844 1115 1510 1138 3990 102\n","I0429 23:09:23.544516 140629406877568 create_pretraining_data.py:161] input_ids: 101 103 2025 103 1105 4039 3576 178 1541 1274 1204 1341 1157 1142 1112 1277 27016 103 2566 1274 1204 1508 1251 4482 1107 1343 1877 1104 2025 170 1415 3849 1104 7 13863 1103 2006 5837 8175 3283 1886 14458 103 1662 1157 1151 22572 9823 1174 103 178 1928 1605 2707 1151 1500 1113 2967 6070 103 1139 1768 22240 2762 102 1121 1141 1104 1343 8593 2126 1133 178 5342 1114 1122 1155 103 1343 103 22218 1116 1444 1234 18528 170 2598 2178 1115 1547 103 1129 1107 1103 1844 103 1128 1169 1243 1126 3990 1634 2261 103 16607 1137 3068 1654 1199 1104 1103 6706 103 1138 17599 25181 191 9992 6360 1105 22828 1718 7844 1115 1510 1138 3990 102\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","I0429 23:09:23.544814 140629406877568 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","I0429 23:09:23.545094 140629406877568 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","INFO:tensorflow:masked_lm_positions: 1 3 9 16 17 42 49 52 59 62 63 77 79 84 90 95 103 104 112 0\n","I0429 23:09:23.545254 140629406877568 create_pretraining_data.py:161] masked_lm_positions: 1 3 9 16 17 42 49 52 59 62 63 77 79 84 90 95 103 104 112 0\n","INFO:tensorflow:masked_lm_ids: 2140 1972 1274 1112 1103 1177 7158 178 1115 20233 2762 1104 25128 1114 1136 2853 4374 5863 2557 0\n","I0429 23:09:23.545382 140629406877568 create_pretraining_data.py:161] masked_lm_ids: 2140 1972 1274 1112 1103 1177 7158 178 1115 20233 2762 1104 25128 1114 1136 2853 4374 5863 2557 0\n","INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n","I0429 23:09:23.545524 140629406877568 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n","INFO:tensorflow:next_sentence_labels: 1\n","I0429 23:09:23.545641 140629406877568 create_pretraining_data.py:161] next_sentence_labels: 1\n","INFO:tensorflow:*** Example ***\n","I0429 23:09:23.546255 140629406877568 create_pretraining_data.py:149] *** Example ***\n","INFO:tensorflow:tokens: [CLS] level hardware lovers end up [MASK] c ##e here ##s municipality my view of [MASK] [MASK] topics in the two [MASK] Edison now i am overs [MASK] ##p ##lify ##ing a bit for flavor Studies major can tackle most of these problems computer science c ##s algorithms software engineering software engineering compiler compiler computational biology distributed systems computer engineering c [MASK] u ##bi [SEP] his own hometown of port c [MASK] ##on oh ##io port c ##lint ##on oh ##io [MASK] [MASK] [MASK] show the disc ##re [MASK] ##cies between rich and poor children in education and early development in life he told the story of disadvantage ##d mother [MASK] ##ry sue to illustrate [MASK] growing rate of child blush over the last few decades everything [SEP]\n","I0429 23:09:23.546442 140629406877568 create_pretraining_data.py:151] tokens: [CLS] level hardware lovers end up [MASK] c ##e here ##s municipality my view of [MASK] [MASK] topics in the two [MASK] Edison now i am overs [MASK] ##p ##lify ##ing a bit for flavor Studies major can tackle most of these problems computer science c ##s algorithms software engineering software engineering compiler compiler computational biology distributed systems computer engineering c [MASK] u ##bi [SEP] his own hometown of port c [MASK] ##on oh ##io port c ##lint ##on oh ##io [MASK] [MASK] [MASK] show the disc ##re [MASK] ##cies between rich and poor children in education and early development in life he told the story of disadvantage ##d mother [MASK] ##ry sue to illustrate [MASK] growing rate of child blush over the last few decades everything [SEP]\n","INFO:tensorflow:input_ids: 101 1634 8172 12977 1322 1146 103 172 1162 1303 1116 2667 1139 2458 1104 103 103 7662 1107 1103 1160 103 18221 1208 178 1821 17074 103 1643 22881 1158 170 2113 1111 16852 3829 1558 1169 11407 1211 1104 1292 2645 2775 2598 172 1116 14975 3594 3752 3594 3752 26012 26012 19903 10256 4901 2344 2775 3752 172 103 190 5567 102 1117 1319 9694 1104 4104 172 103 1320 9294 2660 4104 172 22761 1320 9294 2660 103 103 103 1437 1103 6187 1874 103 9805 1206 3987 1105 2869 1482 1107 1972 1105 1346 1718 1107 1297 1119 1500 1103 1642 1104 22611 1181 1534 103 1616 25762 1106 20873 103 2898 2603 1104 2027 18407 1166 1103 1314 1374 4397 1917 102\n","I0429 23:09:23.546586 140629406877568 create_pretraining_data.py:161] input_ids: 101 1634 8172 12977 1322 1146 103 172 1162 1303 1116 2667 1139 2458 1104 103 103 7662 1107 1103 1160 103 18221 1208 178 1821 17074 103 1643 22881 1158 170 2113 1111 16852 3829 1558 1169 11407 1211 1104 1292 2645 2775 2598 172 1116 14975 3594 3752 3594 3752 26012 26012 19903 10256 4901 2344 2775 3752 172 103 190 5567 102 1117 1319 9694 1104 4104 172 103 1320 9294 2660 4104 172 22761 1320 9294 2660 103 103 103 1437 1103 6187 1874 103 9805 1206 3987 1105 2869 1482 1107 1972 1105 1346 1718 1107 1297 1119 1500 1103 1642 1104 22611 1181 1534 103 1616 25762 1106 20873 103 2898 2603 1104 2027 18407 1166 1103 1314 1374 4397 1917 102\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","I0429 23:09:23.546725 140629406877568 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","I0429 23:09:23.546870 140629406877568 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","INFO:tensorflow:masked_lm_positions: 6 15 16 21 22 27 35 51 61 71 80 81 82 83 88 101 110 115 120 0\n","I0429 23:09:23.546967 140629406877568 create_pretraining_data.py:161] masked_lm_positions: 6 15 16 21 22 27 35 51 61 71 80 81 82 83 88 101 110 115 120 0\n","INFO:tensorflow:masked_lm_ids: 1107 1103 2633 3872 1268 4060 1719 3752 1162 22761 2660 9294 2660 1106 10224 1297 12477 1103 5224 0\n","I0429 23:09:23.547072 140629406877568 create_pretraining_data.py:161] masked_lm_ids: 1107 1103 2633 3872 1268 4060 1719 3752 1162 22761 2660 9294 2660 1106 10224 1297 12477 1103 5224 0\n","INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n","I0429 23:09:23.547175 140629406877568 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n","INFO:tensorflow:next_sentence_labels: 1\n","I0429 23:09:23.547260 140629406877568 create_pretraining_data.py:161] next_sentence_labels: 1\n","INFO:tensorflow:*** Example ***\n","I0429 23:09:23.547685 140629406877568 create_pretraining_data.py:149] *** Example ***\n","INFO:tensorflow:tokens: [CLS] [MASK] them light to create a [MASK] environment that removes any idea of per ##man ##ence [MASK] might impose constraints on creativity one such example is [MASK] [MASK] ##s and p ##eg [MASK] system which consists of a Fringe of unfinished two by two wooden [MASK] ##ks which have p ##eg ##s that hold light weight portable dry erase boards these boards can [SEP] mean that the actual philosophical [MASK] [MASK] hold are any more valid than the next persons they may be better at putting their beliefs and [MASK] into words but that [MASK] all their expertise provides with wikistemfieldsefelbbe [MASK] majority of [MASK] subject matter your ##e dealing with is objective [MASK] things that are practically and demons ##tra ##bly true through [MASK] ##enia [SEP]\n","I0429 23:09:23.547861 140629406877568 create_pretraining_data.py:151] tokens: [CLS] [MASK] them light to create a [MASK] environment that removes any idea of per ##man ##ence [MASK] might impose constraints on creativity one such example is [MASK] [MASK] ##s and p ##eg [MASK] system which consists of a Fringe of unfinished two by two wooden [MASK] ##ks which have p ##eg ##s that hold light weight portable dry erase boards these boards can [SEP] mean that the actual philosophical [MASK] [MASK] hold are any more valid than the next persons they may be better at putting their beliefs and [MASK] into words but that [MASK] all their expertise provides with wikistemfieldsefelbbe [MASK] majority of [MASK] subject matter your ##e dealing with is objective [MASK] things that are practically and demons ##tra ##bly true through [MASK] ##enia [SEP]\n","INFO:tensorflow:input_ids: 101 103 1172 1609 1106 2561 170 103 3750 1115 22391 1251 1911 1104 1679 1399 7008 103 1547 19103 15651 1113 17980 1141 1216 1859 1110 103 103 1116 1105 185 12606 103 1449 1134 2923 1104 170 23863 1104 16623 1160 1118 1160 4122 103 4616 1134 1138 185 12606 1116 1115 2080 1609 2841 15139 3712 27224 8190 1292 8190 1169 102 1928 1115 1103 4315 11388 103 103 2080 1132 1251 1167 9221 1190 1103 1397 4983 1152 1336 1129 1618 1120 4518 1147 8810 1105 103 1154 1734 1133 1115 103 1155 1147 11717 2790 1114 80 103 2656 1104 103 2548 2187 1240 1162 6705 1114 1110 7649 103 1614 1115 1132 7892 1105 8568 4487 4999 2276 1194 103 23179 102\n","I0429 23:09:23.548007 140629406877568 create_pretraining_data.py:161] input_ids: 101 103 1172 1609 1106 2561 170 103 3750 1115 22391 1251 1911 1104 1679 1399 7008 103 1547 19103 15651 1113 17980 1141 1216 1859 1110 103 103 1116 1105 185 12606 103 1449 1134 2923 1104 170 23863 1104 16623 1160 1118 1160 4122 103 4616 1134 1138 185 12606 1116 1115 2080 1609 2841 15139 3712 27224 8190 1292 8190 1169 102 1928 1115 1103 4315 11388 103 103 2080 1132 1251 1167 9221 1190 1103 1397 4983 1152 1336 1129 1618 1120 4518 1147 8810 1105 103 1154 1734 1133 1115 103 1155 1147 11717 2790 1114 80 103 2656 1104 103 2548 2187 1240 1162 6705 1114 1110 7649 103 1614 1115 1132 7892 1105 8568 4487 4999 2276 1194 103 23179 102\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","I0429 23:09:23.548157 140629406877568 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","I0429 23:09:23.548300 140629406877568 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","INFO:tensorflow:masked_lm_positions: 1 3 7 17 27 28 33 38 39 46 70 71 90 95 97 102 105 114 125 0\n","I0429 23:09:23.548403 140629406877568 create_pretraining_data.py:161] masked_lm_positions: 1 3 7 17 27 28 33 38 39 46 70 71 90 95 97 102 105 114 125 0\n","INFO:tensorflow:masked_lm_ids: 1106 3005 13156 1115 1103 24498 1116 170 1326 2197 4133 1152 4133 1116 1147 1103 1103 1864 5576 0\n","I0429 23:09:23.548717 140629406877568 create_pretraining_data.py:161] masked_lm_ids: 1106 3005 13156 1115 1103 24498 1116 170 1326 2197 4133 1152 4133 1116 1147 1103 1103 1864 5576 0\n","INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n","I0429 23:09:23.548875 140629406877568 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n","INFO:tensorflow:next_sentence_labels: 1\n","I0429 23:09:23.548986 140629406877568 create_pretraining_data.py:161] next_sentence_labels: 1\n","INFO:tensorflow:*** Example ***\n","I0429 23:09:23.549445 140629406877568 create_pretraining_data.py:149] *** Example ***\n","INFO:tensorflow:tokens: [CLS] back 95 000 in a year m ##r bank [MASK] think pulsing central bank are going to find in the next [MASK] crash is that credit expansion isn [MASK] effective anymore because they have [MASK] long ago eliminated the scar ##city of loan ##able De what they will be left with is only literally printing money and giving it [MASK] to consumers [MASK] [SEP] and re emerging exchanges [MASK] chin ##a it needs [MASK] time and [MASK] it Cindy go for [MASK] at [MASK] we can expect the general sentiment of cry ##pt ##oc ##ur [MASK] ##cies to follow through as well some good points about et ##h 1 despite [MASK] delays there are several projects ##erry on secondary [MASK] [MASK] ##ing solutions for plasma cough [SEP]\n","I0429 23:09:23.549624 140629406877568 create_pretraining_data.py:151] tokens: [CLS] back 95 000 in a year m ##r bank [MASK] think pulsing central bank are going to find in the next [MASK] crash is that credit expansion isn [MASK] effective anymore because they have [MASK] long ago eliminated the scar ##city of loan ##able De what they will be left with is only literally printing money and giving it [MASK] to consumers [MASK] [SEP] and re emerging exchanges [MASK] chin ##a it needs [MASK] time and [MASK] it Cindy go for [MASK] at [MASK] we can expect the general sentiment of cry ##pt ##oc ##ur [MASK] ##cies to follow through as well some good points about et ##h 1 despite [MASK] delays there are several projects ##erry on secondary [MASK] [MASK] ##ing solutions for plasma cough [SEP]\n","INFO:tensorflow:input_ids: 101 1171 4573 1288 1107 170 1214 182 1197 3085 103 1341 26069 2129 3085 1132 1280 1106 1525 1107 1103 1397 103 5683 1110 1115 4755 4298 2762 103 3903 4169 1272 1152 1138 103 1263 2403 5802 1103 14161 9041 1104 4891 1895 3177 1184 1152 1209 1129 1286 1114 1110 1178 6290 8455 1948 1105 2368 1122 103 1106 11060 103 102 1105 1231 8999 17755 103 5144 1161 1122 2993 103 1159 1105 103 1122 16774 1301 1111 103 1120 103 1195 1169 5363 1103 1704 17024 1104 5354 6451 13335 2149 103 9805 1106 2812 1194 1112 1218 1199 1363 1827 1164 3084 1324 122 2693 103 15308 1175 1132 1317 3203 16463 1113 3718 103 103 1158 7995 1111 13441 21810 102\n","I0429 23:09:23.549801 140629406877568 create_pretraining_data.py:161] input_ids: 101 1171 4573 1288 1107 170 1214 182 1197 3085 103 1341 26069 2129 3085 1132 1280 1106 1525 1107 1103 1397 103 5683 1110 1115 4755 4298 2762 103 3903 4169 1272 1152 1138 103 1263 2403 5802 1103 14161 9041 1104 4891 1895 3177 1184 1152 1209 1129 1286 1114 1110 1178 6290 8455 1948 1105 2368 1122 103 1106 11060 103 102 1105 1231 8999 17755 103 5144 1161 1122 2993 103 1159 1105 103 1122 16774 1301 1111 103 1120 103 1195 1169 5363 1103 1704 17024 1104 5354 6451 13335 2149 103 9805 1106 2812 1194 1112 1218 1199 1363 1827 1164 3084 1324 122 2693 103 15308 1175 1132 1317 3203 16463 1113 3718 103 103 1158 7995 1111 13441 21810 102\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","I0429 23:09:23.549975 140629406877568 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","I0429 23:09:23.550128 140629406877568 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","INFO:tensorflow:masked_lm_positions: 10 12 22 29 35 45 60 63 69 74 77 79 82 84 96 111 117 120 121 0\n","I0429 23:09:23.550234 140629406877568 create_pretraining_data.py:161] masked_lm_positions: 10 12 22 29 35 45 60 63 69 74 77 79 82 84 96 111 117 120 121 0\n","INFO:tensorflow:masked_lm_ids: 178 1184 1558 1204 1640 4381 2626 1134 1107 1157 1191 1674 1330 1324 5123 1103 1684 188 7867 0\n","I0429 23:09:23.550355 140629406877568 create_pretraining_data.py:161] masked_lm_ids: 178 1184 1558 1204 1640 4381 2626 1134 1107 1157 1191 1674 1330 1324 5123 1103 1684 188 7867 0\n","INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n","I0429 23:09:23.550472 140629406877568 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n","INFO:tensorflow:next_sentence_labels: 1\n","I0429 23:09:23.550578 140629406877568 create_pretraining_data.py:161] next_sentence_labels: 1\n","INFO:tensorflow:*** Example ***\n","I0429 23:09:23.550992 140629406877568 create_pretraining_data.py:149] *** Example ***\n","INFO:tensorflow:tokens: [CLS] ##ive changes in [MASK] [MASK] unlikely to follow immediately with [MASK] this in mind it is no surprise that most students find the survey irrelevant and if not irrelevant then at the very least low on their [MASK] of things to do [MASK] s ##zu [MASK] ##j 19 despite receiving [MASK] couple of reminder ##s from [MASK] [MASK] wide b ##litz sent [MASK] [SEP] Remix medical preparation intro ch ##em calculus and physics are definitely pretty bare ##bone computer barely scratch the surface and i have [MASK] [MASK] intro bio here i wouldn ##t [MASK] someone on medical school with just a year of biology i honestly think an actual background [MASK] cellular and molecular biology and perhaps a couple of courses in bio ##chemistry human [SEP]\n","I0429 23:09:23.551162 140629406877568 create_pretraining_data.py:151] tokens: [CLS] ##ive changes in [MASK] [MASK] unlikely to follow immediately with [MASK] this in mind it is no surprise that most students find the survey irrelevant and if not irrelevant then at the very least low on their [MASK] of things to do [MASK] s ##zu [MASK] ##j 19 despite receiving [MASK] couple of reminder ##s from [MASK] [MASK] wide b ##litz sent [MASK] [SEP] Remix medical preparation intro ch ##em calculus and physics are definitely pretty bare ##bone computer barely scratch the surface and i have [MASK] [MASK] intro bio here i wouldn ##t [MASK] someone on medical school with just a year of biology i honestly think an actual background [MASK] cellular and molecular biology and perhaps a couple of courses in bio ##chemistry human [SEP]\n","INFO:tensorflow:input_ids: 101 2109 2607 1107 103 103 9803 1106 2812 2411 1114 103 1142 1107 1713 1122 1110 1185 3774 1115 1211 1651 1525 1103 5980 25707 1105 1191 1136 25707 1173 1120 1103 1304 1655 1822 1113 1147 103 1104 1614 1106 1202 103 188 10337 103 3361 1627 2693 4172 103 2337 1104 15656 1116 1121 103 103 2043 171 15516 1850 103 102 8076 2657 7288 27553 22572 5521 27323 1105 7094 1132 5397 2785 6038 9527 2775 3742 14515 1103 2473 1105 178 1138 103 103 27553 25128 1303 178 2010 1204 103 1800 1113 2657 1278 1114 1198 170 1214 1104 10256 178 12051 1341 1126 4315 3582 103 14391 1105 9546 10256 1105 3229 170 2337 1104 4770 1107 25128 18533 1769 102\n","I0429 23:09:23.551359 140629406877568 create_pretraining_data.py:161] input_ids: 101 2109 2607 1107 103 103 9803 1106 2812 2411 1114 103 1142 1107 1713 1122 1110 1185 3774 1115 1211 1651 1525 1103 5980 25707 1105 1191 1136 25707 1173 1120 1103 1304 1655 1822 1113 1147 103 1104 1614 1106 1202 103 188 10337 103 3361 1627 2693 4172 103 2337 1104 15656 1116 1121 103 103 2043 171 15516 1850 103 102 8076 2657 7288 27553 22572 5521 27323 1105 7094 1132 5397 2785 6038 9527 2775 3742 14515 1103 2473 1105 178 1138 103 103 27553 25128 1303 178 2010 1204 103 1800 1113 2657 1278 1114 1198 170 1214 1104 10256 178 12051 1341 1126 4315 3582 103 14391 1105 9546 10256 1105 3229 170 2337 1104 4770 1107 25128 18533 1769 102\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","I0429 23:09:23.647371 140629406877568 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","I0429 23:09:23.647610 140629406877568 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","INFO:tensorflow:masked_lm_positions: 4 5 11 38 43 46 51 54 57 58 63 65 85 87 88 95 96 103 112 0\n","I0429 23:09:23.647713 140629406877568 create_pretraining_data.py:161] masked_lm_positions: 4 5 11 38 43 46 51 54 57 58 63 65 85 87 88 95 96 103 112 0\n","INFO:tensorflow:masked_lm_ids: 2593 1132 1155 2190 26181 2328 170 15656 170 3314 1118 1240 178 27629 1181 5768 1800 1214 1107 0\n","I0429 23:09:23.647798 140629406877568 create_pretraining_data.py:161] masked_lm_ids: 2593 1132 1155 2190 26181 2328 170 15656 170 3314 1118 1240 178 27629 1181 5768 1800 1214 1107 0\n","INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n","I0429 23:09:23.647917 140629406877568 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n","INFO:tensorflow:next_sentence_labels: 1\n","I0429 23:09:23.648021 140629406877568 create_pretraining_data.py:161] next_sentence_labels: 1\n","INFO:tensorflow:*** Example ***\n","I0429 23:09:23.648667 140629406877568 create_pretraining_data.py:149] *** Example ***\n","INFO:tensorflow:tokens: [CLS] get over a on the very easy exam when greeted company stopped being run by people that worked their way up [MASK] the ranks and started [MASK] run by college graduates that have never been in the industry and have no idea how it actually works [MASK] im not [MASK] [MASK] put clear my browser history in [MASK] will [MASK] ##ug a [MASK] [SEP] my wife that i would not talk to [MASK] [MASK] about why [MASK] don ##t believe in the truth or even god for [MASK] matter in return she agreed that [MASK] would encourage our children to attend [MASK] local public year division university that is nearby i ##ve been saving up to make that happen [MASK] a very long time my only [SEP]\n","I0429 23:09:23.648902 140629406877568 create_pretraining_data.py:151] tokens: [CLS] get over a on the very easy exam when greeted company stopped being run by people that worked their way up [MASK] the ranks and started [MASK] run by college graduates that have never been in the industry and have no idea how it actually works [MASK] im not [MASK] [MASK] put clear my browser history in [MASK] will [MASK] ##ug a [MASK] [SEP] my wife that i would not talk to [MASK] [MASK] about why [MASK] don ##t believe in the truth or even god for [MASK] matter in return she agreed that [MASK] would encourage our children to attend [MASK] local public year division university that is nearby i ##ve been saving up to make that happen [MASK] a very long time my only [SEP]\n","INFO:tensorflow:input_ids: 101 1243 1166 170 1113 1103 1304 3123 12211 1165 11196 1419 2141 1217 1576 1118 1234 1115 1589 1147 1236 1146 103 1103 6496 1105 1408 103 1576 1118 2134 11529 1115 1138 1309 1151 1107 1103 2380 1105 1138 1185 1911 1293 1122 2140 1759 103 13280 1136 103 103 1508 2330 1139 19089 1607 1107 103 1209 103 9610 170 103 102 1139 1676 1115 178 1156 1136 2037 1106 103 103 1164 1725 103 1274 1204 2059 1107 1103 3062 1137 1256 5540 1111 103 2187 1107 1862 1131 2675 1115 103 1156 8343 1412 1482 1106 4739 103 1469 1470 1214 2417 2755 1115 1110 2721 178 2707 1151 7740 1146 1106 1294 1115 3333 103 170 1304 1263 1159 1139 1178 102\n","I0429 23:09:23.649085 140629406877568 create_pretraining_data.py:161] input_ids: 101 1243 1166 170 1113 1103 1304 3123 12211 1165 11196 1419 2141 1217 1576 1118 1234 1115 1589 1147 1236 1146 103 1103 6496 1105 1408 103 1576 1118 2134 11529 1115 1138 1309 1151 1107 1103 2380 1105 1138 1185 1911 1293 1122 2140 1759 103 13280 1136 103 103 1508 2330 1139 19089 1607 1107 103 1209 103 9610 170 103 102 1139 1676 1115 178 1156 1136 2037 1106 103 103 1164 1725 103 1274 1204 2059 1107 1103 3062 1137 1256 5540 1111 103 2187 1107 1862 1131 2675 1115 103 1156 8343 1412 1482 1106 4739 103 1469 1470 1214 2417 2755 1115 1110 2721 178 2707 1151 7740 1146 1106 1294 1115 3333 103 170 1304 1263 1159 1139 1178 102\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","I0429 23:09:23.649249 140629406877568 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","I0429 23:09:23.650440 140629406877568 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","INFO:tensorflow:masked_lm_positions: 4 9 10 22 27 47 50 51 58 60 63 71 73 74 77 88 95 102 120 0\n","I0429 23:09:23.650586 140629406877568 create_pretraining_data.py:161] masked_lm_positions: 4 9 10 22 27 47 50 51 58 60 63 71 73 74 77 88 95 102 120 0\n","INFO:tensorflow:masked_lm_ids: 1113 1165 1103 1194 1217 1184 3155 1106 1139 1202 3439 2037 1412 1482 178 1115 1195 1103 1111 0\n","I0429 23:09:23.650684 140629406877568 create_pretraining_data.py:161] masked_lm_ids: 1113 1165 1103 1194 1217 1184 3155 1106 1139 1202 3439 2037 1412 1482 178 1115 1195 1103 1111 0\n","INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n","I0429 23:09:23.650786 140629406877568 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n","INFO:tensorflow:next_sentence_labels: 1\n","I0429 23:09:23.650982 140629406877568 create_pretraining_data.py:161] next_sentence_labels: 1\n","INFO:tensorflow:*** Example ***\n","I0429 23:09:23.651616 140629406877568 create_pretraining_data.py:149] *** Example ***\n","INFO:tensorflow:tokens: [CLS] [MASK] and sciences hut courses the content in the wikimathematicsiaaei courses predecessor match up but many c ##s students [MASK] ##t [MASK] to take ch ##em physics while many [MASK] don ##t want to take languages wikihumanitiesseiez courses its up to you pick which ##ever one you like you think you have a better [MASK] at getting accepted to if you have literally [SEP] can literally major [MASK] c ##s from one of the top [MASK] in the nation that has un ##mat ##ched r ##oi and minor in [MASK] through classes at p [MASK] ##na or another [MASK] im headed to mud ##d [MASK] fall so hope [MASK] [MASK] you there if you haven ##t made a decision yet feel free to pm [MASK] edit [SEP]\n","I0429 23:09:23.651853 140629406877568 create_pretraining_data.py:151] tokens: [CLS] [MASK] and sciences hut courses the content in the wikimathematicsiaaei courses predecessor match up but many c ##s students [MASK] ##t [MASK] to take ch ##em physics while many [MASK] don ##t want to take languages wikihumanitiesseiez courses its up to you pick which ##ever one you like you think you have a better [MASK] at getting accepted to if you have literally [SEP] can literally major [MASK] c ##s from one of the top [MASK] in the nation that has un ##mat ##ched r ##oi and minor in [MASK] through classes at p [MASK] ##na or another [MASK] im headed to mud ##d [MASK] fall so hope [MASK] [MASK] you there if you haven ##t made a decision yet feel free to pm [MASK] edit [SEP]\n","INFO:tensorflow:input_ids: 101 103 1105 8614 16148 4770 1103 3438 1107 1103 17 4770 8283 1801 1146 1133 1242 172 1116 1651 103 1204 103 1106 1321 22572 5521 7094 1229 1242 103 1274 1204 1328 1106 1321 3483 2 4770 1157 1146 1106 1128 3368 1134 17791 1141 1128 1176 1128 1341 1128 1138 170 1618 103 1120 2033 3134 1106 1191 1128 1138 6290 102 1169 6290 1558 103 172 1116 1121 1141 1104 1103 1499 103 1107 1103 3790 1115 1144 8362 21943 6428 187 8136 1105 3137 1107 103 1194 3553 1120 185 103 1605 1137 1330 103 13280 2917 1106 9052 1181 103 2303 1177 2810 103 103 1128 1175 1191 1128 3983 1204 1189 170 2383 1870 1631 1714 1106 9852 103 14609 102\n","I0429 23:09:23.652031 140629406877568 create_pretraining_data.py:161] input_ids: 101 103 1105 8614 16148 4770 1103 3438 1107 1103 17 4770 8283 1801 1146 1133 1242 172 1116 1651 103 1204 103 1106 1321 22572 5521 7094 1229 1242 103 1274 1204 1328 1106 1321 3483 2 4770 1157 1146 1106 1128 3368 1134 17791 1141 1128 1176 1128 1341 1128 1138 170 1618 103 1120 2033 3134 1106 1191 1128 1138 6290 102 1169 6290 1558 103 172 1116 1121 1141 1104 1103 1499 103 1107 1103 3790 1115 1144 8362 21943 6428 187 8136 1105 3137 1107 103 1194 3553 1120 185 103 1605 1137 1330 103 13280 2917 1106 9052 1181 103 2303 1177 2810 103 103 1128 1175 1191 1128 3983 1204 1189 170 2383 1870 1631 1714 1106 9852 103 14609 102\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","I0429 23:09:23.652186 140629406877568 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","I0429 23:09:23.652330 140629406877568 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","INFO:tensorflow:masked_lm_positions: 1 4 12 20 22 30 54 55 68 76 90 95 99 105 109 110 121 122 125 0\n","I0429 23:09:23.652435 140629406877568 create_pretraining_data.py:161] masked_lm_positions: 1 4 12 20 22 30 54 55 68 76 90 95 99 105 109 110 121 122 125 0\n","INFO:tensorflow:masked_lm_ids: 3959 17 1209 1274 1328 1639 1618 2046 1107 2648 5027 18445 172 1142 1106 1267 1631 1714 1143 0\n","I0429 23:09:23.652542 140629406877568 create_pretraining_data.py:161] masked_lm_ids: 3959 17 1209 1274 1328 1639 1618 2046 1107 2648 5027 18445 172 1142 1106 1267 1631 1714 1143 0\n","INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n","I0429 23:09:23.652642 140629406877568 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n","INFO:tensorflow:next_sentence_labels: 1\n","I0429 23:09:23.652725 140629406877568 create_pretraining_data.py:161] next_sentence_labels: 1\n","INFO:tensorflow:*** Example ***\n","I0429 23:09:23.653293 140629406877568 create_pretraining_data.py:149] *** Example ***\n","INFO:tensorflow:tokens: [CLS] have a right to voice it too but the stuff leveled Nottingham wikibarackobamasefebb [MASK] just literally not true that [MASK] ##s [MASK] socialist that [MASK] ##s a na ##zi that he ##s a k ##en ##yan that [MASK] ##s a m ##us ##lim etc the stuff leveled at [MASK] ##rump is literally true that [MASK] administration represents an active interest against things like [SEP] ##ming that is if you study [MASK] hours a day for days you ##ll likely [MASK] [MASK] than you would studying [MASK] [MASK] the night before learned this in cognitive ps ##ych and its [MASK] pretty well for me anyway you really [MASK] ##t compare hours of studying depends on [MASK] subject shrug course [MASK] the exam itself [MASK] your methods of [SEP]\n","I0429 23:09:23.653479 140629406877568 create_pretraining_data.py:151] tokens: [CLS] have a right to voice it too but the stuff leveled Nottingham wikibarackobamasefebb [MASK] just literally not true that [MASK] ##s [MASK] socialist that [MASK] ##s a na ##zi that he ##s a k ##en ##yan that [MASK] ##s a m ##us ##lim etc the stuff leveled at [MASK] ##rump is literally true that [MASK] administration represents an active interest against things like [SEP] ##ming that is if you study [MASK] hours a day for days you ##ll likely [MASK] [MASK] than you would studying [MASK] [MASK] the night before learned this in cognitive ps ##ych and its [MASK] pretty well for me anyway you really [MASK] ##t compare hours of studying depends on [MASK] subject shrug course [MASK] the exam itself [MASK] your methods of [SEP]\n","INFO:tensorflow:input_ids: 101 1138 170 1268 1106 1490 1122 1315 1133 1103 4333 25569 11330 60 103 1198 6290 1136 2276 1115 103 1116 103 11181 1115 103 1116 170 9468 5303 1115 1119 1116 170 180 1424 6582 1115 103 1116 170 182 1361 24891 3576 1103 4333 25569 1120 103 27321 1110 6290 2276 1115 103 3469 5149 1126 2327 2199 1222 1614 1176 102 5031 1115 1110 1191 1128 2025 103 2005 170 1285 1111 1552 1128 2339 2620 103 103 1190 1128 1156 5076 103 103 1103 1480 1196 3560 1142 1107 12176 15604 21155 1105 1157 103 2785 1218 1111 1143 4050 1128 1541 103 1204 14133 2005 1104 5076 9113 1113 103 2548 13786 1736 103 1103 12211 2111 103 1240 4069 1104 102\n","I0429 23:09:23.654697 140629406877568 create_pretraining_data.py:161] input_ids: 101 1138 170 1268 1106 1490 1122 1315 1133 1103 4333 25569 11330 60 103 1198 6290 1136 2276 1115 103 1116 103 11181 1115 103 1116 170 9468 5303 1115 1119 1116 170 180 1424 6582 1115 103 1116 170 182 1361 24891 3576 1103 4333 25569 1120 103 27321 1110 6290 2276 1115 103 3469 5149 1126 2327 2199 1222 1614 1176 102 5031 1115 1110 1191 1128 2025 103 2005 170 1285 1111 1552 1128 2339 2620 103 103 1190 1128 1156 5076 103 103 1103 1480 1196 3560 1142 1107 12176 15604 21155 1105 1157 103 2785 1218 1111 1143 4050 1128 1541 103 1204 14133 2005 1104 5076 9113 1113 103 2548 13786 1736 103 1103 12211 2111 103 1240 4069 1104 102\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","I0429 23:09:23.654928 140629406877568 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","I0429 23:09:23.655106 140629406877568 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","INFO:tensorflow:masked_lm_positions: 12 14 20 22 25 38 49 55 71 80 81 86 87 99 107 115 117 119 123 0\n","I0429 23:09:23.655212 140629406877568 create_pretraining_data.py:161] masked_lm_positions: 12 14 20 22 25 38 49 55 71 80 81 86 87 99 107 115 117 119 123 0\n","INFO:tensorflow:masked_lm_ids: 1120 1108 1119 170 1119 1119 189 1117 1111 1202 1618 1111 2005 1589 1169 1103 1103 3083 1105 0\n","I0429 23:09:23.655307 140629406877568 create_pretraining_data.py:161] masked_lm_ids: 1120 1108 1119 170 1119 1119 189 1117 1111 1202 1618 1111 2005 1589 1169 1103 1103 3083 1105 0\n","INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n","I0429 23:09:23.655413 140629406877568 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n","INFO:tensorflow:next_sentence_labels: 1\n","I0429 23:09:23.655503 140629406877568 create_pretraining_data.py:161] next_sentence_labels: 1\n","INFO:tensorflow:*** Example ***\n","I0429 23:09:23.656723 140629406877568 create_pretraining_data.py:149] *** Example ***\n","INFO:tensorflow:tokens: [CLS] m ##us ##lim [MASK] [MASK] [MASK] which according to some friends of mine students in a [MASK] on campus leadership class [MASK] to attend there is an extremely b [MASK] ##ed [MASK] people [MASK] written in chalk that having blue buttons you know the ones that call [MASK] [MASK] not enough to [MASK] them and that the school refuses to protect students from [SEP] mental health and or ##th ##od ##ont ##ics [MASK] that [MASK] ##t covered by insurance share [MASK] [MASK] m ##2 ##isse with a view of the mountains with two good friends drive 10 minutes to [MASK] get world class backpack ##ing and or climbing and [MASK] skiing every weekend types of restaurants once [MASK] twice a week one or two international trips [SEP]\n","I0429 23:09:23.656952 140629406877568 create_pretraining_data.py:151] tokens: [CLS] m ##us ##lim [MASK] [MASK] [MASK] which according to some friends of mine students in a [MASK] on campus leadership class [MASK] to attend there is an extremely b [MASK] ##ed [MASK] people [MASK] written in chalk that having blue buttons you know the ones that call [MASK] [MASK] not enough to [MASK] them and that the school refuses to protect students from [SEP] mental health and or ##th ##od ##ont ##ics [MASK] that [MASK] ##t covered by insurance share [MASK] [MASK] m ##2 ##isse with a view of the mountains with two good friends drive 10 minutes to [MASK] get world class backpack ##ing and or climbing and [MASK] skiing every weekend types of restaurants once [MASK] twice a week one or two international trips [SEP]\n","INFO:tensorflow:input_ids: 101 182 1361 24891 103 103 103 1134 2452 1106 1199 2053 1104 2317 1651 1107 170 103 1113 3314 3645 1705 103 1106 4739 1175 1110 1126 4450 171 103 1174 103 1234 103 1637 1107 23410 1115 1515 2221 11760 1128 1221 1103 3200 1115 1840 103 103 1136 1536 1106 103 1172 1105 1115 1103 1278 10078 1106 3244 1651 1121 102 4910 2332 1105 1137 1582 5412 9921 4724 103 1115 103 1204 2262 1118 5986 2934 103 103 182 1477 19202 1114 170 2458 1104 1103 5000 1114 1160 1363 2053 2797 1275 1904 1106 103 1243 1362 1705 13846 1158 1105 1137 8259 1105 103 14106 1451 5138 3322 1104 7724 1517 103 3059 170 1989 1141 1137 1160 1835 9185 102\n","I0429 23:09:23.657153 140629406877568 create_pretraining_data.py:161] input_ids: 101 182 1361 24891 103 103 103 1134 2452 1106 1199 2053 1104 2317 1651 1107 170 103 1113 3314 3645 1705 103 1106 4739 1175 1110 1126 4450 171 103 1174 103 1234 103 1637 1107 23410 1115 1515 2221 11760 1128 1221 1103 3200 1115 1840 103 103 1136 1536 1106 103 1172 1105 1115 1103 1278 10078 1106 3244 1651 1121 102 4910 2332 1105 1137 1582 5412 9921 4724 103 1115 103 1204 2262 1118 5986 2934 103 103 182 1477 19202 1114 170 2458 1104 1103 5000 1114 1160 1363 2053 2797 1275 1904 1106 103 1243 1362 1705 13846 1158 1105 1137 8259 1105 103 14106 1451 5138 3322 1104 7724 1517 103 3059 170 1989 1141 1137 1160 1835 9185 102\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","I0429 23:09:23.657326 140629406877568 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","I0429 23:09:23.751209 140629406877568 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","INFO:tensorflow:masked_lm_positions: 4 5 6 17 22 30 32 34 48 49 53 73 75 81 82 85 100 110 118 0\n","I0429 23:09:23.751600 140629406877568 create_pretraining_data.py:161] masked_lm_positions: 4 5 6 17 22 30 32 34 48 49 53 73 75 81 82 85 100 110 118 0\n","INFO:tensorflow:masked_lm_ids: 1116 7162 3576 1705 1125 23223 3469 1138 11396 1132 3244 14115 4597 170 3127 1402 1250 1137 1137 0\n","I0429 23:09:23.751882 140629406877568 create_pretraining_data.py:161] masked_lm_ids: 1116 7162 3576 1705 1125 23223 3469 1138 11396 1132 3244 14115 4597 170 3127 1402 1250 1137 1137 0\n","INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n","I0429 23:09:23.752161 140629406877568 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n","INFO:tensorflow:next_sentence_labels: 1\n","I0429 23:09:23.752304 140629406877568 create_pretraining_data.py:161] next_sentence_labels: 1\n","INFO:tensorflow:*** Example ***\n","I0429 23:09:23.753104 140629406877568 create_pretraining_data.py:149] *** Example ***\n","INFO:tensorflow:tokens: [CLS] be in debt for the rest of your life even if you do get a job in [MASK] field if you take political science [MASK] wikigenderstudiessaafq or [MASK] other useless flu ##ff degree you ##ll either work at star ##cu ##cks or become a teacher even a generic degree in wikiliberalartsiablf has [MASK] career application however [MASK] you pick a stem major and [SEP] [MASK] the position a divorce would put [MASK] [MASK] in this long term thinking [MASK] mit ##iga ##tion of [MASK] [MASK] decision making is not only [MASK] lawyers are [MASK] personality wise but is necessary and reinforced in [MASK] daily job [MASK] this type of behaviour pattern is more con ##g ##rue ##nt to conservative ##s hence why the law field is [SEP]\n","I0429 23:09:23.753280 140629406877568 create_pretraining_data.py:151] tokens: [CLS] be in debt for the rest of your life even if you do get a job in [MASK] field if you take political science [MASK] wikigenderstudiessaafq or [MASK] other useless flu ##ff degree you ##ll either work at star ##cu ##cks or become a teacher even a generic degree in wikiliberalartsiablf has [MASK] career application however [MASK] you pick a stem major and [SEP] [MASK] the position a divorce would put [MASK] [MASK] in this long term thinking [MASK] mit ##iga ##tion of [MASK] [MASK] decision making is not only [MASK] lawyers are [MASK] personality wise but is necessary and reinforced in [MASK] daily job [MASK] this type of behaviour pattern is more con ##g ##rue ##nt to conservative ##s hence why the law field is [SEP]\n","INFO:tensorflow:input_ids: 101 1129 1107 6695 1111 1103 1832 1104 1240 1297 1256 1191 1128 1202 1243 170 2261 1107 103 1768 1191 1128 1321 1741 2598 103 48 1137 103 1168 12277 23896 3101 2178 1128 2339 1719 1250 1120 2851 10182 8770 1137 1561 170 3218 1256 170 13179 2178 1107 1 1144 103 1578 4048 1649 103 1128 3368 170 8175 1558 1105 102 103 1103 1700 170 8126 1156 1508 103 103 1107 1142 1263 1858 2422 103 26410 13499 2116 1104 103 103 2383 1543 1110 1136 1178 103 10630 1132 103 5935 10228 1133 1110 3238 1105 11331 1107 103 3828 2261 103 1142 2076 1104 9151 4844 1110 1167 14255 1403 26930 2227 1106 6588 1116 7544 1725 1103 1644 1768 1110 102\n","I0429 23:09:23.753428 140629406877568 create_pretraining_data.py:161] input_ids: 101 1129 1107 6695 1111 1103 1832 1104 1240 1297 1256 1191 1128 1202 1243 170 2261 1107 103 1768 1191 1128 1321 1741 2598 103 48 1137 103 1168 12277 23896 3101 2178 1128 2339 1719 1250 1120 2851 10182 8770 1137 1561 170 3218 1256 170 13179 2178 1107 1 1144 103 1578 4048 1649 103 1128 3368 170 8175 1558 1105 102 103 1103 1700 170 8126 1156 1508 103 103 1107 1142 1263 1858 2422 103 26410 13499 2116 1104 103 103 2383 1543 1110 1136 1178 103 10630 1132 103 5935 10228 1133 1110 3238 1105 11331 1107 103 3828 2261 103 1142 2076 1104 9151 4844 1110 1167 14255 1403 26930 2227 1106 6588 1116 7544 1725 1103 1644 1768 1110 102\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","I0429 23:09:23.753570 140629406877568 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","I0429 23:09:23.753709 140629406877568 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","INFO:tensorflow:masked_lm_positions: 18 25 28 39 53 57 65 66 72 73 76 79 84 85 91 94 103 106 107 0\n","I0429 23:09:23.753802 140629406877568 create_pretraining_data.py:161] masked_lm_positions: 18 25 28 39 53 57 65 66 72 73 76 79 84 85 91 94 103 106 107 0\n","INFO:tensorflow:masked_lm_ids: 1240 1137 1199 2851 1167 1191 4484 1103 1147 3016 1263 1105 92 1154 1184 1176 1147 2099 1142 0\n","I0429 23:09:23.753917 140629406877568 create_pretraining_data.py:161] masked_lm_ids: 1240 1137 1199 2851 1167 1191 4484 1103 1147 3016 1263 1105 92 1154 1184 1176 1147 2099 1142 0\n","INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n","I0429 23:09:23.754027 140629406877568 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n","INFO:tensorflow:next_sentence_labels: 1\n","I0429 23:09:23.754114 140629406877568 create_pretraining_data.py:161] next_sentence_labels: 1\n","INFO:tensorflow:*** Example ***\n","I0429 23:09:23.754491 140629406877568 create_pretraining_data.py:149] *** Example ***\n","INFO:tensorflow:tokens: [CLS] champions [MASK] affiliate likely to [MASK] [MASK] Ways and boys into stem women may also not go into [MASK] because of [MASK] culture around it for example [MASK] sisters in high school and has to pick [MASK] classes based on what she wants to do as a job she ##s really good at math and [MASK] and shit at en ##gli ##sh and [SEP] normal ##cy i [MASK] h ##yper [MASK] if i had to [MASK] out in public during [MASK] hours i was certain i was branded somehow and i just desperately wanted a leg ##iti ##mizing factor to make me normal i mouthed within school everyone has this same [MASK] but as a highly em ##o i attributed the outsider feeling to being homes [SEP]\n","I0429 23:09:23.754645 140629406877568 create_pretraining_data.py:151] tokens: [CLS] champions [MASK] affiliate likely to [MASK] [MASK] Ways and boys into stem women may also not go into [MASK] because of [MASK] culture around it for example [MASK] sisters in high school and has to pick [MASK] classes based on what she wants to do as a job she ##s really good at math and [MASK] and shit at en ##gli ##sh and [SEP] normal ##cy i [MASK] h ##yper [MASK] if i had to [MASK] out in public during [MASK] hours i was certain i was branded somehow and i just desperately wanted a leg ##iti ##mizing factor to make me normal i mouthed within school everyone has this same [MASK] but as a highly em ##o i attributed the outsider feeling to being homes [SEP]\n","INFO:tensorflow:input_ids: 101 5461 103 8563 2620 1106 103 103 24058 1105 3287 1154 8175 1535 1336 1145 1136 1301 1154 103 1272 1104 103 2754 1213 1122 1111 1859 103 5919 1107 1344 1278 1105 1144 1106 3368 103 3553 1359 1113 1184 1131 3349 1106 1202 1112 170 2261 1131 1116 1541 1363 1120 12523 1105 103 1105 4170 1120 4035 23655 2737 1105 102 2999 3457 178 103 177 24312 103 1191 178 1125 1106 103 1149 1107 1470 1219 103 2005 178 1108 2218 178 1108 11450 5006 1105 178 1198 9600 1458 170 3420 17030 25596 5318 1106 1294 1143 2999 178 22546 1439 1278 2490 1144 1142 1269 103 1133 1112 170 3023 9712 1186 178 6547 1103 26948 2296 1106 1217 4481 102\n","I0429 23:09:23.754782 140629406877568 create_pretraining_data.py:161] input_ids: 101 5461 103 8563 2620 1106 103 103 24058 1105 3287 1154 8175 1535 1336 1145 1136 1301 1154 103 1272 1104 103 2754 1213 1122 1111 1859 103 5919 1107 1344 1278 1105 1144 1106 3368 103 3553 1359 1113 1184 1131 3349 1106 1202 1112 170 2261 1131 1116 1541 1363 1120 12523 1105 103 1105 4170 1120 4035 23655 2737 1105 102 2999 3457 178 103 177 24312 103 1191 178 1125 1106 103 1149 1107 1470 1219 103 2005 178 1108 2218 178 1108 11450 5006 1105 178 1198 9600 1458 170 3420 17030 25596 5318 1106 1294 1143 2999 178 22546 1439 1278 2490 1144 1142 1269 103 1133 1112 170 3023 9712 1186 178 6547 1103 26948 2296 1106 1217 4481 102\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","I0429 23:09:23.754935 140629406877568 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","I0429 23:09:23.755077 140629406877568 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","INFO:tensorflow:masked_lm_positions: 1 2 3 6 7 8 19 22 28 37 44 56 68 70 71 76 81 105 112 0\n","I0429 23:09:23.755169 140629406877568 create_pretraining_data.py:161] masked_lm_positions: 1 2 3 6 7 8 19 22 28 37 44 56 68 70 71 76 81 105 112 0\n","INFO:tensorflow:masked_lm_ids: 2636 1132 1167 1301 1154 1 8175 1103 1139 1123 1106 2598 1108 24312 4484 1129 1278 1221 2296 0\n","I0429 23:09:23.755267 140629406877568 create_pretraining_data.py:161] masked_lm_ids: 2636 1132 1167 1301 1154 1 8175 1103 1139 1123 1106 2598 1108 24312 4484 1129 1278 1221 2296 0\n","INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n","I0429 23:09:23.755371 140629406877568 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n","INFO:tensorflow:next_sentence_labels: 1\n","I0429 23:09:23.755459 140629406877568 create_pretraining_data.py:161] next_sentence_labels: 1\n","INFO:tensorflow:*** Example ***\n","I0429 23:09:23.755810 140629406877568 create_pretraining_data.py:149] *** Example ***\n","INFO:tensorflow:tokens: [CLS] it did Order predict grades in classes or ratings about their actual teaching f ##rank [MASK] said so [MASK] do not require it in districts education anymore the g ##re general test does not [MASK] even test students intended areas [MASK] study not all students go on [MASK] study [MASK] and knowing what [MASK] word go ##ssa [MASK] means certainly does not predict [SEP] things like ca ##s it seeks [MASK] make a wikicriticalthinkingzlbalz out of [MASK] through ca ##s and the e ##e and [MASK] aims to spark [MASK] curiosity in a number of subjects which you most definitely will pursue in one way or another in university [MASK] will [MASK] with me but the i ##b was definitely worth it [MASK] me also this [SEP]\n","I0429 23:09:23.755988 140629406877568 create_pretraining_data.py:151] tokens: [CLS] it did Order predict grades in classes or ratings about their actual teaching f ##rank [MASK] said so [MASK] do not require it in districts education anymore the g ##re general test does not [MASK] even test students intended areas [MASK] study not all students go on [MASK] study [MASK] and knowing what [MASK] word go ##ssa [MASK] means certainly does not predict [SEP] things like ca ##s it seeks [MASK] make a wikicriticalthinkingzlbalz out of [MASK] through ca ##s and the e ##e and [MASK] aims to spark [MASK] curiosity in a number of subjects which you most definitely will pursue in one way or another in university [MASK] will [MASK] with me but the i ##b was definitely worth it [MASK] me also this [SEP]\n","INFO:tensorflow:input_ids: 101 1122 1225 2864 17163 7093 1107 3553 1137 8532 1164 1147 4315 3679 175 14687 103 1163 1177 103 1202 1136 4752 1122 1107 4210 1972 4169 1103 176 1874 1704 2774 1674 1136 103 1256 2774 1651 3005 1877 103 2025 1136 1155 1651 1301 1113 103 2025 103 1105 3650 1184 103 1937 1301 11655 103 2086 4664 1674 1136 17163 102 1614 1176 11019 1116 1122 11053 103 1294 170 30 1149 1104 103 1194 11019 1116 1105 1103 174 1162 1105 103 8469 1106 14798 103 11380 1107 170 1295 1104 5174 1134 1128 1211 5397 1209 6799 1107 1141 1236 1137 1330 1107 2755 103 1209 103 1114 1143 1133 1103 178 1830 1108 5397 3869 1122 103 1143 1145 1142 102\n","I0429 23:09:23.756199 140629406877568 create_pretraining_data.py:161] input_ids: 101 1122 1225 2864 17163 7093 1107 3553 1137 8532 1164 1147 4315 3679 175 14687 103 1163 1177 103 1202 1136 4752 1122 1107 4210 1972 4169 1103 176 1874 1704 2774 1674 1136 103 1256 2774 1651 3005 1877 103 2025 1136 1155 1651 1301 1113 103 2025 103 1105 3650 1184 103 1937 1301 11655 103 2086 4664 1674 1136 17163 102 1614 1176 11019 1116 1122 11053 103 1294 170 30 1149 1104 103 1194 11019 1116 1105 1103 174 1162 1105 103 8469 1106 14798 103 11380 1107 170 1295 1104 5174 1134 1128 1211 5397 1209 6799 1107 1141 1236 1137 1330 1107 2755 103 1209 103 1114 1143 1133 1103 178 1830 1108 5397 3869 1122 103 1143 1145 1142 102\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","I0429 23:09:23.756347 140629406877568 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","I0429 23:09:23.756482 140629406877568 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","INFO:tensorflow:masked_lm_positions: 3 16 19 25 35 41 48 50 54 58 60 71 77 84 86 90 110 112 123 0\n","I0429 23:09:23.756574 140629406877568 create_pretraining_data.py:161] masked_lm_positions: 3 16 19 25 35 41 48 50 54 58 60 71 77 84 86 90 110 112 123 0\n","INFO:tensorflow:masked_lm_ids: 1136 1162 1195 3218 9073 1104 1106 6686 1103 4027 4664 1106 1128 1162 1122 1240 1211 23423 1111 0\n","I0429 23:09:23.756672 140629406877568 create_pretraining_data.py:161] masked_lm_ids: 1136 1162 1195 3218 9073 1104 1106 6686 1103 4027 4664 1106 1128 1162 1122 1240 1211 23423 1111 0\n","INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n","I0429 23:09:23.756775 140629406877568 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n","INFO:tensorflow:next_sentence_labels: 1\n","I0429 23:09:23.756880 140629406877568 create_pretraining_data.py:161] next_sentence_labels: 1\n","INFO:tensorflow:*** Example ***\n","I0429 23:09:23.757283 140629406877568 create_pretraining_data.py:149] *** Example ***\n","INFO:tensorflow:tokens: [CLS] your life and em ##power you as a human ##rat [MASK] percent is [MASK] s ##cott mon ##ty [MASK] chief executive officer and founder of s [MASK] mon ##ty strategies took the floor next to explain the seven deadly [MASK] [MASK] deadly sins [MASK] wikisocialmediasaqllfz [MASK] seventy one percent of consumers have ended their relationship ##ω a company due to poor customer service [SEP] in ##dia by [MASK] an [MASK] of un ##so ##cia [MASK] drone ##s who do not appreciate fine ##r things [MASK] came articles about [MASK] there is a discrimination against students of wikihumanitiesseiez [MASK] how they are treated with di ##s ##dain and why in [MASK] should concentrate on humanities if [MASK] pins to be a world leader the v ##aj ##pa [SEP]\n","I0429 23:09:23.757443 140629406877568 create_pretraining_data.py:151] tokens: [CLS] your life and em ##power you as a human ##rat [MASK] percent is [MASK] s ##cott mon ##ty [MASK] chief executive officer and founder of s [MASK] mon ##ty strategies took the floor next to explain the seven deadly [MASK] [MASK] deadly sins [MASK] wikisocialmediasaqllfz [MASK] seventy one percent of consumers have ended their relationship ##ω a company due to poor customer service [SEP] in ##dia by [MASK] an [MASK] of un ##so ##cia [MASK] drone ##s who do not appreciate fine ##r things [MASK] came articles about [MASK] there is a discrimination against students of wikihumanitiesseiez [MASK] how they are treated with di ##s ##dain and why in [MASK] should concentrate on humanities if [MASK] pins to be a world leader the v ##aj ##pa [SEP]\n","INFO:tensorflow:input_ids: 101 1240 1297 1105 9712 9447 1128 1112 170 1769 7625 103 3029 1110 103 188 11627 19863 2340 103 2705 3275 2575 1105 3249 1104 188 103 19863 2340 10700 1261 1103 1837 1397 1106 4137 1103 1978 10310 103 103 10310 19452 103 8 103 12911 1141 3029 1104 11060 1138 2207 1147 2398 28360 170 1419 1496 1106 2869 8132 1555 102 1107 7168 1118 103 1126 103 1104 8362 7301 6052 103 22020 1116 1150 1202 1136 8856 2503 1197 1614 103 1338 4237 1164 103 1175 1110 170 9480 1222 1651 1104 2 103 1293 1152 1132 5165 1114 4267 1116 21314 1105 1725 1107 103 1431 10191 1113 26419 1191 103 18607 1106 1129 170 1362 2301 1103 191 12487 4163 102\n","I0429 23:09:23.853803 140629406877568 create_pretraining_data.py:161] input_ids: 101 1240 1297 1105 9712 9447 1128 1112 170 1769 7625 103 3029 1110 103 188 11627 19863 2340 103 2705 3275 2575 1105 3249 1104 188 103 19863 2340 10700 1261 1103 1837 1397 1106 4137 1103 1978 10310 103 103 10310 19452 103 8 103 12911 1141 3029 1104 11060 1138 2207 1147 2398 28360 170 1419 1496 1106 2869 8132 1555 102 1107 7168 1118 103 1126 103 1104 8362 7301 6052 103 22020 1116 1150 1202 1136 8856 2503 1197 1614 103 1338 4237 1164 103 1175 1110 170 9480 1222 1651 1104 2 103 1293 1152 1132 5165 1114 4267 1116 21314 1105 1725 1107 103 1431 10191 1113 26419 1191 103 18607 1106 1129 170 1362 2301 1103 191 12487 4163 102\n","INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","I0429 23:09:23.855258 140629406877568 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","I0429 23:09:23.855710 140629406877568 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","INFO:tensorflow:masked_lm_positions: 10 11 14 19 27 40 41 44 46 56 68 70 75 85 89 98 110 116 117 0\n","I0429 23:09:23.860397 140629406877568 create_pretraining_data.py:161] masked_lm_positions: 10 11 14 19 27 40 41 44 46 56 68 70 75 85 89 98 110 116 117 0\n","INFO:tensorflow:masked_lm_ids: 1105 1512 1661 1103 11627 19452 1978 1104 6213 1114 3780 2306 2165 1190 1293 1105 7168 1122 3349 0\n","I0429 23:09:23.860532 140629406877568 create_pretraining_data.py:161] masked_lm_ids: 1105 1512 1661 1103 11627 19452 1978 1104 6213 1114 3780 2306 2165 1190 1293 1105 7168 1122 3349 0\n","INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n","I0429 23:09:23.860640 140629406877568 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n","INFO:tensorflow:next_sentence_labels: 1\n","I0429 23:09:23.860723 140629406877568 create_pretraining_data.py:161] next_sentence_labels: 1\n","INFO:tensorflow:Wrote 395026 total instances\n","I0429 23:10:49.031875 140629406877568 create_pretraining_data.py:166] Wrote 395026 total instances\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oFhqHCzUjPw1","colab_type":"text"},"source":["### Preparing to PreTrain"]},{"cell_type":"markdown","metadata":{"id":"1u35Bx8D2CsO","colab_type":"text"},"source":["BEFORE EXECUTING THESE,\n","\n","\n","1.   We need to upload the weight and config files to a Google Cloud Storage Bucket from where this BERT model will get and save its data.\n","2.   To do this, create a new Google Cloud Storage Bucket\n","3.   Then upload all the files we downloaded from the Google BERT Pre-trained model repository to the Cloud Bucket in a folder whose name is the same as that in `MODEL_DIR` variable below.\n","4.   Replace the `vocab.txt` file on the Cloud Bucket with the `vocab.txt` file we modified and used to build the pre-training data. \n","5.   Make sure that the `BUCKET_NAME` variable below matches the exact name of the Google Cloud Bucket we created, and the name of the vocab file matches the one we uploaded.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"QQiFd_DVjSXv","colab_type":"code","colab":{}},"source":["BUCKET_NAME = \"<bucket_name>\" #@param {type:\"string\"}\n","MODEL_DIR = \"<dir_name>\" #@param {type:\"string\"}\n","PRETRAINING_DIR = \"<dir_name>\" #@param {type:\"string\"}\n","VOC_FNAME = \"vocab.txt\" #@param {type:\"string\"}\n","\n","tf.gfile.MkDir(MODEL_DIR)\n","\n","if not BUCKET_NAME:\n","  log.warning(\"WARNING: BUCKET_NAME is not set. \"\n","              \"You will not be able to train the model.\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jWxeJ_ZyjalO","colab_type":"code","outputId":"dfa20ded-2ef7-4f7e-c7db-d2bb478b9db5","executionInfo":{"status":"ok","timestamp":1588219909780,"user_tz":300,"elapsed":10209,"user":{"displayName":"Eadhunath Venghatesan","photoUrl":"","userId":"10975244633908642389"}},"colab":{"base_uri":"https://localhost:8080/","height":269}},"source":["# Copy the pretraining data -> .tfrecord files to the GCS Bucket\n","if BUCKET_NAME:\n","  !gsutil -m cp -r $PRETRAINING_DIR gs://$BUCKET_NAME"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Copying file://pretraining_data/shard_0000.tfrecord [Content-Type=application/octet-stream]...\n","==> NOTE: You are uploading one or more large file(s), which would run\n","significantly faster if you enable parallel composite uploads. This\n","feature can be enabled by editing the\n","\"parallel_composite_upload_threshold\" value in your .boto\n","configuration file. However, note that if you do this large files will\n","be uploaded as `composite objects\n","<https://cloud.google.com/storage/docs/composite-objects>`_,which\n","means that any user who downloads such objects will need to have a\n","compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n","without a compiled crcmod, computing checksums on composite objects is\n","so slow that gsutil disables downloads of composite objects.\n","\n","/ [1/1 files][315.3 MiB/315.3 MiB] 100% Done                                    \n","Operation completed over 1 objects/315.3 MiB.                                    \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HRTw1pnWj8Ao","colab_type":"text"},"source":["### Pre-train"]},{"cell_type":"code","metadata":{"id":"ItyQYnSWjigw","colab_type":"code","outputId":"9940b1e3-2974-412f-df6b-8f9f3144b762","executionInfo":{"status":"ok","timestamp":1588219964038,"user_tz":300,"elapsed":1680,"user":{"displayName":"Eadhunath Venghatesan","photoUrl":"","userId":"10975244633908642389"}},"colab":{"base_uri":"https://localhost:8080/","height":118}},"source":["# Input data pipeline config\n","TRAIN_BATCH_SIZE = 128 #@param {type:\"integer\"}\n","MAX_PREDICTIONS = 20 #@param {type:\"integer\"}\n","MAX_SEQ_LENGTH = 128 #@param {type:\"integer\"}\n","MASKED_LM_PROB = 0.15 #@param\n","\n","# Training procedure config\n","EVAL_BATCH_SIZE = 64\n","LEARNING_RATE = 1e-4\n","TRAIN_STEPS = 10000 #@param {type:\"integer\"}\n","SAVE_CHECKPOINTS_STEPS = 2500 #@param {type:\"integer\"}\n","NUM_TPU_CORES = 8\n","\n","if BUCKET_NAME:\n","  BUCKET_PATH = \"gs://{}\".format(BUCKET_NAME)\n","else:\n","  BUCKET_PATH = \".\"\n","\n","BERT_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, MODEL_DIR)\n","DATA_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, PRETRAINING_DIR)\n","\n","VOCAB_FILE = os.path.join(BERT_GCS_DIR, VOC_FNAME)\n","CONFIG_FILE = os.path.join(BERT_GCS_DIR, \"bert_config.json\")\n","\n","# If we want to start off from Google's Checkpoint :\n","INIT_CHECKPOINT = os.path.join(BERT_GCS_DIR, \"bert_model.ckpt\")\n","\n","# If we want to start off from our own latest checkpoint\n","# INIT_CHECKPOINT = tf.train.latest_checkpoint(BERT_GCS_DIR)\n","\n","\n","bert_config = modeling.BertConfig.from_json_file(CONFIG_FILE)\n","input_files = tf.gfile.Glob(os.path.join(DATA_GCS_DIR,'*tfrecord'))\n","\n","log.info(\"Using checkpoint: {}\".format(INIT_CHECKPOINT))\n","log.info(\"Using {} data shards\".format(len(input_files)))\n","log.info(\"BERT_GCS_DIR {}\".format(BERT_GCS_DIR))\n","log.info(\"Using {} vocab file\".format(VOCAB_FILE))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["2020-04-29 23:12:42,740 :  From /content/bert/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n","\n","2020-04-29 23:12:44,176 :  Using checkpoint: gs://bert_apr/bert_model_final/bert_model.ckpt\n","2020-04-29 23:12:44,179 :  Using 1 data shards\n","2020-04-29 23:12:44,182 :  BERT_GCS_DIR gs://bert_apr/bert_model_final\n","2020-04-29 23:12:44,184 :  Using gs://bert_apr/bert_model_final/vocab.txt vocab file\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"n2zSzaCVlz5d","colab_type":"code","outputId":"c75bbd7c-62f9-46b1-c5ca-aeec3ce506f5","executionInfo":{"status":"ok","timestamp":1588219966264,"user_tz":300,"elapsed":3174,"user":{"displayName":"Eadhunath Venghatesan","photoUrl":"","userId":"10975244633908642389"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Verifying if the correct vocab file is being used in the pre-training,\n","# Reading the vocab.txt file on the cloud bucket.\n","\n","!gsutil cat $VOCAB_FILE | head -107"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[PAD]\n","wikiliberalartsiablf\n","wikihumanitiesseiez\n","wikiacademictermsfssll\n","wikiwhitelistiazebq\n","wikiprofessoriblaoo\n","wikisocialscienceszblai\n","wikiredditeazqoos\n","wikisocialmediasaqllfz\n","wikiblogeebfs\n","wikiunitedstatesefeflso\n","wikicurriculumflqqae\n","wikigraduateschoolefbbza\n","wikigraphicsinterchangeformatizloz\n","wikiportablenetworkgraphicszfeob\n","wikijpegibooq\n","wikidirectimagefunctoreozzlff\n","wikimathematicsiaaei\n","wikituitionzfqezo\n","wikilolzaaelq\n","wikidoctorofphilosophyalls\n","wikiundergraduateeducationzilzaq\n","wikishiteazzi\n","wikitwitterqqaaial\n","wikifascismiiosf\n","wikihomeworkezbsfb\n","wikistudentfinancialaidiiloqaa\n","wikiinternzqazzi\n","wikidissertationeoozis\n","wikilaboureconomicsiaila\n","wikicriticalthinkingzlbalz\n","wikipermalinkalaqzb\n","wikifacebooklszqela\n","wikiwikipediasofelef\n","wikihypertexttransferprotocoloversecuresocketlayeriesab\n","wikiemailqlea\n","wikiuniversityeilqf\n","wikischolarshipiaoeel\n","wikidudeeesleq\n","wikitheresioisibsz\n","wikisexismzlibs\n","wikiharvarduniversityiafzbsoi\n","wikiinterdisciplinarityiszoi\n","wikiyoutubeeszflbb\n","wikiartsintegrationqqebfel\n","wikipoliticalsciencezfeaa\n","wikicomputerscienceseze\n","wikicaliforniasfol\n","wikigenderstudiessaafq\n","wikijournalismisqza\n","wikidonaldtrumpfafazlz\n","wikilaptopiqasaf\n","wikibullshitfofaf\n","wikieuropeqzeq\n","wikiivyleagueifqls\n","wikisakezasoo\n","wikiworkingtimessziba\n","wikifucksslslzz\n","wikicapitalismsfib\n","wikibiologyqizlbez\n","wikibarackobamasefebb\n","wikisyllabusiqllzas\n","wikifineartqoeil\n","wikiyaleuniversityefzle\n","wikiwikiezasi\n","wikichemistrysiao\n","wikiessayioelf\n","wikiunemploymenteilfi\n","wikiliberalartscollegeiabiz\n","wikitenureeifqsi\n","wikicarnegiemellonuniversityfaoqe\n","wikiwowrecordingibfzoss\n","wikinonprofitorganizationlzfal\n","wikimarxismiqofose\n","wikiaccountancyzsqe\n","wikinaturalscienceeaaqo\n","wikisieaq\n","wikiphpzfiei\n","wikiidiotisoof\n","wikipodcastioeffse\n","wikistemfieldsefelbbe\n","wikiaccountabilityibiqle\n","wikiliteracyiafsb\n","wikistudentloanfbfqqo\n","wikiideologylsfbq\n","wikistudentsunionsiqqoi\n","wikielectricalengineeringqsei\n","wikirussiazseqi\n","wikiseminarbllooo\n","wikirhetoriczsffl\n","wikiworkexperienceiiifloe\n","wikitextbookzsfioa\n","wikiempathyeozeiq\n","wikifeminismiiias\n","wikiacademicsenateezieiiq\n","wikimechanicalengineeringiqsza\n","wikistudyabroadiabeiff\n","wikielectroniclearninglfbll\n","wikistandardizedtestzzsqli\n","wikitutorzqqqbs\n","[UNK]\n","[CLS]\n","[SEP]\n","[MASK]\n","wikistateschoolsleqee\n","[unused101]\n","!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MMpVQKVVmpqZ","colab_type":"code","outputId":"9524b3f1-2a39-4cbf-9fe4-3622b1b9c7af","executionInfo":{"status":"ok","timestamp":1588219985605,"user_tz":300,"elapsed":1427,"user":{"displayName":"Eadhunath Venghatesan","photoUrl":"","userId":"10975244633908642389"}},"colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["model_fn = model_fn_builder(\n","      bert_config=bert_config,\n","      init_checkpoint=INIT_CHECKPOINT,\n","      learning_rate=LEARNING_RATE,\n","      num_train_steps=TRAIN_STEPS,\n","      num_warmup_steps=10,\n","      use_tpu=USE_TPU,\n","      use_one_hot_embeddings=True)\n","\n","tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n","\n","run_config = tf.contrib.tpu.RunConfig(\n","    cluster=tpu_cluster_resolver,\n","    model_dir=BERT_GCS_DIR,\n","    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n","    tpu_config=tf.contrib.tpu.TPUConfig(\n","        iterations_per_loop=SAVE_CHECKPOINTS_STEPS,\n","        num_shards=NUM_TPU_CORES,\n","        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n","\n","estimator = tf.contrib.tpu.TPUEstimator(\n","    use_tpu=USE_TPU,\n","    model_fn=model_fn,\n","    config=run_config,\n","    train_batch_size=TRAIN_BATCH_SIZE,\n","    eval_batch_size=EVAL_BATCH_SIZE)\n","  \n","train_input_fn = input_fn_builder(\n","        input_files=input_files,\n","        max_seq_length=MAX_SEQ_LENGTH,\n","        max_predictions_per_seq=MAX_PREDICTIONS,\n","        is_training=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["2020-04-29 23:13:05,718 :  Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7fdb90c8dae8>) includes params argument, but params are not passed to Estimator.\n","2020-04-29 23:13:05,722 :  Using config: {'_model_dir': 'gs://bert_apr/bert_model_final', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 2500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","cluster_def {\n","  job {\n","    name: \"worker\"\n","    tasks {\n","      key: 0\n","      value: \"10.39.244.154:8470\"\n","    }\n","  }\n","}\n","isolate_session_state: true\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fdb35482470>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.39.244.154:8470', '_evaluation_master': 'grpc://10.39.244.154:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=2500, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7fda77fd2860>}\n","2020-04-29 23:13:05,724 :  _TPUContext: eval_on_tpu True\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"YalnxEWdnQZe","colab_type":"code","outputId":"435f6ad6-0949-410a-95de-9dd20be58e6b","executionInfo":{"status":"ok","timestamp":1588221218884,"user_tz":300,"elapsed":1224947,"user":{"displayName":"Eadhunath Venghatesan","photoUrl":"","userId":"10975244633908642389"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["estimator.train(input_fn=train_input_fn, max_steps=TRAIN_STEPS)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["2020-04-29 23:13:14,549 :  Querying Tensorflow master (grpc://10.39.244.154:8470) for TPU system metadata.\n","2020-04-29 23:13:14,580 :  Found TPU system:\n","2020-04-29 23:13:14,581 :  *** Num TPU Cores: 8\n","2020-04-29 23:13:14,582 :  *** Num TPU Workers: 1\n","2020-04-29 23:13:14,585 :  *** Num TPU Cores Per Worker: 8\n","2020-04-29 23:13:14,587 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 15751215702725798585)\n","2020-04-29 23:13:14,589 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 17154297229405720259)\n","2020-04-29 23:13:14,591 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 708830170763276420)\n","2020-04-29 23:13:14,592 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 12685520322608514198)\n","2020-04-29 23:13:14,594 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 13560109840942124914)\n","2020-04-29 23:13:14,596 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 9245110623436857921)\n","2020-04-29 23:13:14,596 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 6710977302737588046)\n","2020-04-29 23:13:14,599 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 217849255487262044)\n","2020-04-29 23:13:14,601 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 9272335680116686909)\n","2020-04-29 23:13:14,606 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 15037758823718979103)\n","2020-04-29 23:13:14,607 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7964654767410372320)\n","2020-04-29 23:13:14,636 :  From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","2020-04-29 23:13:14,638 :  From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n","2020-04-29 23:13:14,668 :  Calling model_fn.\n","2020-04-29 23:13:14,670 :  From /content/bert/run_pretraining.py:337: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n","\n","2020-04-29 23:13:14,685 :  From /content/bert/run_pretraining.py:368: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.experimental.parallel_interleave(...)`.\n","2020-04-29 23:13:14,686 :  From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n","2020-04-29 23:13:17,086 :  From /content/bert/run_pretraining.py:385: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.experimental.map_and_batch(...)`.\n","2020-04-29 23:13:17,087 :  From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/data/python/ops/batching.py:276: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n","2020-04-29 23:13:17,170 :  Entity <function input_fn_builder.<locals>.input_fn.<locals>.<lambda> at 0x7fda7a1efea0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n","2020-04-29 23:13:17,172 :  From /content/bert/run_pretraining.py:393: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n","\n","2020-04-29 23:13:17,196 :  From /content/bert/run_pretraining.py:400: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","2020-04-29 23:13:17,244 :  Found small feature: next_sentence_labels [16, 1]\n","2020-04-29 23:13:17,248 :  Found small feature: next_sentence_labels [16, 1]\n","2020-04-29 23:13:17,254 :  Found small feature: next_sentence_labels [16, 1]\n","2020-04-29 23:13:17,259 :  Found small feature: next_sentence_labels [16, 1]\n","2020-04-29 23:13:17,265 :  Found small feature: next_sentence_labels [16, 1]\n","2020-04-29 23:13:17,270 :  Found small feature: next_sentence_labels [16, 1]\n","2020-04-29 23:13:17,275 :  Found small feature: next_sentence_labels [16, 1]\n","2020-04-29 23:13:17,281 :  Found small feature: next_sentence_labels [16, 1]\n","2020-04-29 23:13:17,333 :  From /content/bert/run_pretraining.py:117: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n","\n","2020-04-29 23:13:17,334 :  *** Features ***\n","2020-04-29 23:13:17,335 :    name = input_ids, shape = (16, 128)\n","2020-04-29 23:13:17,336 :    name = input_mask, shape = (16, 128)\n","2020-04-29 23:13:17,336 :    name = masked_lm_ids, shape = (16, 20)\n","2020-04-29 23:13:17,337 :    name = masked_lm_positions, shape = (16, 20)\n","2020-04-29 23:13:17,338 :    name = masked_lm_weights, shape = (16, 20)\n","2020-04-29 23:13:17,338 :    name = next_sentence_labels, shape = (16, 1)\n","2020-04-29 23:13:17,339 :    name = segment_ids, shape = (16, 128)\n","2020-04-29 23:13:17,341 :  From bert/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n","\n","2020-04-29 23:13:17,348 :  From bert/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING: Entity <function input_fn_builder.<locals>.input_fn.<locals>.<lambda> at 0x7fda7a1efea0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n"],"name":"stdout"},{"output_type":"stream","text":["2020-04-29 23:13:17,408 :  From bert/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n","\n","2020-04-29 23:13:17,471 :  From bert/modeling.py:358: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","2020-04-29 23:13:17,497 :  From bert/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.Dense instead.\n","2020-04-29 23:13:17,502 :  From /tensorflow-1.15.2/python3.6/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n","2020-04-29 23:13:20,291 :  From /content/bert/run_pretraining.py:150: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n","\n","2020-04-29 23:13:20,956 :  **** Trainable Variables ****\n","2020-04-29 23:13:20,957 :    name = bert/embeddings/word_embeddings:0, shape = (28996, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,958 :    name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,960 :    name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,962 :    name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,963 :    name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,964 :    name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,966 :    name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,967 :    name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,968 :    name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,969 :    name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,969 :    name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,970 :    name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,971 :    name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,972 :    name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,973 :    name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,974 :    name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,974 :    name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,975 :    name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,976 :    name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,977 :    name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,977 :    name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,978 :    name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,979 :    name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,979 :    name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,980 :    name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,981 :    name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,982 :    name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,982 :    name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,983 :    name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,983 :    name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,984 :    name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,985 :    name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,986 :    name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,986 :    name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,987 :    name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,988 :    name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,989 :    name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,990 :    name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,991 :    name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,991 :    name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,992 :    name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,993 :    name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,993 :    name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,994 :    name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,995 :    name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,996 :    name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,998 :    name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,998 :    name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","2020-04-29 23:13:20,999 :    name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,000 :    name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,000 :    name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,001 :    name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,002 :    name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,002 :    name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,003 :    name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,004 :    name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,005 :    name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,005 :    name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,006 :    name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,007 :    name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,008 :    name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,008 :    name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,009 :    name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,010 :    name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,010 :    name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,011 :    name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,012 :    name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,013 :    name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,013 :    name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,014 :    name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,015 :    name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,016 :    name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,017 :    name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,018 :    name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,018 :    name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,019 :    name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,020 :    name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,021 :    name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,021 :    name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,022 :    name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,023 :    name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,023 :    name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,024 :    name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,024 :    name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,025 :    name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,026 :    name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,026 :    name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,027 :    name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,027 :    name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,028 :    name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,028 :    name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,029 :    name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,029 :    name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,030 :    name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,030 :    name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,031 :    name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,034 :    name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,035 :    name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,035 :    name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,036 :    name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,037 :    name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,037 :    name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,038 :    name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,039 :    name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,039 :    name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,040 :    name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,041 :    name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,042 :    name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,042 :    name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,043 :    name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,043 :    name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,044 :    name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,045 :    name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,045 :    name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,046 :    name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,047 :    name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,047 :    name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,048 :    name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,049 :    name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,049 :    name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,050 :    name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,051 :    name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,051 :    name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,054 :    name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,055 :    name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,056 :    name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,056 :    name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,057 :    name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,060 :    name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,061 :    name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,063 :    name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,064 :    name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,065 :    name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,066 :    name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,067 :    name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,068 :    name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,069 :    name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,070 :    name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,071 :    name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,072 :    name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,073 :    name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,074 :    name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,075 :    name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,076 :    name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,076 :    name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,077 :    name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,078 :    name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,079 :    name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,080 :    name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,081 :    name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,081 :    name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,082 :    name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,084 :    name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,085 :    name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,085 :    name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,086 :    name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,087 :    name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,088 :    name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,089 :    name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,090 :    name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,091 :    name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,092 :    name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,093 :    name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,093 :    name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,095 :    name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,095 :    name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,096 :    name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,097 :    name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,098 :    name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,099 :    name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,100 :    name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,101 :    name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,102 :    name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,103 :    name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,104 :    name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,105 :    name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,106 :    name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,107 :    name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,108 :    name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,109 :    name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,109 :    name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,110 :    name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,112 :    name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,112 :    name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,113 :    name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,114 :    name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,115 :    name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,116 :    name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,117 :    name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,118 :    name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,119 :    name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,120 :    name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,121 :    name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,121 :    name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,122 :    name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,123 :    name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,124 :    name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,125 :    name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,126 :    name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,127 :    name = cls/predictions/transform/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,127 :    name = cls/predictions/transform/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,128 :    name = cls/predictions/transform/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,129 :    name = cls/predictions/transform/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,130 :    name = cls/predictions/output_bias:0, shape = (28996,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,131 :    name = cls/seq_relationship/output_weights:0, shape = (2, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,132 :    name = cls/seq_relationship/output_bias:0, shape = (2,), *INIT_FROM_CKPT*\n","2020-04-29 23:13:21,133 :  From bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n","\n","2020-04-29 23:13:21,137 :  From bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n","\n","2020-04-29 23:13:21,746 :  From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","2020-04-29 23:13:35,380 :  From /content/bert/run_pretraining.py:160: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.\n","\n","2020-04-29 23:13:37,066 :  From /content/bert/run_pretraining.py:161: The name tf.train.Scaffold is deprecated. Please use tf.compat.v1.train.Scaffold instead.\n","\n","2020-04-29 23:13:38,087 :  Create CheckpointSaverHook.\n","2020-04-29 23:13:41,985 :  Done calling model_fn.\n","2020-04-29 23:13:44,713 :  TPU job name worker\n","2020-04-29 23:13:46,283 :  Graph was finalized.\n","2020-04-29 23:14:10,395 :  Running local_init_op.\n","2020-04-29 23:14:10,900 :  Done running local_init_op.\n","2020-04-29 23:14:18,850 :  Saving checkpoints for 0 into gs://bert_apr/bert_model_final/model.ckpt.\n","2020-04-29 23:14:32,842 :  From /tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:751: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Prefer Variable.assign which has equivalent behavior in 2.X.\n","2020-04-29 23:14:33,900 :  Initialized dataset iterators in 0 seconds\n","2020-04-29 23:14:33,901 :  Installing graceful shutdown hook.\n","2020-04-29 23:14:33,909 :  Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n","2020-04-29 23:14:33,913 :  Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n","\n","2020-04-29 23:14:33,922 :  Init TPU system\n","2020-04-29 23:14:33,954 :  Initialized TPU in 0 seconds\n","2020-04-29 23:14:33,955 :  Starting infeed thread controller.\n","2020-04-29 23:14:33,956 :  Starting outfeed thread controller.\n","2020-04-29 23:14:34,483 :  Enqueue next (2500) batch(es) of data to infeed.\n","2020-04-29 23:14:34,484 :  Dequeue next (2500) batch(es) of data from outfeed.\n","2020-04-29 23:15:08,323 :  Outfeed finished for iteration (0, 0)\n","2020-04-29 23:16:08,361 :  Outfeed finished for iteration (0, 591)\n","2020-04-29 23:17:08,397 :  Outfeed finished for iteration (0, 1182)\n","2020-04-29 23:18:08,436 :  Outfeed finished for iteration (0, 1773)\n","2020-04-29 23:19:08,477 :  Outfeed finished for iteration (0, 2364)\n","2020-04-29 23:19:22,933 :  Saving checkpoints for 2500 into gs://bert_apr/bert_model_final/model.ckpt.\n","2020-04-29 23:19:37,999 :  loss = 2.0342393, step = 2500\n","2020-04-29 23:19:38,005 :  Enqueue next (2500) batch(es) of data to infeed.\n","2020-04-29 23:19:38,007 :  Dequeue next (2500) batch(es) of data from outfeed.\n","2020-04-29 23:20:08,498 :  Outfeed finished for iteration (1, 239)\n","2020-04-29 23:21:08,530 :  Outfeed finished for iteration (1, 830)\n","2020-04-29 23:22:08,564 :  Outfeed finished for iteration (1, 1421)\n","2020-04-29 23:23:08,597 :  Outfeed finished for iteration (1, 2012)\n","2020-04-29 23:23:58,781 :  Saving checkpoints for 5000 into gs://bert_apr/bert_model_final/model.ckpt.\n","2020-04-29 23:24:16,737 :  loss = 1.7703768, step = 5000 (278.737 sec)\n","2020-04-29 23:24:16,740 :  global_step/sec: 8.96908\n","2020-04-29 23:24:16,744 :  examples/sec: 1148.04\n","2020-04-29 23:24:16,747 :  Enqueue next (2500) batch(es) of data to infeed.\n","2020-04-29 23:24:16,750 :  Dequeue next (2500) batch(es) of data from outfeed.\n","2020-04-29 23:24:17,952 :  Outfeed finished for iteration (2, 0)\n","2020-04-29 23:25:17,999 :  Outfeed finished for iteration (2, 591)\n","2020-04-29 23:26:18,047 :  Outfeed finished for iteration (2, 1182)\n","2020-04-29 23:27:18,094 :  Outfeed finished for iteration (2, 1773)\n","2020-04-29 23:28:18,141 :  Outfeed finished for iteration (2, 2364)\n","2020-04-29 23:28:32,581 :  Saving checkpoints for 7500 into gs://bert_apr/bert_model_final/model.ckpt.\n","2020-04-29 23:28:49,562 :  loss = 1.8816596, step = 7500 (272.825 sec)\n","2020-04-29 23:28:49,567 :  global_step/sec: 9.16329\n","2020-04-29 23:28:49,568 :  examples/sec: 1172.9\n","2020-04-29 23:28:49,573 :  Enqueue next (2500) batch(es) of data to infeed.\n","2020-04-29 23:28:49,576 :  Dequeue next (2500) batch(es) of data from outfeed.\n","2020-04-29 23:29:18,194 :  Outfeed finished for iteration (3, 270)\n","2020-04-29 23:30:18,242 :  Outfeed finished for iteration (3, 861)\n","2020-04-29 23:31:18,290 :  Outfeed finished for iteration (3, 1452)\n","2020-04-29 23:32:18,338 :  Outfeed finished for iteration (3, 2043)\n","2020-04-29 23:33:05,413 :  Saving checkpoints for 10000 into gs://bert_apr/bert_model_final/model.ckpt.\n","2020-04-29 23:33:37,452 :  loss = 1.426305, step = 10000 (287.891 sec)\n","2020-04-29 23:33:37,455 :  global_step/sec: 8.68394\n","2020-04-29 23:33:37,457 :  examples/sec: 1111.54\n","2020-04-29 23:33:37,937 :  Stop infeed thread controller\n","2020-04-29 23:33:37,938 :  Shutting down InfeedController thread.\n","2020-04-29 23:33:37,943 :  InfeedController received shutdown signal, stopping.\n","2020-04-29 23:33:37,944 :  Infeed thread finished, shutting down.\n","2020-04-29 23:33:37,947 :  infeed marked as finished\n","2020-04-29 23:33:37,949 :  Stop output thread controller\n","2020-04-29 23:33:37,951 :  Shutting down OutfeedController thread.\n","2020-04-29 23:33:37,953 :  OutfeedController received shutdown signal, stopping.\n","2020-04-29 23:33:37,954 :  Outfeed thread finished, shutting down.\n","2020-04-29 23:33:37,955 :  outfeed marked as finished\n","2020-04-29 23:33:37,957 :  Shutdown TPU system.\n","2020-04-29 23:33:38,981 :  Loss for final step: 1.426305.\n","2020-04-29 23:33:38,984 :  training_loop marked as finished\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow_estimator.python.estimator.tpu.tpu_estimator.TPUEstimator at 0x7fda77fd2b00>"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"LxsXIbgXnTuE","colab_type":"code","outputId":"d6cf0aa1-28e8-46e0-f76d-4b360565b533","executionInfo":{"status":"ok","timestamp":1588225721439,"user_tz":300,"elapsed":4488374,"user":{"displayName":"Eadhunath Venghatesan","photoUrl":"","userId":"10975244633908642389"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["estimator.train(input_fn=train_input_fn, max_steps=5*TRAIN_STEPS)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["2020-04-29 23:33:54,505 :  Calling model_fn.\n","2020-04-29 23:33:54,555 :  Entity <function input_fn_builder.<locals>.input_fn.<locals>.<lambda> at 0x7fda75449ae8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n","2020-04-29 23:33:54,593 :  Found small feature: next_sentence_labels [16, 1]\n","2020-04-29 23:33:54,596 :  Found small feature: next_sentence_labels [16, 1]\n","2020-04-29 23:33:54,601 :  Found small feature: next_sentence_labels [16, 1]\n","2020-04-29 23:33:54,605 :  Found small feature: next_sentence_labels [16, 1]\n","2020-04-29 23:33:54,610 :  Found small feature: next_sentence_labels [16, 1]\n","2020-04-29 23:33:54,614 :  Found small feature: next_sentence_labels [16, 1]\n","2020-04-29 23:33:54,619 :  Found small feature: next_sentence_labels [16, 1]\n","2020-04-29 23:33:54,624 :  Found small feature: next_sentence_labels [16, 1]\n","2020-04-29 23:33:54,665 :  *** Features ***\n","2020-04-29 23:33:54,666 :    name = input_ids, shape = (16, 128)\n","2020-04-29 23:33:54,667 :    name = input_mask, shape = (16, 128)\n","2020-04-29 23:33:54,668 :    name = masked_lm_ids, shape = (16, 20)\n","2020-04-29 23:33:54,669 :    name = masked_lm_positions, shape = (16, 20)\n","2020-04-29 23:33:54,669 :    name = masked_lm_weights, shape = (16, 20)\n","2020-04-29 23:33:54,670 :    name = next_sentence_labels, shape = (16, 1)\n","2020-04-29 23:33:54,671 :    name = segment_ids, shape = (16, 128)\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING: Entity <function input_fn_builder.<locals>.input_fn.<locals>.<lambda> at 0x7fda75449ae8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\n"],"name":"stdout"},{"output_type":"stream","text":["2020-04-29 23:34:02,126 :  **** Trainable Variables ****\n","2020-04-29 23:34:02,142 :    name = bert/embeddings/word_embeddings:0, shape = (28996, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,149 :    name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,151 :    name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,155 :    name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,157 :    name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,159 :    name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,161 :    name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,164 :    name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,169 :    name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,170 :    name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,175 :    name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,176 :    name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,177 :    name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,179 :    name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,182 :    name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,192 :    name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,193 :    name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,201 :    name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,202 :    name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,204 :    name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,214 :    name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,215 :    name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,221 :    name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,222 :    name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,226 :    name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,233 :    name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,235 :    name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,237 :    name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,241 :    name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,242 :    name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,246 :    name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,248 :    name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,249 :    name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,252 :    name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,253 :    name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,255 :    name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,257 :    name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,258 :    name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,261 :    name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,267 :    name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,268 :    name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,272 :    name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,274 :    name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,275 :    name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,278 :    name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,279 :    name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,280 :    name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,285 :    name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,286 :    name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,290 :    name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,291 :    name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,292 :    name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,297 :    name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,298 :    name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,300 :    name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,301 :    name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,304 :    name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,306 :    name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,310 :    name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,311 :    name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,314 :    name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,314 :    name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,317 :    name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,319 :    name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,322 :    name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,324 :    name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,325 :    name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,328 :    name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,331 :    name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,332 :    name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,335 :    name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,339 :    name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,340 :    name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,345 :    name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,346 :    name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,347 :    name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,351 :    name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,352 :    name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,360 :    name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,361 :    name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,362 :    name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,371 :    name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,372 :    name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,377 :    name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,378 :    name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,379 :    name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,382 :    name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,383 :    name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,386 :    name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,388 :    name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,389 :    name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,394 :    name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,395 :    name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,395 :    name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,399 :    name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,400 :    name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,403 :    name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,404 :    name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,405 :    name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,408 :    name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,409 :    name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,410 :    name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,413 :    name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,414 :    name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,417 :    name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,418 :    name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,419 :    name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,421 :    name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,428 :    name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,429 :    name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,433 :    name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,434 :    name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,435 :    name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,440 :    name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,441 :    name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,445 :    name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,446 :    name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,447 :    name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,451 :    name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,452 :    name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,453 :    name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,455 :    name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,458 :    name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,459 :    name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,463 :    name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,464 :    name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,472 :    name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,473 :    name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,477 :    name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,479 :    name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,488 :    name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,489 :    name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,492 :    name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,496 :    name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,498 :    name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,499 :    name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,502 :    name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,503 :    name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,508 :    name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,509 :    name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,510 :    name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,513 :    name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,514 :    name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,519 :    name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,520 :    name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,521 :    name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,523 :    name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,525 :    name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,530 :    name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,532 :    name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,535 :    name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,536 :    name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,537 :    name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,541 :    name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,542 :    name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,544 :    name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,546 :    name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,548 :    name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,552 :    name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,554 :    name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,557 :    name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,558 :    name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,564 :    name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,565 :    name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,566 :    name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,569 :    name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,570 :    name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,571 :    name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,575 :    name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,576 :    name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,579 :    name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,580 :    name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,582 :    name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,584 :    name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,587 :    name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,590 :    name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,591 :    name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,593 :    name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,595 :    name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,596 :    name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,599 :    name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,601 :    name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,607 :    name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,609 :    name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,610 :    name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,612 :    name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,619 :    name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,626 :    name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,629 :    name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,630 :    name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,632 :    name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,634 :    name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,639 :    name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,642 :    name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,643 :    name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,645 :    name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,650 :    name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,651 :    name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,654 :    name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,655 :    name = cls/predictions/transform/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,660 :    name = cls/predictions/transform/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,661 :    name = cls/predictions/transform/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,662 :    name = cls/predictions/transform/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,666 :    name = cls/predictions/output_bias:0, shape = (28996,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,667 :    name = cls/seq_relationship/output_weights:0, shape = (2, 768), *INIT_FROM_CKPT*\n","2020-04-29 23:34:02,668 :    name = cls/seq_relationship/output_bias:0, shape = (2,), *INIT_FROM_CKPT*\n","2020-04-29 23:34:20,397 :  Create CheckpointSaverHook.\n","2020-04-29 23:34:20,747 :  Done calling model_fn.\n","2020-04-29 23:34:20,748 :  TPU job name worker\n","2020-04-29 23:34:22,301 :  Graph was finalized.\n","2020-04-29 23:34:22,570 :  Restoring parameters from gs://bert_apr/bert_model_final/model.ckpt-10000\n","2020-04-29 23:34:48,540 :  From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file utilities to get mtimes.\n","2020-04-29 23:34:50,119 :  Running local_init_op.\n","2020-04-29 23:34:50,712 :  Done running local_init_op.\n","2020-04-29 23:34:58,979 :  Saving checkpoints for 10000 into gs://bert_apr/bert_model_final/model.ckpt.\n","2020-04-29 23:35:16,077 :  Initialized dataset iterators in 0 seconds\n","2020-04-29 23:35:16,079 :  Installing graceful shutdown hook.\n","2020-04-29 23:35:16,086 :  Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n","2020-04-29 23:35:16,092 :  Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n","\n","2020-04-29 23:35:16,099 :  Init TPU system\n","2020-04-29 23:35:16,126 :  Initialized TPU in 0 seconds\n","2020-04-29 23:35:16,128 :  Starting infeed thread controller.\n","2020-04-29 23:35:16,129 :  Starting outfeed thread controller.\n","2020-04-29 23:35:16,662 :  Enqueue next (2500) batch(es) of data to infeed.\n","2020-04-29 23:35:16,663 :  Dequeue next (2500) batch(es) of data from outfeed.\n","2020-04-29 23:35:51,871 :  Outfeed finished for iteration (0, 0)\n","2020-04-29 23:36:51,915 :  Outfeed finished for iteration (0, 591)\n","2020-04-29 23:37:51,960 :  Outfeed finished for iteration (0, 1182)\n","2020-04-29 23:38:52,004 :  Outfeed finished for iteration (0, 1773)\n","2020-04-29 23:39:52,047 :  Outfeed finished for iteration (0, 2364)\n","2020-04-29 23:40:06,511 :  Saving checkpoints for 12500 into gs://bert_apr/bert_model_final/model.ckpt.\n","2020-04-29 23:40:19,701 :  From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to delete files with this prefix.\n","2020-04-29 23:40:22,656 :  loss = 1.4748628, step = 12500\n","2020-04-29 23:40:22,661 :  Enqueue next (2500) batch(es) of data to infeed.\n","2020-04-29 23:40:22,662 :  Dequeue next (2500) batch(es) of data from outfeed.\n","2020-04-29 23:40:52,074 :  Outfeed finished for iteration (1, 224)\n","2020-04-29 23:41:52,119 :  Outfeed finished for iteration (1, 815)\n","2020-04-29 23:42:52,164 :  Outfeed finished for iteration (1, 1406)\n","2020-04-29 23:43:52,207 :  Outfeed finished for iteration (1, 1997)\n","2020-04-29 23:44:43,971 :  Saving checkpoints for 15000 into gs://bert_apr/bert_model_final/model.ckpt.\n","2020-04-29 23:44:59,906 :  loss = 1.1786946, step = 15000 (277.250 sec)\n","2020-04-29 23:44:59,910 :  global_step/sec: 9.01711\n","2020-04-29 23:44:59,912 :  examples/sec: 1154.19\n","2020-04-29 23:44:59,915 :  Enqueue next (2500) batch(es) of data to infeed.\n","2020-04-29 23:44:59,917 :  Dequeue next (2500) batch(es) of data from outfeed.\n","2020-04-29 23:45:01,138 :  Outfeed finished for iteration (2, 0)\n","2020-04-29 23:46:01,176 :  Outfeed finished for iteration (2, 591)\n","2020-04-29 23:47:01,213 :  Outfeed finished for iteration (2, 1182)\n","2020-04-29 23:48:01,250 :  Outfeed finished for iteration (2, 1773)\n","2020-04-29 23:49:01,284 :  Outfeed finished for iteration (2, 2364)\n","2020-04-29 23:49:15,739 :  Saving checkpoints for 17500 into gs://bert_apr/bert_model_final/model.ckpt.\n","2020-04-29 23:49:31,891 :  loss = 1.4175102, step = 17500 (271.986 sec)\n","2020-04-29 23:49:31,894 :  global_step/sec: 9.19172\n","2020-04-29 23:49:31,896 :  examples/sec: 1176.54\n","2020-04-29 23:49:31,898 :  Enqueue next (2500) batch(es) of data to infeed.\n","2020-04-29 23:49:31,900 :  Dequeue next (2500) batch(es) of data from outfeed.\n","2020-04-29 23:50:01,359 :  Outfeed finished for iteration (3, 278)\n","2020-04-29 23:51:01,398 :  Outfeed finished for iteration (3, 869)\n","2020-04-29 23:52:01,438 :  Outfeed finished for iteration (3, 1460)\n","2020-04-29 23:53:01,479 :  Outfeed finished for iteration (3, 2051)\n","2020-04-29 23:53:47,694 :  Saving checkpoints for 20000 into gs://bert_apr/bert_model_final/model.ckpt.\n","2020-04-29 23:54:02,926 :  loss = 1.284128, step = 20000 (271.035 sec)\n","2020-04-29 23:54:02,930 :  global_step/sec: 9.22389\n","2020-04-29 23:54:02,931 :  examples/sec: 1180.66\n","2020-04-29 23:54:02,937 :  Enqueue next (2500) batch(es) of data to infeed.\n","2020-04-29 23:54:02,939 :  Dequeue next (2500) batch(es) of data from outfeed.\n","2020-04-29 23:54:04,153 :  Outfeed finished for iteration (4, 0)\n","2020-04-29 23:55:04,182 :  Outfeed finished for iteration (4, 591)\n","2020-04-29 23:56:04,214 :  Outfeed finished for iteration (4, 1182)\n","2020-04-29 23:57:04,244 :  Outfeed finished for iteration (4, 1773)\n","2020-04-29 23:58:04,274 :  Outfeed finished for iteration (4, 2364)\n","2020-04-29 23:58:18,721 :  Saving checkpoints for 22500 into gs://bert_apr/bert_model_final/model.ckpt.\n","2020-04-29 23:58:38,028 :  loss = 1.2652893, step = 22500 (275.102 sec)\n","2020-04-29 23:58:38,032 :  global_step/sec: 9.08754\n","2020-04-29 23:58:38,033 :  examples/sec: 1163.2\n","2020-04-29 23:58:38,037 :  Enqueue next (2500) batch(es) of data to infeed.\n","2020-04-29 23:58:38,038 :  Dequeue next (2500) batch(es) of data from outfeed.\n","2020-04-29 23:59:04,344 :  Outfeed finished for iteration (5, 247)\n","2020-04-30 00:00:04,373 :  Outfeed finished for iteration (5, 838)\n","2020-04-30 00:01:04,400 :  Outfeed finished for iteration (5, 1429)\n","2020-04-30 00:02:04,430 :  Outfeed finished for iteration (5, 2020)\n","2020-04-30 00:02:53,785 :  Saving checkpoints for 25000 into gs://bert_apr/bert_model_final/model.ckpt.\n","2020-04-30 00:03:09,786 :  loss = 1.5843805, step = 25000 (271.758 sec)\n","2020-04-30 00:03:09,789 :  global_step/sec: 9.19939\n","2020-04-30 00:03:09,790 :  examples/sec: 1177.52\n","2020-04-30 00:03:09,794 :  Enqueue next (2500) batch(es) of data to infeed.\n","2020-04-30 00:03:09,796 :  Dequeue next (2500) batch(es) of data from outfeed.\n","2020-04-30 00:03:11,008 :  Outfeed finished for iteration (6, 0)\n","2020-04-30 00:04:11,054 :  Outfeed finished for iteration (6, 591)\n","2020-04-30 00:05:11,099 :  Outfeed finished for iteration (6, 1182)\n","2020-04-30 00:06:11,144 :  Outfeed finished for iteration (6, 1773)\n","2020-04-30 00:07:11,190 :  Outfeed finished for iteration (6, 2364)\n","2020-04-30 00:07:25,640 :  Saving checkpoints for 27500 into gs://bert_apr/bert_model_final/model.ckpt.\n","2020-04-30 00:07:45,254 :  loss = 1.5009868, step = 27500 (275.467 sec)\n","2020-04-30 00:07:45,256 :  global_step/sec: 9.07548\n","2020-04-30 00:07:45,258 :  examples/sec: 1161.66\n","2020-04-30 00:07:45,263 :  Enqueue next (2500) batch(es) of data to infeed.\n","2020-04-30 00:07:45,264 :  Dequeue next (2500) batch(es) of data from outfeed.\n","2020-04-30 00:08:11,281 :  Outfeed finished for iteration (7, 244)\n","2020-04-30 00:09:11,325 :  Outfeed finished for iteration (7, 835)\n","2020-04-30 00:10:11,369 :  Outfeed finished for iteration (7, 1426)\n","2020-04-30 00:11:11,414 :  Outfeed finished for iteration (7, 2017)\n","2020-04-30 00:12:01,146 :  Saving checkpoints for 30000 into gs://bert_apr/bert_model_final/model.ckpt.\n","2020-04-30 00:12:18,982 :  loss = 1.6915066, step = 30000 (273.729 sec)\n","2020-04-30 00:12:18,985 :  global_step/sec: 9.13312\n","2020-04-30 00:12:18,986 :  examples/sec: 1169.04\n","2020-04-30 00:12:18,991 :  Enqueue next (2500) batch(es) of data to infeed.\n","2020-04-30 00:12:18,992 :  Dequeue next (2500) batch(es) of data from outfeed.\n","2020-04-30 00:12:20,225 :  Outfeed finished for iteration (8, 0)\n","2020-04-30 00:13:20,287 :  Outfeed finished for iteration (8, 591)\n","2020-04-30 00:14:20,347 :  Outfeed finished for iteration (8, 1182)\n","2020-04-30 00:15:20,407 :  Outfeed finished for iteration (8, 1773)\n","2020-04-30 00:16:20,468 :  Outfeed finished for iteration (8, 2364)\n","2020-04-30 00:16:34,926 :  Saving checkpoints for 32500 into gs://bert_apr/bert_model_final/model.ckpt.\n","2020-04-30 00:16:53,217 :  loss = 1.5366663, step = 32500 (274.235 sec)\n","2020-04-30 00:16:53,223 :  global_step/sec: 9.11618\n","2020-04-30 00:16:53,224 :  examples/sec: 1166.87\n","2020-04-30 00:16:53,227 :  Enqueue next (2500) batch(es) of data to infeed.\n","2020-04-30 00:16:53,228 :  Dequeue next (2500) batch(es) of data from outfeed.\n","2020-04-30 00:17:20,515 :  Outfeed finished for iteration (9, 256)\n","2020-04-30 00:18:20,552 :  Outfeed finished for iteration (9, 847)\n","2020-04-30 00:19:20,588 :  Outfeed finished for iteration (9, 1438)\n","2020-04-30 00:20:20,625 :  Outfeed finished for iteration (9, 2029)\n","2020-04-30 00:21:09,074 :  Saving checkpoints for 35000 into gs://bert_apr/bert_model_final/model.ckpt.\n","2020-04-30 00:21:26,491 :  loss = 1.1998644, step = 35000 (273.274 sec)\n","2020-04-30 00:21:26,494 :  global_step/sec: 9.14842\n","2020-04-30 00:21:26,497 :  examples/sec: 1171\n","2020-04-30 00:21:26,500 :  Enqueue next (2500) batch(es) of data to infeed.\n","2020-04-30 00:21:26,502 :  Dequeue next (2500) batch(es) of data from outfeed.\n","2020-04-30 00:21:27,711 :  Outfeed finished for iteration (10, 0)\n","2020-04-30 00:22:27,753 :  Outfeed finished for iteration (10, 591)\n","2020-04-30 00:23:27,796 :  Outfeed finished for iteration (10, 1182)\n","2020-04-30 00:24:27,839 :  Outfeed finished for iteration (10, 1773)\n","2020-04-30 00:25:27,883 :  Outfeed finished for iteration (10, 2364)\n","2020-04-30 00:25:42,305 :  Saving checkpoints for 37500 into gs://bert_apr/bert_model_final/model.ckpt.\n","2020-04-30 00:25:59,332 :  loss = 1.6120564, step = 37500 (272.841 sec)\n","2020-04-30 00:25:59,335 :  global_step/sec: 9.16284\n","2020-04-30 00:25:59,338 :  examples/sec: 1172.84\n","2020-04-30 00:25:59,342 :  Enqueue next (2500) batch(es) of data to infeed.\n","2020-04-30 00:25:59,348 :  Dequeue next (2500) batch(es) of data from outfeed.\n","2020-04-30 00:26:27,901 :  Outfeed finished for iteration (11, 269)\n","2020-04-30 00:27:27,951 :  Outfeed finished for iteration (11, 860)\n","2020-04-30 00:28:27,999 :  Outfeed finished for iteration (11, 1451)\n","2020-04-30 00:29:28,045 :  Outfeed finished for iteration (11, 2042)\n","2020-04-30 00:30:15,206 :  Saving checkpoints for 40000 into gs://bert_apr/bert_model_final/model.ckpt.\n","2020-04-30 00:30:32,197 :  loss = 1.469377, step = 40000 (272.864 sec)\n","2020-04-30 00:30:32,200 :  global_step/sec: 9.16207\n","2020-04-30 00:30:32,204 :  examples/sec: 1172.74\n","2020-04-30 00:30:32,207 :  Enqueue next (2500) batch(es) of data to infeed.\n","2020-04-30 00:30:32,209 :  Dequeue next (2500) batch(es) of data from outfeed.\n","2020-04-30 00:30:33,438 :  Outfeed finished for iteration (12, 0)\n","2020-04-30 00:31:33,473 :  Outfeed finished for iteration (12, 591)\n","2020-04-30 00:32:33,506 :  Outfeed finished for iteration (12, 1182)\n","2020-04-30 00:33:33,541 :  Outfeed finished for iteration (12, 1773)\n","2020-04-30 00:34:33,575 :  Outfeed finished for iteration (12, 2364)\n","2020-04-30 00:34:48,033 :  Saving checkpoints for 42500 into gs://bert_apr/bert_model_final/model.ckpt.\n","2020-04-30 00:35:05,424 :  loss = 1.168827, step = 42500 (273.227 sec)\n","2020-04-30 00:35:05,427 :  global_step/sec: 9.14988\n","2020-04-30 00:35:05,430 :  examples/sec: 1171.19\n","2020-04-30 00:35:05,433 :  Enqueue next (2500) batch(es) of data to infeed.\n","2020-04-30 00:35:05,434 :  Dequeue next (2500) batch(es) of data from outfeed.\n","2020-04-30 00:35:33,595 :  Outfeed finished for iteration (13, 265)\n","2020-04-30 00:36:33,655 :  Outfeed finished for iteration (13, 856)\n","2020-04-30 00:37:33,714 :  Outfeed finished for iteration (13, 1447)\n","2020-04-30 00:38:33,774 :  Outfeed finished for iteration (13, 2038)\n","2020-04-30 00:39:21,341 :  Saving checkpoints for 45000 into gs://bert_apr/bert_model_final/model.ckpt.\n","2020-04-30 00:39:35,857 :  loss = 1.3803614, step = 45000 (270.433 sec)\n","2020-04-30 00:39:35,860 :  global_step/sec: 9.24444\n","2020-04-30 00:39:35,862 :  examples/sec: 1183.29\n","2020-04-30 00:39:35,866 :  Enqueue next (2500) batch(es) of data to infeed.\n","2020-04-30 00:39:35,868 :  Dequeue next (2500) batch(es) of data from outfeed.\n","2020-04-30 00:39:37,102 :  Outfeed finished for iteration (14, 0)\n","2020-04-30 00:40:37,154 :  Outfeed finished for iteration (14, 591)\n","2020-04-30 00:41:37,203 :  Outfeed finished for iteration (14, 1182)\n","2020-04-30 00:42:37,252 :  Outfeed finished for iteration (14, 1773)\n","2020-04-30 00:43:37,302 :  Outfeed finished for iteration (14, 2364)\n","2020-04-30 00:43:51,758 :  Saving checkpoints for 47500 into gs://bert_apr/bert_model_final/model.ckpt.\n","2020-04-30 00:44:07,681 :  loss = 1.2934004, step = 47500 (271.824 sec)\n","2020-04-30 00:44:07,685 :  global_step/sec: 9.1971\n","2020-04-30 00:44:07,687 :  examples/sec: 1177.23\n","2020-04-30 00:44:07,692 :  Enqueue next (2500) batch(es) of data to infeed.\n","2020-04-30 00:44:07,693 :  Dequeue next (2500) batch(es) of data from outfeed.\n","2020-04-30 00:44:37,378 :  Outfeed finished for iteration (15, 280)\n","2020-04-30 00:45:37,422 :  Outfeed finished for iteration (15, 871)\n","2020-04-30 00:46:37,465 :  Outfeed finished for iteration (15, 1462)\n","2020-04-30 00:47:37,508 :  Outfeed finished for iteration (15, 2053)\n","2020-04-30 00:48:23,573 :  Saving checkpoints for 50000 into gs://bert_apr/bert_model_final/model.ckpt.\n","2020-04-30 00:48:39,776 :  loss = 1.5060102, step = 50000 (272.095 sec)\n","2020-04-30 00:48:39,778 :  global_step/sec: 9.18801\n","2020-04-30 00:48:39,780 :  examples/sec: 1176.07\n","2020-04-30 00:48:40,378 :  Stop infeed thread controller\n","2020-04-30 00:48:40,379 :  Shutting down InfeedController thread.\n","2020-04-30 00:48:40,382 :  InfeedController received shutdown signal, stopping.\n","2020-04-30 00:48:40,385 :  Infeed thread finished, shutting down.\n","2020-04-30 00:48:40,389 :  infeed marked as finished\n","2020-04-30 00:48:40,393 :  Stop output thread controller\n","2020-04-30 00:48:40,395 :  Shutting down OutfeedController thread.\n","2020-04-30 00:48:40,397 :  OutfeedController received shutdown signal, stopping.\n","2020-04-30 00:48:40,398 :  Outfeed thread finished, shutting down.\n","2020-04-30 00:48:40,401 :  outfeed marked as finished\n","2020-04-30 00:48:40,402 :  Shutdown TPU system.\n","2020-04-30 00:48:41,604 :  Loss for final step: 1.5060102.\n","2020-04-30 00:48:41,605 :  training_loop marked as finished\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow_estimator.python.estimator.tpu.tpu_estimator.TPUEstimator at 0x7fda77fd2b00>"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"BHpgvoDwTMjW","colab_type":"code","outputId":"01654d2a-dbd5-4435-bc14-8ced0dcb836e","executionInfo":{"status":"ok","timestamp":1588225749505,"user_tz":300,"elapsed":6293,"user":{"displayName":"Eadhunath Venghatesan","photoUrl":"","userId":"10975244633908642389"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(\"DONE\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["DONE\n"],"name":"stdout"}]}],"metadata":{"colab":{"name":"1.BERT_Pretraining.ipynb","provenance":[{"file_id":"1Y9Cc2RpKlM9_npshn1DqZwhFO4MTZFzW","timestamp":1589214954792}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"nbformat":4,"nbformat_minor":0}